{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flood Segmentation with Deep Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FM7QdfvHzyww"
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15277,
     "status": "ok",
     "timestamp": 1619196726721,
     "user": {
      "displayName": "Kushanav Bhuyan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhpJDzYajMCPL_21Vep_aig5cbphA73G195JQkf=s64",
      "userId": "13534778218088456884"
     },
     "user_tz": -120
    },
    "id": "WHTzdTb3y2Y8",
    "outputId": "4644402d-8522-4525-e6b6-10c2c5932c99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing packages.......\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kusha\\anaconda3\\envs\\DL\\lib\\site-packages\\IPython\\core\\magics\\pylab.py:160: UserWarning: pylab import has clobbered these variables: ['random']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Models: using `tf.keras` framework.\n",
      "Libraries imported\n"
     ]
    }
   ],
   "source": [
    "# Import the required Libraries\n",
    "\n",
    "print(\"Importing packages.......\")\n",
    "\n",
    "import gc\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from osgeo import gdal\n",
    "import tensorflow.keras\n",
    "import tensorflow as tf\n",
    "import util\n",
    "\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt \n",
    "import sys\n",
    "import random\n",
    "import cv2\n",
    "# import losses \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "\n",
    "pylab.rcParams['figure.figsize'] = (15, 10)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from skimage import exposure\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from skimage import (img_as_float,\n",
    "                    img_as_ubyte,\n",
    "                    img_as_uint,\n",
    "                    img_as_int,\n",
    "                    io\n",
    "                    )          \n",
    "import cv2\n",
    "\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras import backend as keras\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.keras.applications import VGG19, densenet\n",
    "\n",
    "sess = K.get_session()\n",
    "from tensorflow.compat.v1.keras.backend import set_session\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve # roc curve tools\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import imgaug as ia\n",
    "import imgaug.augmenters as iaa\n",
    "import segmentation_models as sm\n",
    "\n",
    "# print(\"Installing packages.......\")\n",
    "\n",
    "# !pip install keras-tqdm\n",
    "# from keras_tqdm import TQDMCallback, TQDMNotebookCallback\n",
    "\n",
    "# !pip install ipython-autotime\n",
    "# %load_ext autotime\n",
    "\n",
    "print(\"Libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJSfvuyV01zL"
   },
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Du1-Gc6Pz3M8"
   },
   "source": [
    "### Load and pre-process the arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFolder = 'D:/Research/PhD/Flood Detection Sydney/Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readdata(directory, folder_name):\n",
    "      \n",
    "    data_path = os.path.join(directory, folder_name)\n",
    "    data_list = {}\n",
    "\n",
    "    for f in sorted(os.listdir(data_path)):\n",
    "        fdir = os.path.join(data_path, f)\n",
    "        _, ext = os.path.splitext(f)\n",
    "        if ext.lower() == \".tif\":\n",
    "            imgtype = f[-55:-4] # Number of characters of the image file. Example, \"Image_1.tif\" = 11 characters\n",
    "            image_data=gdal.Open(fdir)\n",
    "            bands = [image_data.GetRasterBand(i+1).ReadAsArray() for i in range(image_data.RasterCount)]\n",
    "            data_list[imgtype] = np.stack(bands, axis=2) \n",
    "\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list = readdata(directory=DataFolder, folder_name=\"Stacks\")\n",
    "label_list = readdata(directory=DataFolder, folder_name=\"Masks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The collection of stacks available in the image list are:\n",
      "------------------------------\n",
      "PCA_24\n",
      "SEECWSWI_frame2_1\n",
      "SEECWSWI_frame2_2\n",
      "SEECW_frame2_1\n",
      "SEECW_frame2_2\n",
      "coherence_20210324_VH\n",
      "coherence_20210324_VV\n",
      "md_sigma_energy_entropy_coherence_water_20210324_VH\n",
      "energy_20210324_VH\n",
      "energy_20210324_VV\n",
      "energy_entropy_20210324_VH\n",
      "energy_entropy_20210324_VV\n",
      "entropy_20210324_VH\n",
      "entropy_20210324_VV\n",
      "sigma_20210312_VH\n",
      "sigma_20210312_VV\n",
      "sigma_20210324_VH\n",
      "sigma_20210324_VV\n",
      "sigma_energy_entropy_20210324_VH\n",
      "sigma_energy_entropy_20210324_VV\n",
      "sigma_energy_entropy_coherence_20210324_VH\n",
      "sigma_energy_entropy_coherence_20210324_VV\n",
      "sigma_energy_entropy_coherence_water_20210324_VH\n"
     ]
    }
   ],
   "source": [
    "# Print all the stacks that are collected\n",
    "print(\"The collection of stacks available in the image list are:\")\n",
    "print(\"-\"*30)\n",
    "print(*image_list, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The collection of masks available in the image list are:\n",
      "------------------------------\n",
      "SEECW_frame2_1_mask\n",
      "SEECW_frame2_2_mask\n",
      "flood_mask\n"
     ]
    }
   ],
   "source": [
    "# Print all the stacks that are collected\n",
    "print(\"The collection of masks available in the image list are:\")\n",
    "print(\"-\"*30)\n",
    "print(*label_list, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of satellite images = 23\n",
      "Shape of the sample image = (1734, 2279, 5)\n",
      "Number of label images = 3\n",
      "Shape of the image = (1734, 2279, 1)\n"
     ]
    }
   ],
   "source": [
    "#Check If Everything Works (CIEW)\n",
    "print(\"Number of satellite images = \" + str(len(image_list)))\n",
    "print(\"Shape of the sample image = \"+ str(image_list[\"SEECW_frame2_1\"].shape))\n",
    "\n",
    "print(\"Number of label images = \" + str(len(label_list)))\n",
    "print(\"Shape of the image = \"+ str(label_list[\"SEECW_frame2_1_mask\"].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridwise_sample(imgarray, patchsize):\n",
    "\n",
    "    \"\"\"Extract sample patches of size patchsize x patchsize from an image (imgarray) in a gridwise manner.\n",
    "    \"\"\"\n",
    "    nrows, ncols, nbands = imgarray.shape\n",
    "    patchsamples = np.zeros(shape=(0, patchsize, patchsize, nbands),\n",
    "                            dtype=imgarray.dtype)\n",
    "    for i in range(int(nrows/patchsize)):\n",
    "        for j in range(int(ncols/patchsize)):\n",
    "            tocat = imgarray[i*patchsize:(i+1)*patchsize,\n",
    "                             j*patchsize:(j+1)*patchsize, :]\n",
    "            tocat = np.expand_dims(tocat, axis=0)\n",
    "            patchsamples = np.concatenate((patchsamples, tocat),\n",
    "                                          axis=0)\n",
    "    return patchsamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 400 number of training patches\n",
      "Data type of Xtrain is uint16\n",
      "Data type of Ytrain is uint16\n"
     ]
    }
   ],
   "source": [
    "# FOR GENERATING TRAINING SITES\n",
    "\n",
    "PATCHSIZE = 128\n",
    "NBANDS = 5 # According to the stack, change the number of bands here.\n",
    "\n",
    "Xtrain = np.zeros(shape=(0, PATCHSIZE, PATCHSIZE, NBANDS), dtype=np.float32)\n",
    "Ytrain = np.zeros(shape=(0, PATCHSIZE, PATCHSIZE, 1), dtype=np.float32)\n",
    "\n",
    "# Sample each training tile systematically in a gridwise manner\n",
    "train_areas = [\"sigma_energy_entropy_coherence_water_20210324_VH\"] # Choose the required stack for your use case\n",
    "train_areas_mask = [\"flood_mask\"] # The mask has already been made for you. Simply put the associated masks here\n",
    "\n",
    "for area in train_areas:\n",
    "    X_toadd = gridwise_sample(image_list[area], PATCHSIZE)\n",
    "    Xtrain = np.concatenate((Xtrain, X_toadd), axis=0)\n",
    "    Xtrain = Xtrain.astype('uint16')\n",
    "    \n",
    "for area_mask in train_areas_mask:\n",
    "    Y_toadd = gridwise_sample(label_list[area_mask], PATCHSIZE)\n",
    "    Ytrain = np.concatenate((Ytrain, Y_toadd), axis=0)\n",
    "    Ytrain = Ytrain.astype('uint16')\n",
    "\n",
    "# Encode all flood and background labels into their respective classes \"0\" and \"1\" (Categorical)\n",
    "Ytrain[Ytrain==0] = 0\n",
    "Ytrain[Ytrain!=0] = 1\n",
    "\n",
    "print(\"There are %i number of training patches\" % (Xtrain.shape[0]))\n",
    "print(f'Data type of Xtrain is {Xtrain.dtype}')\n",
    "print(f'Data type of Ytrain is {Ytrain.dtype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel percentage of flood = 0.75836181640625 %\n",
      "Pixel percentage of background = 99.24163818359375 %\n"
     ]
    }
   ],
   "source": [
    "#CIEW for training patches\n",
    "bd_px=len(Ytrain[Ytrain==1])/(len(Ytrain[Ytrain==1])+len(Ytrain[Ytrain==0]))\n",
    "bg_px=len(Ytrain[Ytrain==0])/(len(Ytrain[Ytrain==1])+len(Ytrain[Ytrain==0]))\n",
    "\n",
    "print(\"Pixel percentage of flood = \" + str(bd_px* 100)+ \" %\") \n",
    "print(\"Pixel percentage of background = \" + str(bg_px* 100)+ \" %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Zero Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_zeros(images_array,masks_array):\n",
    "    \n",
    "    all_zeros = []\n",
    "    for i in range(masks_array.shape[0]):\n",
    "        if masks_array[i].max() == 0:\n",
    "            all_zeros.append(i)\n",
    "    print(\"There are: {} arrays with just 0 values, and {} arrays with non zero values \".format(len(all_zeros), (\n",
    "                images_array.shape[0] - len(all_zeros))))\n",
    "    images = []\n",
    "    masks = []\n",
    "    for i in range(images_array.shape[0]):\n",
    "        if i not in all_zeros:\n",
    "            images.append(images_array[i])\n",
    "            masks.append(masks_array[i])\n",
    "\n",
    "    # Convert to array\n",
    "    images = np.array(images, dtype=\"float64\")\n",
    "    masks = np.array(masks, dtype=\"float32\")\n",
    "    print(\"Image shape: {}, Mask shape: {}\".format(images.shape, masks.shape))\n",
    "    return images, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are: 366 arrays with just 0 values, and 34 arrays with non zero values \n",
      "Image shape: (34, 128, 128, 5), Mask shape: (34, 128, 128, 1)\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "There are 34 number of training patches\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train = remove_zeros(Xtrain, Ytrain)\n",
    "\n",
    "print(\"----\"*15)\n",
    "\n",
    "# X_test, Y_test = remove_zeros(Xtest, Ytest)\n",
    "\n",
    "print(\"----\"*15)\n",
    "\n",
    "print(\"There are %i number of training patches\" % (X_train.shape[0]))\n",
    "# print(\"There are %i number of testing patches\" % (X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel percentage of flood = 8.921903722426471 %\n",
      "Pixel percentage of background = 91.07809627757352 %\n"
     ]
    }
   ],
   "source": [
    "#CIEW for training patches\n",
    "bd_px=len(Y_train[Y_train==1])/(len(Y_train[Y_train==1])+len(Y_train[Y_train==0]))\n",
    "bg_px=len(Y_train[Y_train==0])/(len(Y_train[Y_train==1])+len(Y_train[Y_train==0]))\n",
    "\n",
    "print(\"Pixel percentage of flood = \" + str(bd_px* 100)+ \" %\") \n",
    "print(\"Pixel percentage of background = \" + str(bg_px* 100)+ \" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34, 128, 128, 5)\n",
      "(34, 128, 128, 1)\n",
      "(170, 128, 128, 5)\n",
      "(170, 128, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "# Try augmentations..........\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "\n",
    "seq = iaa.Sequential([\n",
    "    iaa.Fliplr(1)\n",
    "])\n",
    "seq1 = iaa.Sequential([\n",
    "    iaa.Flipud(1)\n",
    "])\n",
    "\n",
    "seq2 = iaa.Sequential([\n",
    "    iaa.ShearX((-20, 20))\n",
    "])\n",
    "\n",
    "seq3 = iaa.Sequential([\n",
    "    iaa.ShearY((-20, 20))\n",
    "])\n",
    "\n",
    "seq4 = iaa.Sequential([\n",
    "    iaa.Rotate((-45, 45))\n",
    "])\n",
    "\n",
    "# Horizontal Flip\n",
    "X_aug, Y_aug = seq(images=X_train, heatmaps=Y_train)\n",
    "\n",
    "# Vertical Flip\n",
    "X_aug1, Y_aug1 = seq1(images=X_train, heatmaps=Y_train)\n",
    "\n",
    "# Shear X\n",
    "X_aug2, Y_aug2 = seq2(images=X_train, heatmaps=Y_train)\n",
    "\n",
    "# Shear Y\n",
    "X_aug3, Y_aug3 = seq3(images=X_train, heatmaps=Y_train)\n",
    "\n",
    "# Rotate +-45\n",
    "X_aug4, Y_aug4 = seq4(images=X_train, heatmaps=Y_train)\n",
    "\n",
    "X_train = np.concatenate((X_train, X_aug1, X_aug2, X_aug3,  X_aug4), axis=0)\n",
    "Y_train = np.concatenate((Y_train, Y_aug1, Y_aug2, Y_aug3,  Y_aug4), axis=0)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-tag into new variables. All training and testing images will be saved into a new variable\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_train, Y_train, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Pixel percentage of the Background Class on the Training Set = 91.9497964446729 %\n",
      "Pixel percentage of Flood Class on the Training Set = 8.0502035553271 %\n",
      "--------------------------------------------------\n",
      "Pixel percentage of the Background Class on the Testing Set = 91.97678144594174 %\n",
      "Pixel percentage of Flood Class on the Testing Set = 8.023218554058268 %\n"
     ]
    }
   ],
   "source": [
    "# CIEW for pixel-class ratio within the patches \n",
    "\n",
    "print(\"-\"*50)\n",
    "\n",
    "# For training set\n",
    "ls_px_train=len(Y_train[Y_train==1])/(len(Y_train[Y_train==1])+len(Y_train[Y_train==0]))\n",
    "bg_px_train=len(Y_train[Y_train==0])/(len(Y_train[Y_train==1])+len(Y_train[Y_train==0]))\n",
    "\n",
    "print(\"Pixel percentage of the Background Class on the Training Set = \" + str(bg_px_train* 100)+ \" %\")\n",
    "print(\"Pixel percentage of Flood Class on the Training Set = \" + str(ls_px_train* 100) + \" %\") \n",
    "\n",
    "print(\"-\"*50)\n",
    "\n",
    "# For testing set\n",
    "ls_px_test=len(Y_test[Y_test==1])/(len(Y_test[Y_test==1])+len(Y_test[Y_test==0]))\n",
    "bg_px_test=len(Y_test[Y_test==0])/(len(Y_test[Y_test==1])+len(Y_test[Y_test==0]))\n",
    "\n",
    "print(\"Pixel percentage of the Background Class on the Testing Set = \" + str(bg_px_test* 100)+ \" %\")\n",
    "print(\"Pixel percentage of Flood Class on the Testing Set = \" + str(ls_px_test* 100) + \" %\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the training satellite image = (113, 128, 128, 5)\n",
      "Shape of the training mask image = (113, 128, 128, 1)\n",
      "----------------------------------------------------------\n",
      "Shape of the testing satellite image = (57, 128, 128, 5)\n",
      "Shape of the testing mask image = (57, 128, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape of the training satellite image = {X_train.shape}')\n",
    "print(f'Shape of the training mask image = {Y_train.shape}')\n",
    "print(\"-\"*58)\n",
    "print(f'Shape of the testing satellite image = {X_test.shape}')\n",
    "print(f'Shape of the testing mask image = {Y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Normalise the values of the satellite image between 0 and 1\n",
    "norm_X_train = (X_train - X_train.min())/(X_train.max() - X_train.min())\n",
    "\n",
    "# print(norm_Xtrain)\n",
    "print(norm_X_train.max())\n",
    "print(norm_X_train.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I8_kQK8sPLUh"
   },
   "source": [
    "## Define loss functions and metrics for various accuracies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluation metrics - Precision, Recall, FScore, IoU\n",
    "metrics_sm = [sm.metrics.Precision(threshold=0.5),sm.metrics.Recall(threshold=0.5),sm.metrics.FScore(threshold=0.5,beta=1),sm.metrics.IOUScore(threshold=0.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 167710,
     "status": "ok",
     "timestamp": 1619182203298,
     "user": {
      "displayName": "Kushanav Bhuyan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhpJDzYajMCPL_21Vep_aig5cbphA73G195JQkf=s64",
      "userId": "13534778218088456884"
     },
     "user_tz": -120
    },
    "id": "h_WKYJtp-5j2",
    "outputId": "c4d51b7b-0b6d-48fc-9e34-88262ea72325"
   },
   "outputs": [],
   "source": [
    "# Losses\n",
    "\n",
    "# Tversky\n",
    "def tversky(y_true, y_pred):\n",
    "    \n",
    "    y_true_pos = K.flatten(y_true)\n",
    "    y_pred_pos = K.flatten(y_pred)\n",
    "    true_pos = K.sum(y_true_pos * y_pred_pos)\n",
    "    false_neg = K.sum(y_true_pos * (1-y_pred_pos))\n",
    "    false_pos = K.sum((1-y_true_pos)*y_pred_pos)\n",
    "    alpha = 0.7\n",
    "    return (true_pos + K.epsilon())/(true_pos + alpha*false_neg + (1-alpha)*false_pos + K.epsilon())\n",
    "\n",
    "def tversky_loss(y_true, y_pred, alpha=0.30, beta=0.70):\n",
    "  \n",
    "    \"\"\"\n",
    "    Function to calculate the Tversky loss for imbalanced data\n",
    "    :param prediction: the logits\n",
    "    :param ground_truth: the segmentation ground_truth\n",
    "    :param alpha: weight of false positives\n",
    "    :param beta: weight of false negatives\n",
    "    :param weight_map:\n",
    "    :return: the loss\n",
    "    \"\"\"\n",
    "    '''\n",
    "    EPSILON = 0.00001 (default)\n",
    "    '''\n",
    "    y_true_pos = K.flatten(y_true)\n",
    "    y_pred_pos = K.flatten(y_pred)\n",
    "    # TP\n",
    "    true_pos = K.sum(y_true_pos * y_pred_pos)\n",
    "    # FN\n",
    "    false_neg = K.sum(y_true_pos * (1-y_pred_pos))\n",
    "    # FP\n",
    "    false_pos = K.sum((1-y_true_pos) * y_pred_pos)\n",
    "    return 1 - (true_pos + K.epsilon())/(true_pos + alpha * false_neg + beta * false_pos + K.epsilon())\n",
    "\n",
    "# Focal Tversky\n",
    "\n",
    "def focal_tversky(y_true,y_pred):\n",
    "    \n",
    "    pt_1 = tversky_loss(y_true, y_pred)\n",
    "    gamma = 0.75\n",
    "    return K.pow((1-pt_1), gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1289,
     "status": "ok",
     "timestamp": 1619197770476,
     "user": {
      "displayName": "Kushanav Bhuyan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhpJDzYajMCPL_21Vep_aig5cbphA73G195JQkf=s64",
      "userId": "13534778218088456884"
     },
     "user_tz": -120
    },
    "id": "NlEP9kAXPU2X",
    "outputId": "e1414246-3880-428d-8127-bbde1e9f4993"
   },
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred, threshold=0.5):\n",
    "    \"\"\"compute accuracy\"\"\"\n",
    "    #y_t = y_true[...,0]\n",
    "    #y_t = y_t[...,np.newaxis]\n",
    "    y_pred = K.round(y_pred +0.5 - threshold)\n",
    "    return K.equal(K.round(y_true), K.round(y_pred))\n",
    "\n",
    "# K.round() returns the Element-wise rounding to the closest integer!!!\n",
    "# So the threshold to determine a true positive is set here!!!!!\n",
    "def true_positives(y_true, y_pred, threshold=0.5):\n",
    "    \"\"\"compute true positive\"\"\"\n",
    "    #y_t = y_true[...,0]\n",
    "    #y_t = y_t[...,np.newaxis]\n",
    "    y_pred = K.round(y_pred +0.5 - threshold)\n",
    "    return K.round(y_true * y_pred)\n",
    "\n",
    "def false_positives(y_true, y_pred, threshold=0.5):\n",
    "    \"\"\"compute false positive\"\"\"\n",
    "    #y_t = y_true[...,0]\n",
    "    #y_t = y_t[...,np.newaxis]\n",
    "    y_pred = K.round(y_pred +0.5 - threshold)\n",
    "    return K.round((1 - y_true) * y_pred)\n",
    "\n",
    "def true_negatives(y_true, y_pred, threshold=0.5):\n",
    "    \"\"\"compute true negative\"\"\"\n",
    "    #y_t = y_true[...,0]\n",
    "    #y_t = y_t[...,np.newaxis]\n",
    "    y_pred = K.round(y_pred +0.5 - threshold)\n",
    "    return K.round((1 - y_true) * (1 - y_pred))\n",
    "\n",
    "def false_negatives(y_true, y_pred, threshold=0.5):\n",
    "    \"\"\"compute false negative\"\"\"\n",
    "    #y_t = y_true[...,0]\n",
    "    #y_t = y_t[...,np.newaxis]\n",
    "    y_pred = K.round(y_pred +0.5 - threshold)\n",
    "    return K.round((y_true) * (1 - y_pred))\n",
    "\n",
    "# K.sum() returns a single integer output unlike the K.round() which returns an element-wise matrix\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    #y_t = y_true[...,0]\n",
    "    #y_t = y_t[...,np.newaxis]\n",
    "    tp = true_positives(y_true, y_pred)\n",
    "    fn = false_negatives(y_true, y_pred)\n",
    "    recall = K.sum(tp) / (K.sum(tp) + K.sum(fn)+ K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    #y_t = y_true[...,0]\n",
    "    #y_t = y_t[...,np.newaxis]\n",
    "    tp = true_positives(y_true, y_pred)\n",
    "    fp = false_positives(y_true, y_pred)\n",
    "    precision = K.sum(tp) / (K.sum(tp) + K.sum(fp)+ K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mi4kBr89-mUW"
   },
   "source": [
    "## Set Hyper-parameters, General Settings and Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1792,
     "status": "ok",
     "timestamp": 1619197771001,
     "user": {
      "displayName": "Kushanav Bhuyan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhpJDzYajMCPL_21Vep_aig5cbphA73G195JQkf=s64",
      "userId": "13534778218088456884"
     },
     "user_tz": -120
    },
    "id": "arnpWqYxQr_s",
    "outputId": "bcf6c2d1-dbee-4b86-a9c9-be5d917c017a"
   },
   "outputs": [],
   "source": [
    "# Checkpoint for saving the weights \n",
    "checkpoint_path = os.path.join('Data/Checkpoints/','weights.{epoch:02d}-{val_loss:.2f}.hdf5')\n",
    "\n",
    "# TensorBoard compilation of logs \n",
    "# tensorboard = tf.keras.callbacks.TensorBoard(log_dir=\"logs/fit\", histogram_freq=1)\n",
    "\n",
    "# Create a callback that saves the model's weights at every epoch\n",
    "callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=False, save_best_only=True, monitor=\"val_loss\", mode = \"min\",\n",
    "                                                 verbose=1)\n",
    "\n",
    "# Create a callback to reduce the learning rate using the ReduceLROnPlateau function\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.33, patience=15, mode=\"min\")\n",
    "\n",
    "# Compile the checkpoints and other settings as a callback\n",
    "callback_list = [callback, reduce_lr]\n",
    "\n",
    "# Other HPs\n",
    "metrics = [accuracy, precision_m, recall_m, f1_m]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-wWOO8CUNP3a"
   },
   "source": [
    "# Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lIaypDa1l9sJ"
   },
   "source": [
    "## U-Net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Net model \n",
    "\n",
    "def UNet(loss, lr, filters, pretrained_weights = None, input_size = (PATCHSIZE, PATCHSIZE, NBANDS)):\n",
    "  \n",
    "    inputs = Input(input_size)\n",
    "    conv1_1 = Conv2D(filters, (3, 3), padding='same', kernel_initializer = 'he_normal')(inputs)\n",
    "    bn1_1 = BatchNormalization(axis=3)(conv1_1)\n",
    "    relu1_1 = Activation('relu')(bn1_1)\n",
    "    conv1_2 = Conv2D(filters, (3, 3), padding='same', kernel_initializer = 'he_normal')(relu1_1)\n",
    "    bn1_2 = BatchNormalization(axis=3)(conv1_2)\n",
    "    relu1_2 = Activation('relu')(bn1_2)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(relu1_2)\n",
    "    \n",
    "    conv2_1 = Conv2D(filters*2, (3, 3), padding='same', kernel_initializer = 'he_normal')(pool1)\n",
    "    bn2_1 = BatchNormalization(axis=3)(conv2_1)\n",
    "    relu2_1 = Activation('relu')(bn2_1)\n",
    "    conv2_2 = Conv2D(filters*2, (3, 3), padding='same', kernel_initializer = 'he_normal')(relu2_1)\n",
    "    bn2_2 = BatchNormalization(axis=3)(conv2_2)\n",
    "    relu2_2 = Activation('relu')(bn2_2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(relu2_2)\n",
    "    \n",
    "    conv3_1 = Conv2D(filters*4, (3, 3), padding='same', kernel_initializer = 'he_normal')(pool2)\n",
    "    bn3_1 = BatchNormalization(axis=3)(conv3_1)\n",
    "    relu3_1 = Activation('relu')(bn3_1)\n",
    "    conv3_2 = Conv2D(filters*4, (3, 3), padding='same', kernel_initializer = 'he_normal')(relu3_1)\n",
    "    bn3_2 = BatchNormalization(axis=3)(conv3_2)\n",
    "    relu3_2 = Activation('relu')(bn3_2)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(relu3_2)\n",
    "    \n",
    "    conv4_1 = Conv2D(filters*8, (3, 3), padding='same', kernel_initializer = 'he_normal')(pool3)\n",
    "    bn4_1 = BatchNormalization(axis=3)(conv4_1)\n",
    "    relu4_1 = Activation('relu')(bn4_1)\n",
    "    conv4_2 = Conv2D(filters*8, (3, 3), padding='same', kernel_initializer = 'he_normal')(relu4_1)\n",
    "    bn4_2 = BatchNormalization(axis=3)(conv4_2)\n",
    "    relu4_2 = Activation('relu')(bn4_2)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(relu4_2)\n",
    "    \n",
    "    conv5_1 = Conv2D(filters*16, (3, 3), padding='same', kernel_initializer = 'he_normal')(pool4)\n",
    "    bn5_1 = BatchNormalization(axis=3)(conv5_1)\n",
    "    relu5_1 = Activation('relu')(bn5_1)\n",
    "    conv5_2 = Conv2D(filters*16, (3, 3), padding='same', kernel_initializer = 'he_normal')(relu5_1)\n",
    "    bn5_2 = BatchNormalization(axis=3)(conv5_2)\n",
    "    relu5_2 = Activation('relu')(bn5_2)\n",
    "    \n",
    "    up6 = Concatenate()([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(relu5_2), relu4_2])\n",
    "    conv6_1 = Conv2D(filters*8, (3, 3), padding='same', kernel_initializer = 'he_normal')(up6)\n",
    "    bn6_1 = BatchNormalization(axis=3)(conv6_1)\n",
    "    relu6_1 = Activation('relu')(bn6_1)\n",
    "    conv6_2 = Conv2D(filters*8, (3, 3), padding='same', kernel_initializer = 'he_normal')(relu6_1)\n",
    "    bn6_2 = BatchNormalization(axis=3)(conv6_2)\n",
    "    relu6_2 = Activation('relu')(bn6_2)\n",
    "    \n",
    "    up7 = Concatenate()([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(relu6_2), relu3_2])\n",
    "    conv7_1 = Conv2D(filters*4, (3, 3), padding='same', kernel_initializer = 'he_normal')(up7)\n",
    "    bn7_1 = BatchNormalization(axis=3)(conv7_1)\n",
    "    relu7_1 = Activation('relu')(bn7_1)\n",
    "    conv7_2 = Conv2D(filters*4, (3, 3), padding='same', kernel_initializer = 'he_normal')(relu7_1)\n",
    "    bn7_2 = BatchNormalization(axis=3)(conv7_2)\n",
    "    relu7_2 = Activation('relu')(bn7_2)\n",
    "    \n",
    "    up8 = Concatenate()([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(relu7_2), relu2_2])\n",
    "    conv8_1 = Conv2D(filters*2, (3, 3), padding='same', kernel_initializer = 'he_normal')(up8)\n",
    "    bn8_1 = BatchNormalization(axis=3)(conv8_1)\n",
    "    relu8_1 = Activation('relu')(bn8_1)\n",
    "    conv8_2 = Conv2D(filters*2, (3, 3), padding='same', kernel_initializer = 'he_normal')(relu8_1)\n",
    "    bn8_2 = BatchNormalization(axis=3)(conv8_2)\n",
    "    relu8_2 = Activation('relu')(bn8_2)\n",
    "    \n",
    "    up9 = Concatenate()([Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(relu8_2), relu1_2])\n",
    "    conv9_1 = Conv2D(filters, (3, 3), padding='same', kernel_initializer = 'he_normal')(up9)\n",
    "    bn9_1 = BatchNormalization(axis=3)(conv9_1)\n",
    "    relu9_1 = Activation('relu')(bn9_1)\n",
    "    conv9_2 = Conv2D(filters, (3, 3), padding='same', kernel_initializer = 'he_normal')(relu9_1)\n",
    "    bn9_2 = BatchNormalization(axis=3)(conv9_2)\n",
    "    relu9_2 = Activation('relu')(bn9_2)\n",
    "    \n",
    "    conv10 = Conv2D(1, (1, 1), activation='sigmoid')(relu9_2)\n",
    "    \n",
    "    model = Model(inputs=[inputs], outputs=[conv10])\n",
    "    model.compile(#tf.keras.optimizers.SGD(learning_rate=lr, momentum=0.0, nesterov=False, name='SGD'), \n",
    "                 tf.keras.optimizers.Adam(learning_rate=lr), \n",
    "                 loss=loss, metrics=metrics)\n",
    "    \n",
    "    print(model.summary())\n",
    "\n",
    "    if(pretrained_weights):\n",
    "        model.load_weights(pretrained_weights)\n",
    "          \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "K.set_image_data_format('channels_last')  # TF dimension ordering in this code\n",
    "kinit = 'glorot_normal'\n",
    "\n",
    "def expend_as(tensor, rep,name):\n",
    "\tmy_repeat = Lambda(lambda x, repnum: K.repeat_elements(x, repnum, axis=3), arguments={'repnum': rep},  name='psi_up'+name)(tensor)\n",
    "\treturn my_repeat\n",
    "\n",
    "\n",
    "def AttnGatingBlock(x, g, inter_shape, name):\n",
    "    \n",
    "    ''' take g which is the spatially smaller signal, do a conv to get the same\n",
    "    number of feature channels as x (bigger spatially)\n",
    "    do a conv on x to also get same geature channels (theta_x)\n",
    "    then, upsample g to be same size as x \n",
    "    add x and g (concat_xg)\n",
    "    relu, 1x1 conv, then sigmoid then upsample the final - this gives us attn coefficients'''\n",
    "    \n",
    "    shape_x = K.int_shape(x)  # 32\n",
    "    shape_g = K.int_shape(g)  # 16\n",
    "\n",
    "    theta_x = Conv2D(inter_shape, (2, 2), strides=(2, 2), padding='same', name='xl'+name)(x)  # 16\n",
    "    shape_theta_x = K.int_shape(theta_x)\n",
    "\n",
    "    phi_g = Conv2D(inter_shape, (1, 1), padding='same')(g)\n",
    "    upsample_g = Conv2DTranspose(inter_shape, (3, 3),strides=(shape_theta_x[1] // shape_g[1], shape_theta_x[2] // shape_g[2]),padding='same', name='g_up'+name)(phi_g)  # 16\n",
    "\n",
    "    concat_xg = add([upsample_g, theta_x])\n",
    "    act_xg = Activation('relu')(concat_xg)\n",
    "    psi = Conv2D(1, (1, 1), padding='same', name='psi'+name)(act_xg)\n",
    "    sigmoid_xg = Activation('sigmoid')(psi)\n",
    "    shape_sigmoid = K.int_shape(sigmoid_xg)\n",
    "    upsample_psi = UpSampling2D(size=(shape_x[1] // shape_sigmoid[1], shape_x[2] // shape_sigmoid[2]))(sigmoid_xg)  # 32\n",
    "\n",
    "    upsample_psi = expend_as(upsample_psi, shape_x[3],  name)\n",
    "    y = multiply([upsample_psi, x], name='q_attn'+name)\n",
    "\n",
    "    result = Conv2D(shape_x[3], (1, 1), padding='same',name='q_attn_conv'+name)(y)\n",
    "    result_bn = BatchNormalization(name='q_attn_bn'+name)(result)\n",
    "    return result_bn\n",
    "\n",
    "def UnetConv2D(input, outdim, is_batchnorm, name):\n",
    "\tx = Conv2D(outdim, (3, 3), strides=(1, 1), kernel_initializer=kinit, padding=\"same\", name=name+'_1')(input)\n",
    "\tif is_batchnorm:\n",
    "\t\tx =BatchNormalization(name=name + '_1_bn')(x)\n",
    "\tx = Activation('relu',name=name + '_1_act')(x)\n",
    "\n",
    "\tx = Conv2D(outdim, (3, 3), strides=(1, 1), kernel_initializer=kinit, padding=\"same\", name=name+'_2')(x)\n",
    "\tif is_batchnorm:\n",
    "\t\tx = BatchNormalization(name=name + '_2_bn')(x)\n",
    "\tx = Activation('relu', name=name + '_2_act')(x)\n",
    "\treturn x\n",
    "\t\n",
    "\n",
    "def UnetGatingSignal(input, is_batchnorm, name):\n",
    "    ''' this is simply 1x1 convolution, bn, activation '''\n",
    "    shape = K.int_shape(input)\n",
    "    x = Conv2D(shape[3] * 1, (1, 1), strides=(1, 1), padding=\"same\",  kernel_initializer=kinit, name=name + '_conv')(input)\n",
    "    if is_batchnorm:\n",
    "        x = BatchNormalization(name=name + '_bn')(x)\n",
    "    x = Activation('relu', name = name + '_act')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attn_unet(loss, lr, filters, pretrained_weights = None, input_size = (PATCHSIZE, PATCHSIZE, NBANDS)):\n",
    "    inputs = Input(shape=input_size)\n",
    "    conv1 = UnetConv2D(inputs, filters, is_batchnorm=True, name='conv1')\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    \n",
    "    conv2 = UnetConv2D(pool1, filters, is_batchnorm=True, name='conv2')\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = UnetConv2D(pool2, filters*2, is_batchnorm=True, name='conv3')\n",
    "    conv3 = Dropout(0.2,name='drop_conv3')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = UnetConv2D(pool3, filters*2, is_batchnorm=True, name='conv4')\n",
    "    conv4 = Dropout(0.2, name='drop_conv4')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    \n",
    "    center = UnetConv2D(pool4, filters*4, is_batchnorm=True, name='center')\n",
    "    \n",
    "    g1 = UnetGatingSignal(center, is_batchnorm=True, name='g1')\n",
    "    attn1 = AttnGatingBlock(conv4, g1, filters*4, '_1')\n",
    "    up1 = concatenate([Conv2DTranspose(filters*2, (3,3), strides=(2,2), padding='same', activation='relu', kernel_initializer=kinit)(center), attn1], name='up1')\n",
    "    \n",
    "    g2 = UnetGatingSignal(up1, is_batchnorm=True, name='g2')\n",
    "    attn2 = AttnGatingBlock(conv3, g2, filters*2, '_2')\n",
    "    up2 = concatenate([Conv2DTranspose(filters*2, (3,3), strides=(2,2), padding='same', activation='relu', kernel_initializer=kinit)(up1), attn2], name='up2')\n",
    "\n",
    "    g3 = UnetGatingSignal(up1, is_batchnorm=True, name='g3')\n",
    "    attn3 = AttnGatingBlock(conv2, g3, filters*2, '_3')\n",
    "    up3 = concatenate([Conv2DTranspose(filters*2, (3,3), strides=(2,2), padding='same', activation='relu', kernel_initializer=kinit)(up2), attn3], name='up3')\n",
    "\n",
    "    up4 = concatenate([Conv2DTranspose(filters*2, (3,3), strides=(2,2), padding='same', activation='relu', kernel_initializer=kinit)(up3), conv1], name='up4')\n",
    "    out = Conv2D(1, (1, 1), activation='sigmoid',  kernel_initializer=kinit, name='final')(up4)\n",
    "    \n",
    "    model = Model(inputs=[inputs], outputs=[out])\n",
    "    model.compile(tf.keras.optimizers.SGD(learning_rate=lr, momentum=0.0, nesterov=False, name='SGD'), \n",
    "                #optimizer=tf.keras.optimizers.Adam(learning_rate=lr), \n",
    "                 loss=loss, metrics=metrics)\n",
    "                 \n",
    "    print(model.summary())\n",
    "    \n",
    "    if(pretrained_weights):\n",
    "        model.load_weights(pretrained_weights)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCr3n2WYvFWF"
   },
   "source": [
    "## Training the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4803157,
     "status": "ok",
     "timestamp": 1619203324726,
     "user": {
      "displayName": "Kushanav Bhuyan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhpJDzYajMCPL_21Vep_aig5cbphA73G195JQkf=s64",
      "userId": "13534778218088456884"
     },
     "user_tz": -120
    },
    "id": "TDVxeZmhzLZO",
    "outputId": "de8bba98-dfa3-4137-e57d-889d62dbf4e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 128, 128, 5) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 128, 128, 8)  368         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 128, 128, 8)  32          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 128, 128, 8)  0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 8)  584         activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 8)  32          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, 128, 8)  0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 64, 64, 8)    0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 16)   1168        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64, 64, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 64, 64, 16)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 64, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64, 64, 16)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 16)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 32)   4640        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 32)   128         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 32)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 32)   9248        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 32)   128         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 32)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 32)   0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 16, 16, 64)   18496       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 16, 16, 64)   256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 16, 16, 64)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 16, 16, 64)   36928       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 16, 16, 64)   256         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16, 16, 64)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 64)     0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 8, 8, 128)    73856       max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 8, 8, 128)    512         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 8, 8, 128)    0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 8, 8, 128)    147584      activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 8, 8, 128)    512         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 8, 8, 128)    0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 16, 16, 128)  65664       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 16, 16, 192)  0           conv2d_transpose[0][0]           \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 64)   110656      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 64)   256         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 64)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 64)   36928       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 64)   256         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 64)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 32, 32, 64)   16448       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 96)   0           conv2d_transpose_1[0][0]         \n",
      "                                                                 activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 32)   27680       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 32, 32, 32)   128         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 32, 32, 32)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 32, 32)   9248        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 32, 32, 32)   128         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 32, 32, 32)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 64, 64, 32)   4128        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 64, 64, 48)   0           conv2d_transpose_2[0][0]         \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 16)   6928        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64, 64, 16)   64          conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 64, 64, 16)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 16)   2320        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 64, 64, 16)   64          conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 64, 64, 16)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 128, 128, 16) 1040        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 128, 128, 24) 0           conv2d_transpose_3[0][0]         \n",
      "                                                                 activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 128, 128, 8)  1736        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 128, 128, 8)  32          conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 128, 128, 8)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 128, 128, 8)  584         activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 128, 128, 8)  32          conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 128, 128, 8)  0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 128, 128, 1)  9           activation_17[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 581,505\n",
      "Trainable params: 580,033\n",
      "Non-trainable params: 1,472\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "Initialising in 3..........2..........1......................................\n",
      "Training on 284 images with learning rate 0.001, batch size 8, and number of filters 8\n",
      "Epoch 1/100\n",
      "32/32 - 48s - loss: 0.9160 - accuracy: 0.2150 - precision_m: 0.0658 - recall_m: 0.9530 - f1_m: 0.1212 - val_loss: 0.9406 - val_accuracy: 0.4378 - val_precision_m: 0.0491 - val_recall_m: 0.7755 - val_f1_m: 0.0913\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.63364\n",
      "Epoch 2/100\n",
      "32/32 - 1s - loss: 0.9047 - accuracy: 0.4111 - precision_m: 0.0767 - recall_m: 0.8431 - f1_m: 0.1390 - val_loss: 0.9332 - val_accuracy: 0.4105 - val_precision_m: 0.0525 - val_recall_m: 0.8231 - val_f1_m: 0.0978\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.63364\n",
      "Epoch 3/100\n",
      "32/32 - 1s - loss: 0.9013 - accuracy: 0.5364 - precision_m: 0.0823 - recall_m: 0.7046 - f1_m: 0.1449 - val_loss: 0.9325 - val_accuracy: 0.4058 - val_precision_m: 0.0521 - val_recall_m: 0.8284 - val_f1_m: 0.0972\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.63364\n",
      "Epoch 4/100\n",
      "32/32 - 1s - loss: 0.8967 - accuracy: 0.5495 - precision_m: 0.0858 - recall_m: 0.6895 - f1_m: 0.1496 - val_loss: 0.9319 - val_accuracy: 0.3952 - val_precision_m: 0.0517 - val_recall_m: 0.8360 - val_f1_m: 0.0966\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.63364\n",
      "Epoch 5/100\n",
      "32/32 - 1s - loss: 0.8796 - accuracy: 0.6437 - precision_m: 0.1092 - recall_m: 0.7093 - f1_m: 0.1855 - val_loss: 0.9477 - val_accuracy: 0.9607 - val_precision_m: 0.0347 - val_recall_m: 3.9277e-04 - val_f1_m: 7.7603e-04\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.63364\n",
      "Epoch 6/100\n",
      "32/32 - 1s - loss: 0.8478 - accuracy: 0.7253 - precision_m: 0.1531 - recall_m: 0.8402 - f1_m: 0.2523 - val_loss: 0.9466 - val_accuracy: 0.9609 - val_precision_m: 0.0389 - val_recall_m: 2.1453e-04 - val_f1_m: 4.2660e-04\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.63364\n",
      "Epoch 7/100\n",
      "32/32 - 1s - loss: 0.8315 - accuracy: 0.7633 - precision_m: 0.1682 - recall_m: 0.8275 - f1_m: 0.2732 - val_loss: 0.9455 - val_accuracy: 0.9577 - val_precision_m: 0.0539 - val_recall_m: 0.0117 - val_f1_m: 0.0192\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.63364\n",
      "Epoch 8/100\n",
      "32/32 - 1s - loss: 0.8151 - accuracy: 0.7897 - precision_m: 0.1874 - recall_m: 0.8006 - f1_m: 0.2936 - val_loss: 0.9502 - val_accuracy: 0.9606 - val_precision_m: 0.0133 - val_recall_m: 1.9165e-04 - val_f1_m: 3.7722e-04\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.63364\n",
      "Epoch 9/100\n",
      "32/32 - 1s - loss: 0.8055 - accuracy: 0.8300 - precision_m: 0.2009 - recall_m: 0.6944 - f1_m: 0.3018 - val_loss: 0.9443 - val_accuracy: 0.9609 - val_precision_m: 0.0068 - val_recall_m: 3.4176e-05 - val_f1_m: 6.8008e-05\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.63364\n",
      "Epoch 10/100\n",
      "32/32 - 1s - loss: 0.7845 - accuracy: 0.8283 - precision_m: 0.2140 - recall_m: 0.7606 - f1_m: 0.3242 - val_loss: 0.9403 - val_accuracy: 0.8549 - val_precision_m: 0.0963 - val_recall_m: 0.3359 - val_f1_m: 0.1472\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.63364\n",
      "Epoch 11/100\n",
      "32/32 - 1s - loss: 0.7578 - accuracy: 0.8545 - precision_m: 0.2500 - recall_m: 0.7179 - f1_m: 0.3594 - val_loss: 0.9441 - val_accuracy: 0.9596 - val_precision_m: 0.0468 - val_recall_m: 0.0015 - val_f1_m: 0.0029\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.63364\n",
      "Epoch 12/100\n",
      "32/32 - 1s - loss: 0.7445 - accuracy: 0.8766 - precision_m: 0.2696 - recall_m: 0.6682 - f1_m: 0.3735 - val_loss: 0.9424 - val_accuracy: 0.9452 - val_precision_m: 0.0749 - val_recall_m: 0.0381 - val_f1_m: 0.0489\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.63364\n",
      "Epoch 13/100\n",
      "32/32 - 1s - loss: 0.7329 - accuracy: 0.8881 - precision_m: 0.2857 - recall_m: 0.6207 - f1_m: 0.3711 - val_loss: 0.9589 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.63364\n",
      "Epoch 14/100\n",
      "32/32 - 1s - loss: 0.7089 - accuracy: 0.8836 - precision_m: 0.2826 - recall_m: 0.7001 - f1_m: 0.3873 - val_loss: 0.9603 - val_accuracy: 0.9609 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.63364\n",
      "Epoch 15/100\n",
      "32/32 - 1s - loss: 0.6953 - accuracy: 0.9097 - precision_m: 0.3347 - recall_m: 0.5957 - f1_m: 0.4082 - val_loss: 0.9695 - val_accuracy: 0.9476 - val_precision_m: 0.0233 - val_recall_m: 0.0081 - val_f1_m: 0.0120\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.63364\n",
      "Epoch 16/100\n",
      "32/32 - 1s - loss: 0.6798 - accuracy: 0.9209 - precision_m: 0.3602 - recall_m: 0.5062 - f1_m: 0.4012 - val_loss: 0.9670 - val_accuracy: 0.9599 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.63364\n",
      "Epoch 17/100\n",
      "32/32 - 1s - loss: 0.6354 - accuracy: 0.9250 - precision_m: 0.3863 - recall_m: 0.6032 - f1_m: 0.4522 - val_loss: 0.9736 - val_accuracy: 0.9604 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.63364\n",
      "Epoch 18/100\n",
      "32/32 - 1s - loss: 0.6640 - accuracy: 0.9059 - precision_m: 0.3276 - recall_m: 0.5933 - f1_m: 0.4095 - val_loss: 0.9714 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.63364\n",
      "Epoch 19/100\n",
      "32/32 - 1s - loss: 0.5985 - accuracy: 0.9334 - precision_m: 0.4366 - recall_m: 0.5769 - f1_m: 0.4688 - val_loss: 0.9617 - val_accuracy: 0.9436 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.63364\n",
      "Epoch 20/100\n",
      "32/32 - 1s - loss: 0.5622 - accuracy: 0.9370 - precision_m: 0.4533 - recall_m: 0.6149 - f1_m: 0.5037 - val_loss: 0.9908 - val_accuracy: 0.7842 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.63364\n",
      "Epoch 21/100\n",
      "32/32 - 1s - loss: 0.5341 - accuracy: 0.9448 - precision_m: 0.4974 - recall_m: 0.5729 - f1_m: 0.5197 - val_loss: 0.9921 - val_accuracy: 0.7083 - val_precision_m: 0.0026 - val_recall_m: 0.0073 - val_f1_m: 0.0038\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.63364\n",
      "Epoch 22/100\n",
      "32/32 - 1s - loss: 0.5196 - accuracy: 0.9476 - precision_m: 0.5166 - recall_m: 0.5847 - f1_m: 0.5292 - val_loss: 0.6603 - val_accuracy: 0.9469 - val_precision_m: 0.3989 - val_recall_m: 0.4716 - val_f1_m: 0.4191\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.63364\n",
      "Epoch 23/100\n",
      "32/32 - 1s - loss: 0.4692 - accuracy: 0.9534 - precision_m: 0.5885 - recall_m: 0.6002 - f1_m: 0.5807 - val_loss: 0.9188 - val_accuracy: 0.9357 - val_precision_m: 0.1021 - val_recall_m: 0.0427 - val_f1_m: 0.0602\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.63364\n",
      "Epoch 24/100\n",
      "32/32 - 1s - loss: 0.5003 - accuracy: 0.9507 - precision_m: 0.5522 - recall_m: 0.5586 - f1_m: 0.5369 - val_loss: 0.9821 - val_accuracy: 0.6459 - val_precision_m: 0.0106 - val_recall_m: 0.0513 - val_f1_m: 0.0173\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.63364\n",
      "Epoch 25/100\n",
      "32/32 - 1s - loss: 0.4872 - accuracy: 0.9514 - precision_m: 0.5534 - recall_m: 0.5877 - f1_m: 0.5496 - val_loss: 0.6898 - val_accuracy: 0.9554 - val_precision_m: 0.4194 - val_recall_m: 0.2692 - val_f1_m: 0.3145\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.63364\n",
      "Epoch 26/100\n",
      "32/32 - 1s - loss: 0.4515 - accuracy: 0.9565 - precision_m: 0.6113 - recall_m: 0.5899 - f1_m: 0.5811 - val_loss: 0.6390 - val_accuracy: 0.9556 - val_precision_m: 0.4427 - val_recall_m: 0.3574 - val_f1_m: 0.3797\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.63364\n",
      "Epoch 27/100\n",
      "32/32 - 1s - loss: 0.4695 - accuracy: 0.9542 - precision_m: 0.6079 - recall_m: 0.5691 - f1_m: 0.5566 - val_loss: 0.6667 - val_accuracy: 0.9541 - val_precision_m: 0.4401 - val_recall_m: 0.3160 - val_f1_m: 0.3460\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.63364\n",
      "Epoch 28/100\n",
      "32/32 - 1s - loss: 0.4338 - accuracy: 0.9573 - precision_m: 0.6430 - recall_m: 0.5629 - f1_m: 0.5835 - val_loss: 0.9912 - val_accuracy: 0.9248 - val_precision_m: 7.6748e-05 - val_recall_m: 2.7473e-04 - val_f1_m: 1.1997e-04\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.63364\n",
      "Epoch 29/100\n",
      "32/32 - 1s - loss: 0.4265 - accuracy: 0.9581 - precision_m: 0.6482 - recall_m: 0.5704 - f1_m: 0.5898 - val_loss: 0.9949 - val_accuracy: 0.9210 - val_precision_m: 5.8038e-05 - val_recall_m: 2.1978e-04 - val_f1_m: 9.1819e-05\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.63364\n",
      "Epoch 30/100\n",
      "32/32 - 1s - loss: 0.4181 - accuracy: 0.9585 - precision_m: 0.6439 - recall_m: 0.5819 - f1_m: 0.5964 - val_loss: 0.9609 - val_accuracy: 0.7934 - val_precision_m: 0.0323 - val_recall_m: 0.1860 - val_f1_m: 0.0501\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.63364\n",
      "Epoch 31/100\n",
      "32/32 - 1s - loss: 0.4365 - accuracy: 0.9575 - precision_m: 0.6492 - recall_m: 0.5451 - f1_m: 0.5716 - val_loss: 0.9967 - val_accuracy: 0.5684 - val_precision_m: 0.0013 - val_recall_m: 0.0169 - val_f1_m: 0.0024\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.63364\n",
      "Epoch 32/100\n",
      "32/32 - 1s - loss: 0.3721 - accuracy: 0.9636 - precision_m: 0.7082 - recall_m: 0.6094 - f1_m: 0.6381 - val_loss: 0.9973 - val_accuracy: 0.6937 - val_precision_m: 0.0014 - val_recall_m: 0.0184 - val_f1_m: 0.0025\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.63364\n",
      "Epoch 33/100\n",
      "32/32 - 1s - loss: 0.3845 - accuracy: 0.9624 - precision_m: 0.7229 - recall_m: 0.5555 - f1_m: 0.6123 - val_loss: 0.5959 - val_accuracy: 0.9647 - val_precision_m: 0.6106 - val_recall_m: 0.3284 - val_f1_m: 0.3978\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.63364 to 0.59585, saving model to Data/Checkpoints\\weights.33-0.60.hdf5\n",
      "Epoch 34/100\n",
      "32/32 - 1s - loss: 0.3992 - accuracy: 0.9611 - precision_m: 0.6842 - recall_m: 0.5633 - f1_m: 0.6013 - val_loss: 0.9933 - val_accuracy: 0.4979 - val_precision_m: 0.0045 - val_recall_m: 0.0664 - val_f1_m: 0.0084\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.59585\n",
      "Epoch 35/100\n",
      "32/32 - 1s - loss: 0.3812 - accuracy: 0.9624 - precision_m: 0.6883 - recall_m: 0.6023 - f1_m: 0.6226 - val_loss: 0.9817 - val_accuracy: 0.6859 - val_precision_m: 0.0149 - val_recall_m: 0.1328 - val_f1_m: 0.0250\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.59585\n",
      "Epoch 36/100\n",
      "32/32 - 1s - loss: 0.3519 - accuracy: 0.9651 - precision_m: 0.7279 - recall_m: 0.6019 - f1_m: 0.6464 - val_loss: 0.9991 - val_accuracy: 0.6175 - val_precision_m: 2.5774e-04 - val_recall_m: 0.0031 - val_f1_m: 4.7604e-04\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.59585\n",
      "Epoch 37/100\n",
      "32/32 - 1s - loss: 0.3182 - accuracy: 0.9680 - precision_m: 0.7668 - recall_m: 0.6122 - f1_m: 0.6734 - val_loss: 0.9964 - val_accuracy: 0.6093 - val_precision_m: 0.0022 - val_recall_m: 0.0201 - val_f1_m: 0.0039\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.59585\n",
      "Epoch 38/100\n",
      "32/32 - 1s - loss: 0.3555 - accuracy: 0.9647 - precision_m: 0.7239 - recall_m: 0.5918 - f1_m: 0.6375 - val_loss: 0.9879 - val_accuracy: 0.4454 - val_precision_m: 0.0082 - val_recall_m: 0.1284 - val_f1_m: 0.0152\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.59585\n",
      "Epoch 39/100\n",
      "32/32 - 1s - loss: 0.3353 - accuracy: 0.9660 - precision_m: 0.7400 - recall_m: 0.6184 - f1_m: 0.6598 - val_loss: 0.9597 - val_accuracy: 0.3941 - val_precision_m: 0.0271 - val_recall_m: 0.3986 - val_f1_m: 0.0502\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.59585\n",
      "Epoch 40/100\n",
      "32/32 - 1s - loss: 0.3443 - accuracy: 0.9651 - precision_m: 0.7316 - recall_m: 0.6077 - f1_m: 0.6474 - val_loss: 0.9825 - val_accuracy: 0.4249 - val_precision_m: 0.0113 - val_recall_m: 0.1686 - val_f1_m: 0.0210\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.59585\n",
      "Epoch 41/100\n",
      "32/32 - 1s - loss: 0.3058 - accuracy: 0.9694 - precision_m: 0.7648 - recall_m: 0.6413 - f1_m: 0.6871 - val_loss: 0.8251 - val_accuracy: 0.8964 - val_precision_m: 0.1494 - val_recall_m: 0.3513 - val_f1_m: 0.2069\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.59585\n",
      "Epoch 42/100\n",
      "32/32 - 1s - loss: 0.3244 - accuracy: 0.9676 - precision_m: 0.7600 - recall_m: 0.6158 - f1_m: 0.6648 - val_loss: 0.9660 - val_accuracy: 0.7289 - val_precision_m: 0.0237 - val_recall_m: 0.0888 - val_f1_m: 0.0368\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.59585\n",
      "Epoch 43/100\n",
      "32/32 - 1s - loss: 0.3158 - accuracy: 0.9676 - precision_m: 0.7744 - recall_m: 0.6129 - f1_m: 0.6697 - val_loss: 0.9988 - val_accuracy: 0.6956 - val_precision_m: 3.7077e-04 - val_recall_m: 0.0032 - val_f1_m: 6.5775e-04\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.59585\n",
      "Epoch 44/100\n",
      "32/32 - 1s - loss: 0.3236 - accuracy: 0.9674 - precision_m: 0.7696 - recall_m: 0.6022 - f1_m: 0.6578 - val_loss: 0.9705 - val_accuracy: 0.6932 - val_precision_m: 0.0204 - val_recall_m: 0.1068 - val_f1_m: 0.0332\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.59585\n",
      "Epoch 45/100\n",
      "32/32 - 1s - loss: 0.2966 - accuracy: 0.9691 - precision_m: 0.7898 - recall_m: 0.6318 - f1_m: 0.6874 - val_loss: 0.9291 - val_accuracy: 0.7167 - val_precision_m: 0.0522 - val_recall_m: 0.3289 - val_f1_m: 0.0870\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.59585\n",
      "Epoch 46/100\n",
      "32/32 - 1s - loss: 0.2852 - accuracy: 0.9707 - precision_m: 0.8000 - recall_m: 0.6343 - f1_m: 0.6956 - val_loss: 0.8211 - val_accuracy: 0.9597 - val_precision_m: 0.4510 - val_recall_m: 0.1063 - val_f1_m: 0.1393\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.59585\n",
      "Epoch 47/100\n",
      "32/32 - 1s - loss: 0.2833 - accuracy: 0.9707 - precision_m: 0.8013 - recall_m: 0.6315 - f1_m: 0.6957 - val_loss: 0.6311 - val_accuracy: 0.9611 - val_precision_m: 0.5915 - val_recall_m: 0.2652 - val_f1_m: 0.3244\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.59585\n",
      "Epoch 48/100\n",
      "32/32 - 1s - loss: 0.3136 - accuracy: 0.9679 - precision_m: 0.7638 - recall_m: 0.6206 - f1_m: 0.6705 - val_loss: 0.9806 - val_accuracy: 0.5942 - val_precision_m: 0.0125 - val_recall_m: 0.0816 - val_f1_m: 0.0213\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.59585\n",
      "Epoch 49/100\n",
      "32/32 - 1s - loss: 0.2500 - accuracy: 0.9736 - precision_m: 0.8226 - recall_m: 0.6762 - f1_m: 0.7326 - val_loss: 0.9567 - val_accuracy: 0.5997 - val_precision_m: 0.0303 - val_recall_m: 0.2463 - val_f1_m: 0.0525\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.59585\n",
      "Epoch 50/100\n",
      "32/32 - 1s - loss: 0.2455 - accuracy: 0.9747 - precision_m: 0.8279 - recall_m: 0.6779 - f1_m: 0.7372 - val_loss: 0.9315 - val_accuracy: 0.7489 - val_precision_m: 0.0516 - val_recall_m: 0.2749 - val_f1_m: 0.0833\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.59585\n",
      "Epoch 51/100\n",
      "32/32 - 1s - loss: 0.2566 - accuracy: 0.9736 - precision_m: 0.8315 - recall_m: 0.6457 - f1_m: 0.7200 - val_loss: 0.9209 - val_accuracy: 0.7747 - val_precision_m: 0.0601 - val_recall_m: 0.3065 - val_f1_m: 0.0965\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.59585\n",
      "Epoch 52/100\n",
      "32/32 - 1s - loss: 0.2536 - accuracy: 0.9748 - precision_m: 0.8224 - recall_m: 0.6708 - f1_m: 0.7294 - val_loss: 0.6427 - val_accuracy: 0.9627 - val_precision_m: 0.5759 - val_recall_m: 0.2657 - val_f1_m: 0.3407\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.59585\n",
      "Epoch 53/100\n",
      "32/32 - 1s - loss: 0.2320 - accuracy: 0.9754 - precision_m: 0.8533 - recall_m: 0.6753 - f1_m: 0.7446 - val_loss: 0.5544 - val_accuracy: 0.9650 - val_precision_m: 0.6446 - val_recall_m: 0.3110 - val_f1_m: 0.3946\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.59585 to 0.55443, saving model to Data/Checkpoints\\weights.53-0.55.hdf5\n",
      "Epoch 54/100\n",
      "32/32 - 1s - loss: 0.2393 - accuracy: 0.9753 - precision_m: 0.8456 - recall_m: 0.6764 - f1_m: 0.7386 - val_loss: 0.9418 - val_accuracy: 0.7446 - val_precision_m: 0.0437 - val_recall_m: 0.2370 - val_f1_m: 0.0707\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.55443\n",
      "Epoch 55/100\n",
      "32/32 - 1s - loss: 0.2404 - accuracy: 0.9744 - precision_m: 0.8414 - recall_m: 0.6742 - f1_m: 0.7404 - val_loss: 0.9746 - val_accuracy: 0.5220 - val_precision_m: 0.0180 - val_recall_m: 0.2249 - val_f1_m: 0.0326\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.55443\n",
      "Epoch 56/100\n",
      "32/32 - 1s - loss: 0.2218 - accuracy: 0.9759 - precision_m: 0.8660 - recall_m: 0.6777 - f1_m: 0.7537 - val_loss: 0.9549 - val_accuracy: 0.4442 - val_precision_m: 0.0326 - val_recall_m: 0.4710 - val_f1_m: 0.0600\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.55443\n",
      "Epoch 57/100\n",
      "32/32 - 1s - loss: 0.2439 - accuracy: 0.9753 - precision_m: 0.8342 - recall_m: 0.6776 - f1_m: 0.7354 - val_loss: 0.9153 - val_accuracy: 0.7877 - val_precision_m: 0.0650 - val_recall_m: 0.3057 - val_f1_m: 0.1029\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.55443\n",
      "Epoch 58/100\n",
      "32/32 - 1s - loss: 0.2145 - accuracy: 0.9768 - precision_m: 0.8545 - recall_m: 0.7075 - f1_m: 0.7672 - val_loss: 0.6629 - val_accuracy: 0.9598 - val_precision_m: 0.5275 - val_recall_m: 0.2354 - val_f1_m: 0.2941\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.55443\n",
      "Epoch 59/100\n",
      "32/32 - 1s - loss: 0.2167 - accuracy: 0.9767 - precision_m: 0.8722 - recall_m: 0.6811 - f1_m: 0.7568 - val_loss: 0.5513 - val_accuracy: 0.9624 - val_precision_m: 0.5955 - val_recall_m: 0.3528 - val_f1_m: 0.4188\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.55443 to 0.55133, saving model to Data/Checkpoints\\weights.59-0.55.hdf5\n",
      "Epoch 60/100\n",
      "32/32 - 1s - loss: 0.2331 - accuracy: 0.9757 - precision_m: 0.8630 - recall_m: 0.6632 - f1_m: 0.7408 - val_loss: 0.5383 - val_accuracy: 0.9624 - val_precision_m: 0.5737 - val_recall_m: 0.3751 - val_f1_m: 0.4340\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.55133 to 0.53828, saving model to Data/Checkpoints\\weights.60-0.54.hdf5\n",
      "Epoch 61/100\n",
      "32/32 - 1s - loss: 0.2398 - accuracy: 0.9754 - precision_m: 0.8358 - recall_m: 0.6834 - f1_m: 0.7399 - val_loss: 0.8556 - val_accuracy: 0.8902 - val_precision_m: 0.1218 - val_recall_m: 0.3025 - val_f1_m: 0.1698\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.53828\n",
      "Epoch 62/100\n",
      "32/32 - 1s - loss: 0.2465 - accuracy: 0.9742 - precision_m: 0.8372 - recall_m: 0.6625 - f1_m: 0.7302 - val_loss: 0.9622 - val_accuracy: 0.5461 - val_precision_m: 0.0263 - val_recall_m: 0.2552 - val_f1_m: 0.0466\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.53828\n",
      "Epoch 63/100\n",
      "32/32 - 1s - loss: 0.2243 - accuracy: 0.9759 - precision_m: 0.8627 - recall_m: 0.6803 - f1_m: 0.7504 - val_loss: 0.9602 - val_accuracy: 0.4764 - val_precision_m: 0.0290 - val_recall_m: 0.3993 - val_f1_m: 0.0530\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.53828\n",
      "Epoch 64/100\n",
      "32/32 - 1s - loss: 0.2252 - accuracy: 0.9751 - precision_m: 0.8667 - recall_m: 0.6685 - f1_m: 0.7467 - val_loss: 0.9622 - val_accuracy: 0.4534 - val_precision_m: 0.0276 - val_recall_m: 0.3985 - val_f1_m: 0.0507\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.53828\n",
      "Epoch 65/100\n",
      "32/32 - 1s - loss: 0.2030 - accuracy: 0.9777 - precision_m: 0.8818 - recall_m: 0.6912 - f1_m: 0.7707 - val_loss: 0.9406 - val_accuracy: 0.6371 - val_precision_m: 0.0429 - val_recall_m: 0.3637 - val_f1_m: 0.0747\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.53828\n",
      "Epoch 66/100\n",
      "32/32 - 1s - loss: 0.2002 - accuracy: 0.9783 - precision_m: 0.8762 - recall_m: 0.7045 - f1_m: 0.7773 - val_loss: 0.6541 - val_accuracy: 0.9610 - val_precision_m: 0.5138 - val_recall_m: 0.2422 - val_f1_m: 0.3106\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.53828\n",
      "Epoch 67/100\n",
      "32/32 - 1s - loss: 0.1967 - accuracy: 0.9781 - precision_m: 0.8889 - recall_m: 0.6936 - f1_m: 0.7756 - val_loss: 0.6456 - val_accuracy: 0.9573 - val_precision_m: 0.4565 - val_recall_m: 0.3106 - val_f1_m: 0.3565\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.53828\n",
      "Epoch 68/100\n",
      "32/32 - 1s - loss: 0.2015 - accuracy: 0.9776 - precision_m: 0.8747 - recall_m: 0.7072 - f1_m: 0.7754 - val_loss: 0.9620 - val_accuracy: 0.6395 - val_precision_m: 0.0266 - val_recall_m: 0.1815 - val_f1_m: 0.0452\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.53828\n",
      "Epoch 69/100\n",
      "32/32 - 1s - loss: 0.1960 - accuracy: 0.9781 - precision_m: 0.8901 - recall_m: 0.6966 - f1_m: 0.7773 - val_loss: 0.9501 - val_accuracy: 0.5391 - val_precision_m: 0.0362 - val_recall_m: 0.4187 - val_f1_m: 0.0651\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.53828\n",
      "Epoch 70/100\n",
      "32/32 - 1s - loss: 0.2059 - accuracy: 0.9777 - precision_m: 0.8780 - recall_m: 0.6873 - f1_m: 0.7663 - val_loss: 0.9346 - val_accuracy: 0.7402 - val_precision_m: 0.0467 - val_recall_m: 0.2259 - val_f1_m: 0.0753\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.53828\n",
      "Epoch 71/100\n",
      "32/32 - 1s - loss: 0.2080 - accuracy: 0.9779 - precision_m: 0.8773 - recall_m: 0.6853 - f1_m: 0.7656 - val_loss: 0.6669 - val_accuracy: 0.9606 - val_precision_m: 0.5109 - val_recall_m: 0.2344 - val_f1_m: 0.3029\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.53828\n",
      "Epoch 72/100\n",
      "32/32 - 1s - loss: 0.2266 - accuracy: 0.9763 - precision_m: 0.8612 - recall_m: 0.6688 - f1_m: 0.7466 - val_loss: 0.5506 - val_accuracy: 0.9657 - val_precision_m: 0.6761 - val_recall_m: 0.2973 - val_f1_m: 0.3879\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.53828\n",
      "Epoch 73/100\n",
      "32/32 - 1s - loss: 0.2145 - accuracy: 0.9779 - precision_m: 0.8658 - recall_m: 0.6887 - f1_m: 0.7608 - val_loss: 0.5314 - val_accuracy: 0.9636 - val_precision_m: 0.5504 - val_recall_m: 0.4410 - val_f1_m: 0.4770\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.53828 to 0.53135, saving model to Data/Checkpoints\\weights.73-0.53.hdf5\n",
      "Epoch 74/100\n",
      "32/32 - 1s - loss: 0.2104 - accuracy: 0.9776 - precision_m: 0.8640 - recall_m: 0.6989 - f1_m: 0.7663 - val_loss: 0.5395 - val_accuracy: 0.9642 - val_precision_m: 0.5936 - val_recall_m: 0.3662 - val_f1_m: 0.4296\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.53135\n",
      "Epoch 75/100\n",
      "32/32 - 1s - loss: 0.1918 - accuracy: 0.9792 - precision_m: 0.8867 - recall_m: 0.7050 - f1_m: 0.7820 - val_loss: 0.8287 - val_accuracy: 0.9225 - val_precision_m: 0.1613 - val_recall_m: 0.2523 - val_f1_m: 0.1898\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.53135\n",
      "Epoch 76/100\n",
      "32/32 - 1s - loss: 0.2019 - accuracy: 0.9782 - precision_m: 0.8849 - recall_m: 0.6871 - f1_m: 0.7683 - val_loss: 0.9056 - val_accuracy: 0.8503 - val_precision_m: 0.0736 - val_recall_m: 0.1888 - val_f1_m: 0.1025\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.53135\n",
      "Epoch 77/100\n",
      "32/32 - 1s - loss: 0.2133 - accuracy: 0.9772 - precision_m: 0.8704 - recall_m: 0.6832 - f1_m: 0.7600 - val_loss: 0.9313 - val_accuracy: 0.7739 - val_precision_m: 0.0510 - val_recall_m: 0.2102 - val_f1_m: 0.0791\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.53135\n",
      "Epoch 78/100\n",
      "32/32 - 1s - loss: 0.1931 - accuracy: 0.9788 - precision_m: 0.8836 - recall_m: 0.7138 - f1_m: 0.7825 - val_loss: 0.9232 - val_accuracy: 0.8103 - val_precision_m: 0.0583 - val_recall_m: 0.1940 - val_f1_m: 0.0866\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.53135\n",
      "Epoch 79/100\n",
      "32/32 - 1s - loss: 0.1904 - accuracy: 0.9787 - precision_m: 0.8885 - recall_m: 0.7080 - f1_m: 0.7839 - val_loss: 0.9468 - val_accuracy: 0.6743 - val_precision_m: 0.0379 - val_recall_m: 0.2402 - val_f1_m: 0.0636\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.53135\n",
      "Epoch 80/100\n",
      "32/32 - 1s - loss: 0.2059 - accuracy: 0.9785 - precision_m: 0.8808 - recall_m: 0.6929 - f1_m: 0.7685 - val_loss: 0.9590 - val_accuracy: 0.5763 - val_precision_m: 0.0283 - val_recall_m: 0.2271 - val_f1_m: 0.0493\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.53135\n",
      "Epoch 81/100\n",
      "32/32 - 1s - loss: 0.1956 - accuracy: 0.9785 - precision_m: 0.8733 - recall_m: 0.7237 - f1_m: 0.7839 - val_loss: 0.9508 - val_accuracy: 0.6263 - val_precision_m: 0.0349 - val_recall_m: 0.2664 - val_f1_m: 0.0601\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.53135\n",
      "Epoch 82/100\n",
      "32/32 - 1s - loss: 0.1986 - accuracy: 0.9786 - precision_m: 0.8842 - recall_m: 0.6952 - f1_m: 0.7734 - val_loss: 0.9042 - val_accuracy: 0.7954 - val_precision_m: 0.0729 - val_recall_m: 0.3210 - val_f1_m: 0.1144\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.53135\n",
      "Epoch 83/100\n",
      "32/32 - 1s - loss: 0.1971 - accuracy: 0.9790 - precision_m: 0.8776 - recall_m: 0.7087 - f1_m: 0.7771 - val_loss: 0.5529 - val_accuracy: 0.9629 - val_precision_m: 0.5717 - val_recall_m: 0.3628 - val_f1_m: 0.4245\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.53135\n",
      "Epoch 84/100\n",
      "32/32 - 1s - loss: 0.1954 - accuracy: 0.9785 - precision_m: 0.8884 - recall_m: 0.6968 - f1_m: 0.7761 - val_loss: 0.5934 - val_accuracy: 0.9613 - val_precision_m: 0.5563 - val_recall_m: 0.3110 - val_f1_m: 0.3633\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.53135\n",
      "Epoch 85/100\n",
      "32/32 - 1s - loss: 0.1939 - accuracy: 0.9794 - precision_m: 0.8817 - recall_m: 0.7072 - f1_m: 0.7800 - val_loss: 0.8682 - val_accuracy: 0.8865 - val_precision_m: 0.1063 - val_recall_m: 0.2275 - val_f1_m: 0.1420\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.53135\n",
      "Epoch 86/100\n",
      "32/32 - 1s - loss: 0.1858 - accuracy: 0.9796 - precision_m: 0.8909 - recall_m: 0.7135 - f1_m: 0.7879 - val_loss: 0.9137 - val_accuracy: 0.8082 - val_precision_m: 0.0650 - val_recall_m: 0.2236 - val_f1_m: 0.0974\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.53135\n",
      "Epoch 87/100\n",
      "32/32 - 1s - loss: 0.1977 - accuracy: 0.9791 - precision_m: 0.8861 - recall_m: 0.6925 - f1_m: 0.7723 - val_loss: 0.5596 - val_accuracy: 0.9632 - val_precision_m: 0.5908 - val_recall_m: 0.3364 - val_f1_m: 0.4011\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.53135\n",
      "Epoch 88/100\n",
      "32/32 - 1s - loss: 0.1753 - accuracy: 0.9804 - precision_m: 0.9050 - recall_m: 0.7172 - f1_m: 0.7958 - val_loss: 0.9396 - val_accuracy: 0.6537 - val_precision_m: 0.0433 - val_recall_m: 0.3234 - val_f1_m: 0.0742\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.53135\n",
      "Epoch 89/100\n",
      "32/32 - 1s - loss: 0.1788 - accuracy: 0.9808 - precision_m: 0.8922 - recall_m: 0.7264 - f1_m: 0.7971 - val_loss: 0.7279 - val_accuracy: 0.9536 - val_precision_m: 0.3390 - val_recall_m: 0.2372 - val_f1_m: 0.2634\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.53135\n",
      "Epoch 90/100\n",
      "32/32 - 1s - loss: 0.1933 - accuracy: 0.9796 - precision_m: 0.8819 - recall_m: 0.7085 - f1_m: 0.7805 - val_loss: 0.5837 - val_accuracy: 0.9635 - val_precision_m: 0.5559 - val_recall_m: 0.3086 - val_f1_m: 0.3781\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.53135\n",
      "Epoch 91/100\n",
      "32/32 - 1s - loss: 0.1781 - accuracy: 0.9808 - precision_m: 0.9024 - recall_m: 0.7124 - f1_m: 0.7932 - val_loss: 0.5510 - val_accuracy: 0.9645 - val_precision_m: 0.6017 - val_recall_m: 0.3354 - val_f1_m: 0.4083\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.53135\n",
      "Epoch 92/100\n",
      "32/32 - 1s - loss: 0.1649 - accuracy: 0.9813 - precision_m: 0.9101 - recall_m: 0.7309 - f1_m: 0.8075 - val_loss: 0.5263 - val_accuracy: 0.9648 - val_precision_m: 0.6121 - val_recall_m: 0.3646 - val_f1_m: 0.4343\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.53135 to 0.52629, saving model to Data/Checkpoints\\weights.92-0.53.hdf5\n",
      "Epoch 93/100\n",
      "32/32 - 1s - loss: 0.1807 - accuracy: 0.9810 - precision_m: 0.8983 - recall_m: 0.7117 - f1_m: 0.7911 - val_loss: 0.5162 - val_accuracy: 0.9659 - val_precision_m: 0.6483 - val_recall_m: 0.3625 - val_f1_m: 0.4369\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.52629 to 0.51620, saving model to Data/Checkpoints\\weights.93-0.52.hdf5\n",
      "Epoch 94/100\n",
      "32/32 - 1s - loss: 0.1745 - accuracy: 0.9807 - precision_m: 0.8978 - recall_m: 0.7272 - f1_m: 0.7997 - val_loss: 0.5469 - val_accuracy: 0.9642 - val_precision_m: 0.6033 - val_recall_m: 0.3402 - val_f1_m: 0.4107\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.51620\n",
      "Epoch 95/100\n",
      "32/32 - 1s - loss: 0.1732 - accuracy: 0.9805 - precision_m: 0.9032 - recall_m: 0.7227 - f1_m: 0.7992 - val_loss: 0.5336 - val_accuracy: 0.9645 - val_precision_m: 0.6058 - val_recall_m: 0.3608 - val_f1_m: 0.4310\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.51620\n",
      "Epoch 96/100\n",
      "32/32 - 1s - loss: 0.1781 - accuracy: 0.9801 - precision_m: 0.8926 - recall_m: 0.7281 - f1_m: 0.7972 - val_loss: 0.5500 - val_accuracy: 0.9643 - val_precision_m: 0.5952 - val_recall_m: 0.3393 - val_f1_m: 0.4108\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.51620\n",
      "Epoch 97/100\n",
      "32/32 - 1s - loss: 0.1804 - accuracy: 0.9795 - precision_m: 0.9038 - recall_m: 0.7098 - f1_m: 0.7894 - val_loss: 0.5337 - val_accuracy: 0.9662 - val_precision_m: 0.6368 - val_recall_m: 0.3362 - val_f1_m: 0.4162\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.51620\n",
      "Epoch 98/100\n",
      "32/32 - 1s - loss: 0.1710 - accuracy: 0.9806 - precision_m: 0.9070 - recall_m: 0.7285 - f1_m: 0.8019 - val_loss: 0.5668 - val_accuracy: 0.9644 - val_precision_m: 0.5982 - val_recall_m: 0.3111 - val_f1_m: 0.3857\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.51620\n",
      "Epoch 99/100\n",
      "32/32 - 1s - loss: 0.1671 - accuracy: 0.9810 - precision_m: 0.9101 - recall_m: 0.7265 - f1_m: 0.8040 - val_loss: 0.5881 - val_accuracy: 0.9627 - val_precision_m: 0.5586 - val_recall_m: 0.3073 - val_f1_m: 0.3738\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.51620\n",
      "Epoch 100/100\n",
      "32/32 - 1s - loss: 0.1723 - accuracy: 0.9810 - precision_m: 0.9032 - recall_m: 0.7284 - f1_m: 0.8017 - val_loss: 0.7929 - val_accuracy: 0.9342 - val_precision_m: 0.2022 - val_recall_m: 0.2546 - val_f1_m: 0.2177\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.51620\n",
      "5/5 - 12s - loss: 0.5600 - accuracy: 0.9335 - precision_m: 0.4366 - recall_m: 0.4966 - f1_m: 0.4637\n",
      "The metrics for the test set with loss tversky, learning rate 0.001,  filters 8, and batch size 8 is [0.5600101947784424, 0.9335240721702576, 0.43661171197891235, 0.49658116698265076, 0.4637046456336975]\n",
      "__________________________________________________________________________________________________________________________________________________________________________\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 128, 128, 5) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 128, 128, 16) 736         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 128, 128, 16) 64          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 128, 128, 16) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 16) 2320        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 16) 64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, 128, 16) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 64, 64, 16)   0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 32)   4640        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64, 64, 32)   128         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 64, 64, 32)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 64, 32)   9248        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 32)   128         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64, 64, 32)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 32)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 64)   18496       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 64)   256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 64)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 64)   36928       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 64)   256         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 64)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 64)   0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 16, 16, 128)  73856       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 16, 16, 128)  512         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 16, 16, 128)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 16, 16, 128)  147584      activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 16, 16, 128)  512         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16, 16, 128)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 128)    0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 8, 8, 256)    295168      max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 8, 8, 256)    1024        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 8, 8, 256)    0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 8, 8, 256)    590080      activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 8, 8, 256)    1024        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 8, 8, 256)    0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 16, 16, 128)  131200      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 16, 16, 256)  0           conv2d_transpose[0][0]           \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 128)  295040      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 128)  512         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 128)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 128)  147584      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 128)  512         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 128)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 32, 32, 64)   32832       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 128)  0           conv2d_transpose_1[0][0]         \n",
      "                                                                 activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 64)   73792       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 32, 32, 64)   256         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 32, 32, 64)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 32, 64)   36928       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 32, 32, 64)   256         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 32, 32, 64)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 64, 64, 32)   8224        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 64, 64, 64)   0           conv2d_transpose_2[0][0]         \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 32)   18464       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64, 64, 32)   128         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 64, 64, 32)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 32)   9248        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 64, 64, 32)   128         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 64, 64, 32)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 128, 128, 16) 2064        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 128, 128, 32) 0           conv2d_transpose_3[0][0]         \n",
      "                                                                 activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 128, 128, 16) 4624        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 128, 128, 16) 64          conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 128, 128, 16) 0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 128, 128, 16) 2320        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 128, 128, 16) 64          conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 128, 128, 16) 0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 128, 128, 1)  17          activation_17[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,947,281\n",
      "Trainable params: 1,944,337\n",
      "Non-trainable params: 2,944\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "Initialising in 3..........2..........1......................................\n",
      "Training on 284 images with learning rate 0.001, batch size 8, and number of filters 16\n",
      "Epoch 1/100\n",
      "32/32 - 3s - loss: 0.9058 - accuracy: 0.4727 - precision_m: 0.0739 - recall_m: 0.7019 - f1_m: 0.1307 - val_loss: 0.9298 - val_accuracy: 0.4724 - val_precision_m: 0.0532 - val_recall_m: 0.7582 - val_f1_m: 0.0982\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.51620\n",
      "Epoch 2/100\n",
      "32/32 - 1s - loss: 0.8793 - accuracy: 0.6454 - precision_m: 0.0968 - recall_m: 0.6610 - f1_m: 0.1639 - val_loss: 0.9454 - val_accuracy: 0.0493 - val_precision_m: 0.0382 - val_recall_m: 0.9897 - val_f1_m: 0.0735\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.51620\n",
      "Epoch 3/100\n",
      "32/32 - 1s - loss: 0.8625 - accuracy: 0.6527 - precision_m: 0.1159 - recall_m: 0.7181 - f1_m: 0.1923 - val_loss: 0.9456 - val_accuracy: 0.0494 - val_precision_m: 0.0382 - val_recall_m: 0.9857 - val_f1_m: 0.0734\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.51620\n",
      "Epoch 4/100\n",
      "32/32 - 1s - loss: 0.8441 - accuracy: 0.7489 - precision_m: 0.1349 - recall_m: 0.6392 - f1_m: 0.2184 - val_loss: 0.9364 - val_accuracy: 0.2382 - val_precision_m: 0.0476 - val_recall_m: 0.9725 - val_f1_m: 0.0904\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.51620\n",
      "Epoch 5/100\n",
      "32/32 - 1s - loss: 0.8168 - accuracy: 0.7858 - precision_m: 0.1649 - recall_m: 0.6704 - f1_m: 0.2545 - val_loss: 0.9439 - val_accuracy: 0.9461 - val_precision_m: 0.0533 - val_recall_m: 0.0421 - val_f1_m: 0.0470\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.51620\n",
      "Epoch 6/100\n",
      "32/32 - 1s - loss: 0.8076 - accuracy: 0.8066 - precision_m: 0.1687 - recall_m: 0.6517 - f1_m: 0.2608 - val_loss: 0.9576 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.51620\n",
      "Epoch 7/100\n",
      "32/32 - 1s - loss: 0.7877 - accuracy: 0.8107 - precision_m: 0.1825 - recall_m: 0.6797 - f1_m: 0.2774 - val_loss: 0.9621 - val_accuracy: 0.9609 - val_precision_m: 0.0093 - val_recall_m: 6.8353e-05 - val_f1_m: 1.3570e-04\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.51620\n",
      "Epoch 8/100\n",
      "32/32 - 1s - loss: 0.7863 - accuracy: 0.8151 - precision_m: 0.1830 - recall_m: 0.6468 - f1_m: 0.2750 - val_loss: 0.9265 - val_accuracy: 0.9583 - val_precision_m: 0.0957 - val_recall_m: 0.0370 - val_f1_m: 0.0533\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.51620\n",
      "Epoch 9/100\n",
      "32/32 - 1s - loss: 0.7858 - accuracy: 0.8401 - precision_m: 0.1855 - recall_m: 0.5776 - f1_m: 0.2699 - val_loss: 0.9652 - val_accuracy: 0.8484 - val_precision_m: 0.0190 - val_recall_m: 0.0540 - val_f1_m: 0.0279\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.51620\n",
      "Epoch 10/100\n",
      "32/32 - 1s - loss: 0.7754 - accuracy: 0.8351 - precision_m: 0.1847 - recall_m: 0.6145 - f1_m: 0.2757 - val_loss: 0.9746 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.51620\n",
      "Epoch 11/100\n",
      "32/32 - 1s - loss: 0.7376 - accuracy: 0.8482 - precision_m: 0.2334 - recall_m: 0.6412 - f1_m: 0.3265 - val_loss: 0.9835 - val_accuracy: 0.9306 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.51620\n",
      "Epoch 12/100\n",
      "32/32 - 1s - loss: 0.7591 - accuracy: 0.8511 - precision_m: 0.2062 - recall_m: 0.5956 - f1_m: 0.2927 - val_loss: 0.9883 - val_accuracy: 0.9234 - val_precision_m: 6.0317e-05 - val_recall_m: 2.1978e-04 - val_f1_m: 9.4648e-05\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.51620\n",
      "Epoch 13/100\n",
      "32/32 - 1s - loss: 0.7455 - accuracy: 0.8547 - precision_m: 0.2178 - recall_m: 0.5896 - f1_m: 0.3057 - val_loss: 0.7772 - val_accuracy: 0.8820 - val_precision_m: 0.2459 - val_recall_m: 0.4609 - val_f1_m: 0.2829\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.51620\n",
      "Epoch 14/100\n",
      "32/32 - 1s - loss: 0.7289 - accuracy: 0.8917 - precision_m: 0.2598 - recall_m: 0.4667 - f1_m: 0.3163 - val_loss: 0.9845 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.51620\n",
      "Epoch 15/100\n",
      "32/32 - 1s - loss: 0.7121 - accuracy: 0.8876 - precision_m: 0.2614 - recall_m: 0.5523 - f1_m: 0.3394 - val_loss: 0.9932 - val_accuracy: 0.9367 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.51620\n",
      "Epoch 16/100\n",
      "32/32 - 1s - loss: 0.7324 - accuracy: 0.8766 - precision_m: 0.2330 - recall_m: 0.5340 - f1_m: 0.3104 - val_loss: 0.9956 - val_accuracy: 0.9238 - val_precision_m: 7.5465e-05 - val_recall_m: 2.7473e-04 - val_f1_m: 1.1840e-04\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.51620\n",
      "Epoch 17/100\n",
      "32/32 - 1s - loss: 0.7147 - accuracy: 0.8851 - precision_m: 0.2515 - recall_m: 0.5522 - f1_m: 0.3357 - val_loss: 0.9939 - val_accuracy: 0.9327 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.51620\n",
      "Epoch 18/100\n",
      "32/32 - 1s - loss: 0.6499 - accuracy: 0.9144 - precision_m: 0.3339 - recall_m: 0.5115 - f1_m: 0.3879 - val_loss: 0.9913 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.51620\n",
      "Epoch 19/100\n",
      "32/32 - 1s - loss: 0.6764 - accuracy: 0.9125 - precision_m: 0.3077 - recall_m: 0.4652 - f1_m: 0.3545 - val_loss: 0.9906 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.51620\n",
      "Epoch 20/100\n",
      "32/32 - 1s - loss: 0.7195 - accuracy: 0.9087 - precision_m: 0.2770 - recall_m: 0.3522 - f1_m: 0.2987 - val_loss: 0.9928 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.51620\n",
      "Epoch 21/100\n",
      "32/32 - 1s - loss: 0.6536 - accuracy: 0.9165 - precision_m: 0.3330 - recall_m: 0.4775 - f1_m: 0.3714 - val_loss: 0.9958 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.51620\n",
      "Epoch 22/100\n",
      "32/32 - 1s - loss: 0.6472 - accuracy: 0.9152 - precision_m: 0.3490 - recall_m: 0.4456 - f1_m: 0.3724 - val_loss: 0.9965 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.51620\n",
      "Epoch 23/100\n",
      "32/32 - 1s - loss: 0.6831 - accuracy: 0.9094 - precision_m: 0.3037 - recall_m: 0.4407 - f1_m: 0.3408 - val_loss: 0.9963 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.51620\n",
      "Epoch 24/100\n",
      "32/32 - 1s - loss: 0.6539 - accuracy: 0.9107 - precision_m: 0.3317 - recall_m: 0.4576 - f1_m: 0.3660 - val_loss: 0.9981 - val_accuracy: 0.9297 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.51620\n",
      "Epoch 25/100\n",
      "32/32 - 1s - loss: 0.6342 - accuracy: 0.9258 - precision_m: 0.3732 - recall_m: 0.4398 - f1_m: 0.3768 - val_loss: 0.9983 - val_accuracy: 0.9387 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.51620\n",
      "Epoch 26/100\n",
      "32/32 - 1s - loss: 0.6229 - accuracy: 0.9322 - precision_m: 0.4008 - recall_m: 0.3797 - f1_m: 0.3780 - val_loss: 0.8344 - val_accuracy: 0.7587 - val_precision_m: 0.1279 - val_recall_m: 0.8287 - val_f1_m: 0.2202\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.51620\n",
      "Epoch 27/100\n",
      "32/32 - 1s - loss: 0.6038 - accuracy: 0.9302 - precision_m: 0.3953 - recall_m: 0.4504 - f1_m: 0.4091 - val_loss: 0.9989 - val_accuracy: 0.9159 - val_precision_m: 1.3378e-05 - val_recall_m: 5.4945e-05 - val_f1_m: 2.1510e-05\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.51620\n",
      "Epoch 28/100\n",
      "32/32 - 1s - loss: 0.6291 - accuracy: 0.9311 - precision_m: 0.3988 - recall_m: 0.3706 - f1_m: 0.3695 - val_loss: 0.9195 - val_accuracy: 0.9352 - val_precision_m: 0.0968 - val_recall_m: 0.0456 - val_f1_m: 0.0618\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.51620\n",
      "Epoch 29/100\n",
      "32/32 - 1s - loss: 0.6103 - accuracy: 0.9317 - precision_m: 0.3905 - recall_m: 0.4451 - f1_m: 0.4009 - val_loss: 0.9985 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.51620\n",
      "Epoch 30/100\n",
      "32/32 - 1s - loss: 0.5928 - accuracy: 0.9348 - precision_m: 0.4193 - recall_m: 0.4437 - f1_m: 0.4131 - val_loss: 0.9985 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.51620\n",
      "Epoch 31/100\n",
      "32/32 - 1s - loss: 0.5761 - accuracy: 0.9384 - precision_m: 0.4489 - recall_m: 0.4334 - f1_m: 0.4216 - val_loss: 0.9984 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.51620\n",
      "Epoch 32/100\n",
      "32/32 - 1s - loss: 0.5497 - accuracy: 0.9448 - precision_m: 0.5120 - recall_m: 0.4178 - f1_m: 0.4365 - val_loss: 0.9987 - val_accuracy: 0.9284 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.51620\n",
      "Epoch 33/100\n",
      "32/32 - 1s - loss: 0.5930 - accuracy: 0.9368 - precision_m: 0.4310 - recall_m: 0.4133 - f1_m: 0.4040 - val_loss: 0.9991 - val_accuracy: 0.9220 - val_precision_m: 1.4820e-05 - val_recall_m: 5.4945e-05 - val_f1_m: 2.3335e-05\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.51620\n",
      "Epoch 34/100\n",
      "32/32 - 1s - loss: 0.5382 - accuracy: 0.9432 - precision_m: 0.4993 - recall_m: 0.4400 - f1_m: 0.4507 - val_loss: 0.9989 - val_accuracy: 0.9300 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.51620\n",
      "Epoch 35/100\n",
      "32/32 - 1s - loss: 0.5342 - accuracy: 0.9442 - precision_m: 0.5042 - recall_m: 0.4513 - f1_m: 0.4573 - val_loss: 0.9989 - val_accuracy: 0.9357 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.51620\n",
      "Epoch 36/100\n",
      "32/32 - 1s - loss: 0.5121 - accuracy: 0.9487 - precision_m: 0.5524 - recall_m: 0.4303 - f1_m: 0.4677 - val_loss: 0.9988 - val_accuracy: 0.9558 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.51620\n",
      "Epoch 37/100\n",
      "32/32 - 1s - loss: 0.5339 - accuracy: 0.9477 - precision_m: 0.5416 - recall_m: 0.3963 - f1_m: 0.4403 - val_loss: 0.9988 - val_accuracy: 0.9585 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.51620\n",
      "Epoch 38/100\n",
      "32/32 - 1s - loss: 0.5242 - accuracy: 0.9476 - precision_m: 0.5389 - recall_m: 0.4281 - f1_m: 0.4564 - val_loss: 0.9991 - val_accuracy: 0.9315 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.51620\n",
      "Epoch 39/100\n",
      "32/32 - 1s - loss: 0.5382 - accuracy: 0.9468 - precision_m: 0.5400 - recall_m: 0.3927 - f1_m: 0.4366 - val_loss: 0.9991 - val_accuracy: 0.9283 - val_precision_m: 1.6439e-05 - val_recall_m: 5.4945e-05 - val_f1_m: 2.5297e-05\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.51620\n",
      "Epoch 40/100\n",
      "32/32 - 1s - loss: 0.4711 - accuracy: 0.9516 - precision_m: 0.5844 - recall_m: 0.4829 - f1_m: 0.5086 - val_loss: 0.9992 - val_accuracy: 0.9278 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.51620\n",
      "Epoch 41/100\n",
      "32/32 - 1s - loss: 0.5612 - accuracy: 0.9423 - precision_m: 0.4904 - recall_m: 0.4128 - f1_m: 0.4242 - val_loss: 0.9956 - val_accuracy: 0.9424 - val_precision_m: 0.0986 - val_recall_m: 9.5694e-04 - val_f1_m: 0.0019\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.51620\n",
      "Epoch 42/100\n",
      "32/32 - 1s - loss: 0.5107 - accuracy: 0.9470 - precision_m: 0.5217 - recall_m: 0.4848 - f1_m: 0.4819 - val_loss: 0.9992 - val_accuracy: 0.9265 - val_precision_m: 1.6113e-05 - val_recall_m: 5.4945e-05 - val_f1_m: 2.4910e-05\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.51620\n",
      "Epoch 43/100\n",
      "32/32 - 1s - loss: 0.4663 - accuracy: 0.9514 - precision_m: 0.6087 - recall_m: 0.4652 - f1_m: 0.5078 - val_loss: 0.9993 - val_accuracy: 0.9291 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.51620\n",
      "Epoch 44/100\n",
      "32/32 - 1s - loss: 0.4762 - accuracy: 0.9523 - precision_m: 0.6014 - recall_m: 0.4580 - f1_m: 0.4961 - val_loss: 0.9993 - val_accuracy: 0.9283 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.51620\n",
      "Epoch 45/100\n",
      "32/32 - 1s - loss: 0.4615 - accuracy: 0.9520 - precision_m: 0.5935 - recall_m: 0.4784 - f1_m: 0.5172 - val_loss: 0.9993 - val_accuracy: 0.9262 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.51620\n",
      "Epoch 46/100\n",
      "32/32 - 1s - loss: 0.4522 - accuracy: 0.9521 - precision_m: 0.5895 - recall_m: 0.5200 - f1_m: 0.5355 - val_loss: 0.9994 - val_accuracy: 0.9261 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.51620\n",
      "Epoch 47/100\n",
      "32/32 - 1s - loss: 0.4381 - accuracy: 0.9552 - precision_m: 0.6254 - recall_m: 0.5187 - f1_m: 0.5412 - val_loss: 0.9994 - val_accuracy: 0.9262 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.51620\n",
      "Epoch 48/100\n",
      "32/32 - 1s - loss: 0.4780 - accuracy: 0.9528 - precision_m: 0.5953 - recall_m: 0.4586 - f1_m: 0.4949 - val_loss: 0.9993 - val_accuracy: 0.9270 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.51620\n",
      "Epoch 49/100\n",
      "32/32 - 1s - loss: 0.4336 - accuracy: 0.9558 - precision_m: 0.6155 - recall_m: 0.5215 - f1_m: 0.5479 - val_loss: 0.9994 - val_accuracy: 0.9266 - val_precision_m: 1.5972e-05 - val_recall_m: 5.4945e-05 - val_f1_m: 2.4741e-05\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.51620\n",
      "Epoch 50/100\n",
      "32/32 - 1s - loss: 0.4517 - accuracy: 0.9542 - precision_m: 0.6040 - recall_m: 0.4938 - f1_m: 0.5276 - val_loss: 0.9994 - val_accuracy: 0.9261 - val_precision_m: 1.5942e-05 - val_recall_m: 5.4945e-05 - val_f1_m: 2.4705e-05\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.51620\n",
      "Epoch 51/100\n",
      "32/32 - 1s - loss: 0.4204 - accuracy: 0.9578 - precision_m: 0.6785 - recall_m: 0.4778 - f1_m: 0.5426 - val_loss: 0.9994 - val_accuracy: 0.9257 - val_precision_m: 1.5767e-05 - val_recall_m: 5.4945e-05 - val_f1_m: 2.4494e-05\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.51620\n",
      "Epoch 52/100\n",
      "32/32 - 1s - loss: 0.4419 - accuracy: 0.9553 - precision_m: 0.6287 - recall_m: 0.4934 - f1_m: 0.5332 - val_loss: 0.9994 - val_accuracy: 0.9264 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.51620\n",
      "Epoch 53/100\n",
      "32/32 - 1s - loss: 0.4420 - accuracy: 0.9559 - precision_m: 0.6343 - recall_m: 0.4829 - f1_m: 0.5301 - val_loss: 0.9993 - val_accuracy: 0.9264 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.51620\n",
      "Epoch 54/100\n",
      "32/32 - 1s - loss: 0.4304 - accuracy: 0.9563 - precision_m: 0.6338 - recall_m: 0.5030 - f1_m: 0.5438 - val_loss: 0.9994 - val_accuracy: 0.9252 - val_precision_m: 1.5705e-05 - val_recall_m: 5.4945e-05 - val_f1_m: 2.4419e-05\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.51620\n",
      "Epoch 55/100\n",
      "32/32 - 1s - loss: 0.4008 - accuracy: 0.9585 - precision_m: 0.6672 - recall_m: 0.5183 - f1_m: 0.5721 - val_loss: 0.9994 - val_accuracy: 0.9248 - val_precision_m: 1.5562e-05 - val_recall_m: 5.4945e-05 - val_f1_m: 2.4246e-05\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.51620\n",
      "Epoch 56/100\n",
      "32/32 - 1s - loss: 0.4108 - accuracy: 0.9575 - precision_m: 0.6562 - recall_m: 0.5252 - f1_m: 0.5659 - val_loss: 0.9994 - val_accuracy: 0.9253 - val_precision_m: 1.5637e-05 - val_recall_m: 5.4945e-05 - val_f1_m: 2.4336e-05\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.51620\n",
      "Epoch 57/100\n",
      "32/32 - 1s - loss: 0.4149 - accuracy: 0.9583 - precision_m: 0.6710 - recall_m: 0.4993 - f1_m: 0.5541 - val_loss: 0.9994 - val_accuracy: 0.9257 - val_precision_m: 1.5702e-05 - val_recall_m: 5.4945e-05 - val_f1_m: 2.4415e-05\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.51620\n",
      "Epoch 58/100\n",
      "32/32 - 1s - loss: 0.3866 - accuracy: 0.9606 - precision_m: 0.6977 - recall_m: 0.5163 - f1_m: 0.5793 - val_loss: 0.9994 - val_accuracy: 0.9253 - val_precision_m: 1.5543e-05 - val_recall_m: 5.4945e-05 - val_f1_m: 2.4223e-05\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.51620\n",
      "Epoch 59/100\n",
      "32/32 - 1s - loss: 0.3615 - accuracy: 0.9628 - precision_m: 0.7182 - recall_m: 0.5333 - f1_m: 0.6019 - val_loss: 0.9994 - val_accuracy: 0.9253 - val_precision_m: 1.5544e-05 - val_recall_m: 5.4945e-05 - val_f1_m: 2.4224e-05\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.51620\n",
      "Epoch 60/100\n",
      "32/32 - 1s - loss: 0.3766 - accuracy: 0.9608 - precision_m: 0.7195 - recall_m: 0.5118 - f1_m: 0.5858 - val_loss: 0.9994 - val_accuracy: 0.9256 - val_precision_m: 1.5652e-05 - val_recall_m: 5.4945e-05 - val_f1_m: 2.4355e-05\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.51620\n",
      "Epoch 61/100\n",
      "32/32 - 1s - loss: 0.3747 - accuracy: 0.9618 - precision_m: 0.7240 - recall_m: 0.5233 - f1_m: 0.5880 - val_loss: 0.9994 - val_accuracy: 0.9249 - val_precision_m: 1.5473e-05 - val_recall_m: 5.4945e-05 - val_f1_m: 2.4138e-05\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.51620\n",
      "Epoch 62/100\n",
      "32/32 - 1s - loss: 0.3944 - accuracy: 0.9604 - precision_m: 0.6882 - recall_m: 0.5099 - f1_m: 0.5722 - val_loss: 0.9994 - val_accuracy: 0.9252 - val_precision_m: 1.5592e-05 - val_recall_m: 5.4945e-05 - val_f1_m: 2.4282e-05\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.51620\n",
      "Epoch 63/100\n",
      "32/32 - 1s - loss: 0.4203 - accuracy: 0.9577 - precision_m: 0.6622 - recall_m: 0.4901 - f1_m: 0.5483 - val_loss: 0.9994 - val_accuracy: 0.9251 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.51620\n",
      "Epoch 64/100\n",
      "32/32 - 1s - loss: 0.4110 - accuracy: 0.9589 - precision_m: 0.6648 - recall_m: 0.5215 - f1_m: 0.5629 - val_loss: 0.9994 - val_accuracy: 0.9243 - val_precision_m: 1.5414e-05 - val_recall_m: 5.4945e-05 - val_f1_m: 2.4066e-05\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.51620\n",
      "Epoch 65/100\n",
      "32/32 - 1s - loss: 0.3536 - accuracy: 0.9631 - precision_m: 0.7241 - recall_m: 0.5525 - f1_m: 0.6155 - val_loss: 0.9994 - val_accuracy: 0.9242 - val_precision_m: 1.5399e-05 - val_recall_m: 5.4945e-05 - val_f1_m: 2.4047e-05\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.51620\n",
      "Epoch 66/100\n",
      "32/32 - 1s - loss: 0.3710 - accuracy: 0.9619 - precision_m: 0.7139 - recall_m: 0.5280 - f1_m: 0.5939 - val_loss: 0.9994 - val_accuracy: 0.9244 - val_precision_m: 1.5404e-05 - val_recall_m: 5.4945e-05 - val_f1_m: 2.4053e-05\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.51620\n",
      "Epoch 67/100\n",
      "32/32 - 1s - loss: 0.3918 - accuracy: 0.9611 - precision_m: 0.6888 - recall_m: 0.5173 - f1_m: 0.5757 - val_loss: 0.9994 - val_accuracy: 0.9248 - val_precision_m: 1.5464e-05 - val_recall_m: 5.4945e-05 - val_f1_m: 2.4126e-05\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.51620\n",
      "Epoch 68/100\n",
      "32/32 - 1s - loss: 0.3777 - accuracy: 0.9614 - precision_m: 0.7138 - recall_m: 0.5247 - f1_m: 0.5872 - val_loss: 0.9994 - val_accuracy: 0.9247 - val_precision_m: 1.5446e-05 - val_recall_m: 5.4945e-05 - val_f1_m: 2.4105e-05\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.51620\n",
      "Epoch 69/100\n",
      "32/32 - 1s - loss: 0.3838 - accuracy: 0.9616 - precision_m: 0.6801 - recall_m: 0.5569 - f1_m: 0.5919 - val_loss: 0.9994 - val_accuracy: 0.9246 - val_precision_m: 1.5481e-05 - val_recall_m: 5.4945e-05 - val_f1_m: 2.4147e-05\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.51620\n",
      "Epoch 70/100\n",
      "32/32 - 1s - loss: 0.3599 - accuracy: 0.9641 - precision_m: 0.7290 - recall_m: 0.5357 - f1_m: 0.6048 - val_loss: 0.9994 - val_accuracy: 0.9244 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.51620\n",
      "Epoch 71/100\n",
      "32/32 - 1s - loss: 0.3695 - accuracy: 0.9619 - precision_m: 0.7296 - recall_m: 0.5180 - f1_m: 0.5915 - val_loss: 0.9995 - val_accuracy: 0.9240 - val_precision_m: 1.5359e-05 - val_recall_m: 5.4945e-05 - val_f1_m: 2.3999e-05\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.51620\n",
      "Epoch 72/100\n",
      "32/32 - 1s - loss: 0.3652 - accuracy: 0.9632 - precision_m: 0.7268 - recall_m: 0.5286 - f1_m: 0.5973 - val_loss: 0.9994 - val_accuracy: 0.9243 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.51620\n",
      "Epoch 73/100\n",
      "32/32 - 1s - loss: 0.3691 - accuracy: 0.9626 - precision_m: 0.7149 - recall_m: 0.5367 - f1_m: 0.5983 - val_loss: 0.9994 - val_accuracy: 0.9248 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.51620\n",
      "Epoch 74/100\n",
      "32/32 - 1s - loss: 0.3652 - accuracy: 0.9638 - precision_m: 0.7186 - recall_m: 0.5332 - f1_m: 0.6015 - val_loss: 0.9995 - val_accuracy: 0.9243 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.51620\n",
      "Epoch 75/100\n",
      "32/32 - 1s - loss: 0.3780 - accuracy: 0.9616 - precision_m: 0.7036 - recall_m: 0.5271 - f1_m: 0.5891 - val_loss: 0.9995 - val_accuracy: 0.9240 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.51620\n",
      "Epoch 76/100\n",
      "32/32 - 1s - loss: 0.3914 - accuracy: 0.9606 - precision_m: 0.6837 - recall_m: 0.5251 - f1_m: 0.5789 - val_loss: 0.9995 - val_accuracy: 0.9234 - val_precision_m: 1.5171e-05 - val_recall_m: 5.4945e-05 - val_f1_m: 2.3768e-05\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.51620\n",
      "Epoch 77/100\n",
      "32/32 - 1s - loss: 0.3516 - accuracy: 0.9639 - precision_m: 0.7296 - recall_m: 0.5521 - f1_m: 0.6154 - val_loss: 0.9995 - val_accuracy: 0.9230 - val_precision_m: 1.5103e-05 - val_recall_m: 5.4945e-05 - val_f1_m: 2.3685e-05\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.51620\n",
      "Epoch 78/100\n",
      "32/32 - 1s - loss: 0.3631 - accuracy: 0.9625 - precision_m: 0.7139 - recall_m: 0.5423 - f1_m: 0.6050 - val_loss: 0.9995 - val_accuracy: 0.9227 - val_precision_m: 1.5049e-05 - val_recall_m: 5.4945e-05 - val_f1_m: 2.3619e-05\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.51620\n",
      "Epoch 79/100\n",
      "32/32 - 1s - loss: 0.3727 - accuracy: 0.9625 - precision_m: 0.7033 - recall_m: 0.5461 - f1_m: 0.5989 - val_loss: 0.9995 - val_accuracy: 0.9228 - val_precision_m: 1.5070e-05 - val_recall_m: 5.4945e-05 - val_f1_m: 2.3645e-05\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.51620\n",
      "Epoch 80/100\n",
      "32/32 - 1s - loss: 0.3689 - accuracy: 0.9626 - precision_m: 0.7139 - recall_m: 0.5332 - f1_m: 0.5975 - val_loss: 0.9995 - val_accuracy: 0.9229 - val_precision_m: 1.5090e-05 - val_recall_m: 5.4945e-05 - val_f1_m: 2.3669e-05\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.51620\n",
      "Epoch 81/100\n",
      "32/32 - 1s - loss: 0.3862 - accuracy: 0.9605 - precision_m: 0.6985 - recall_m: 0.5100 - f1_m: 0.5790 - val_loss: 0.9995 - val_accuracy: 0.9235 - val_precision_m: 1.5252e-05 - val_recall_m: 5.4945e-05 - val_f1_m: 2.3868e-05\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.51620\n",
      "Epoch 82/100\n",
      "32/32 - 1s - loss: 0.3823 - accuracy: 0.9629 - precision_m: 0.6920 - recall_m: 0.5338 - f1_m: 0.5886 - val_loss: 0.9995 - val_accuracy: 0.9230 - val_precision_m: 1.5094e-05 - val_recall_m: 5.4945e-05 - val_f1_m: 2.3674e-05\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.51620\n",
      "Epoch 83/100\n",
      "32/32 - 1s - loss: 0.3746 - accuracy: 0.9623 - precision_m: 0.6974 - recall_m: 0.5354 - f1_m: 0.5955 - val_loss: 0.9995 - val_accuracy: 0.9226 - val_precision_m: 1.5011e-05 - val_recall_m: 5.4945e-05 - val_f1_m: 2.3571e-05\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.51620\n",
      "Epoch 84/100\n",
      "32/32 - 1s - loss: 0.3651 - accuracy: 0.9624 - precision_m: 0.7171 - recall_m: 0.5426 - f1_m: 0.6034 - val_loss: 0.9995 - val_accuracy: 0.9226 - val_precision_m: 1.5029e-05 - val_recall_m: 5.4945e-05 - val_f1_m: 2.3593e-05\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.51620\n",
      "Epoch 85/100\n",
      "32/32 - 1s - loss: 0.3623 - accuracy: 0.9606 - precision_m: 0.7387 - recall_m: 0.5193 - f1_m: 0.5967 - val_loss: 0.9995 - val_accuracy: 0.9230 - val_precision_m: 1.5111e-05 - val_recall_m: 5.4945e-05 - val_f1_m: 2.3695e-05\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.51620\n",
      "Epoch 86/100\n",
      "32/32 - 1s - loss: 0.3603 - accuracy: 0.9632 - precision_m: 0.7179 - recall_m: 0.5535 - f1_m: 0.6089 - val_loss: 0.9995 - val_accuracy: 0.9226 - val_precision_m: 1.5029e-05 - val_recall_m: 5.4945e-05 - val_f1_m: 2.3594e-05\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.51620\n",
      "Epoch 87/100\n",
      "32/32 - 1s - loss: 0.3671 - accuracy: 0.9626 - precision_m: 0.7232 - recall_m: 0.5320 - f1_m: 0.5980 - val_loss: 0.9995 - val_accuracy: 0.9226 - val_precision_m: 1.5037e-05 - val_recall_m: 5.4945e-05 - val_f1_m: 2.3603e-05\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.51620\n",
      "Epoch 88/100\n",
      "32/32 - 1s - loss: 0.3680 - accuracy: 0.9634 - precision_m: 0.7130 - recall_m: 0.5366 - f1_m: 0.5999 - val_loss: 0.9995 - val_accuracy: 0.9229 - val_precision_m: 1.5082e-05 - val_recall_m: 5.4945e-05 - val_f1_m: 2.3659e-05\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.51620\n",
      "Epoch 89/100\n",
      "32/32 - 1s - loss: 0.3809 - accuracy: 0.9612 - precision_m: 0.7001 - recall_m: 0.5314 - f1_m: 0.5876 - val_loss: 0.9995 - val_accuracy: 0.9226 - val_precision_m: 1.5012e-05 - val_recall_m: 5.4945e-05 - val_f1_m: 2.3573e-05\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.51620\n",
      "Epoch 90/100\n",
      "32/32 - 1s - loss: 0.3629 - accuracy: 0.9625 - precision_m: 0.7207 - recall_m: 0.5394 - f1_m: 0.6034 - val_loss: 0.9995 - val_accuracy: 0.9232 - val_precision_m: 1.5146e-05 - val_recall_m: 5.4945e-05 - val_f1_m: 2.3738e-05\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.51620\n",
      "Epoch 91/100\n",
      "32/32 - 1s - loss: 0.4013 - accuracy: 0.9608 - precision_m: 0.6821 - recall_m: 0.5152 - f1_m: 0.5660 - val_loss: 0.9995 - val_accuracy: 0.9229 - val_precision_m: 1.5085e-05 - val_recall_m: 5.4945e-05 - val_f1_m: 2.3662e-05\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.51620\n",
      "Epoch 92/100\n",
      "32/32 - 1s - loss: 0.3510 - accuracy: 0.9643 - precision_m: 0.7261 - recall_m: 0.5601 - f1_m: 0.6172 - val_loss: 0.9995 - val_accuracy: 0.9231 - val_precision_m: 1.5136e-05 - val_recall_m: 5.4945e-05 - val_f1_m: 2.3725e-05\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.51620\n",
      "Epoch 93/100\n",
      "32/32 - 1s - loss: 0.3421 - accuracy: 0.9650 - precision_m: 0.7356 - recall_m: 0.5635 - f1_m: 0.6259 - val_loss: 0.9995 - val_accuracy: 0.9234 - val_precision_m: 1.5201e-05 - val_recall_m: 5.4945e-05 - val_f1_m: 2.3806e-05\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.51620\n",
      "Epoch 94/100\n",
      "32/32 - 1s - loss: 0.3543 - accuracy: 0.9639 - precision_m: 0.7253 - recall_m: 0.5634 - f1_m: 0.6154 - val_loss: 0.9995 - val_accuracy: 0.9236 - val_precision_m: 1.5252e-05 - val_recall_m: 5.4945e-05 - val_f1_m: 2.3868e-05\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.51620\n",
      "Epoch 95/100\n",
      "32/32 - 1s - loss: 0.3714 - accuracy: 0.9629 - precision_m: 0.7043 - recall_m: 0.5497 - f1_m: 0.6000 - val_loss: 0.9995 - val_accuracy: 0.9238 - val_precision_m: 1.5303e-05 - val_recall_m: 5.4945e-05 - val_f1_m: 2.3930e-05\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.51620\n",
      "Epoch 96/100\n",
      "32/32 - 1s - loss: 0.3650 - accuracy: 0.9631 - precision_m: 0.7212 - recall_m: 0.5327 - f1_m: 0.6002 - val_loss: 0.9995 - val_accuracy: 0.9241 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.51620\n",
      "Epoch 97/100\n",
      "32/32 - 1s - loss: 0.3868 - accuracy: 0.9612 - precision_m: 0.6864 - recall_m: 0.5307 - f1_m: 0.5836 - val_loss: 0.9995 - val_accuracy: 0.9242 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.51620\n",
      "Epoch 98/100\n",
      "32/32 - 1s - loss: 0.3754 - accuracy: 0.9618 - precision_m: 0.7087 - recall_m: 0.5366 - f1_m: 0.5932 - val_loss: 0.9995 - val_accuracy: 0.9235 - val_precision_m: 1.5228e-05 - val_recall_m: 5.4945e-05 - val_f1_m: 2.3839e-05\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.51620\n",
      "Epoch 99/100\n",
      "32/32 - 1s - loss: 0.3613 - accuracy: 0.9629 - precision_m: 0.7278 - recall_m: 0.5391 - f1_m: 0.6031 - val_loss: 0.9995 - val_accuracy: 0.9239 - val_precision_m: 1.5315e-05 - val_recall_m: 5.4945e-05 - val_f1_m: 2.3945e-05\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.51620\n",
      "Epoch 100/100\n",
      "32/32 - 1s - loss: 0.3953 - accuracy: 0.9599 - precision_m: 0.6779 - recall_m: 0.5153 - f1_m: 0.5744 - val_loss: 0.9995 - val_accuracy: 0.9231 - val_precision_m: 1.5122e-05 - val_recall_m: 5.4945e-05 - val_f1_m: 2.3708e-05\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.51620\n",
      "5/5 - 0s - loss: 0.9678 - accuracy: 0.9188 - precision_m: 0.0637 - recall_m: 0.0186 - f1_m: 0.0239\n",
      "The metrics for the test set with loss tversky, learning rate 0.001,  filters 16, and batch size 8 is [0.967831552028656, 0.9187877178192139, 0.0637274906039238, 0.018601391464471817, 0.023898927494883537]\n",
      "__________________________________________________________________________________________________________________________________________________________________________\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 128, 128, 5) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 128, 128, 32) 1472        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 128, 128, 32) 128         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 128, 128, 32) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 32) 9248        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 32) 128         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, 128, 32) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 64, 64, 32)   0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 64)   18496       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64, 64, 64)   256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 64, 64, 64)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 64, 64)   36928       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 64)   256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64, 64, 64)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 64)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 128)  73856       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 128)  512         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 128)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 128)  147584      activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 128)  512         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 128)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 128)  0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 16, 16, 256)  295168      max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 16, 16, 256)  1024        conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 16, 16, 256)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 16, 16, 256)  590080      activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 16, 16, 256)  1024        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16, 16, 256)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 256)    0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 8, 8, 512)    1180160     max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 8, 8, 512)    2048        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 8, 8, 512)    0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 8, 8, 512)    2359808     activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 8, 8, 512)    2048        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 8, 8, 512)    0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 16, 16, 128)  262272      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 16, 16, 384)  0           conv2d_transpose[0][0]           \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 256)  884992      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 256)  1024        conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 256)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 256)  590080      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 256)  1024        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 256)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 32, 32, 64)   65600       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 192)  0           conv2d_transpose_1[0][0]         \n",
      "                                                                 activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 128)  221312      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 32, 32, 128)  512         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 32, 32, 128)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 32, 128)  147584      activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 32, 32, 128)  512         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 32, 32, 128)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 64, 64, 32)   16416       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 64, 64, 96)   0           conv2d_transpose_2[0][0]         \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 64)   55360       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64, 64, 64)   256         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 64, 64, 64)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 64)   36928       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 64, 64, 64)   256         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 64, 64, 64)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 128, 128, 16) 4112        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 128, 128, 48) 0           conv2d_transpose_3[0][0]         \n",
      "                                                                 activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 128, 128, 32) 13856       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 128, 128, 32) 128         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 128, 128, 32) 0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 128, 128, 32) 9248        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 128, 128, 32) 128         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 128, 128, 32) 0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 128, 128, 1)  33          activation_17[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 7,032,369\n",
      "Trainable params: 7,026,481\n",
      "Non-trainable params: 5,888\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "Initialising in 3..........2..........1......................................\n",
      "Training on 284 images with learning rate 0.001, batch size 8, and number of filters 32\n",
      "Epoch 1/100\n",
      "32/32 - 4s - loss: 0.8900 - accuracy: 0.5807 - precision_m: 0.0882 - recall_m: 0.6771 - f1_m: 0.1518 - val_loss: 0.9327 - val_accuracy: 0.4756 - val_precision_m: 0.0507 - val_recall_m: 0.7523 - val_f1_m: 0.0937\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.51620\n",
      "Epoch 2/100\n",
      "32/32 - 1s - loss: 0.8387 - accuracy: 0.7923 - precision_m: 0.1407 - recall_m: 0.5452 - f1_m: 0.2182 - val_loss: 0.9453 - val_accuracy: 0.0398 - val_precision_m: 0.0383 - val_recall_m: 1.0000 - val_f1_m: 0.0737\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.51620\n",
      "Epoch 3/100\n",
      "32/32 - 1s - loss: 0.7851 - accuracy: 0.8449 - precision_m: 0.1990 - recall_m: 0.5255 - f1_m: 0.2769 - val_loss: 0.9439 - val_accuracy: 0.0744 - val_precision_m: 0.0397 - val_recall_m: 0.9969 - val_f1_m: 0.0762\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.51620\n",
      "Epoch 4/100\n",
      "32/32 - 1s - loss: 0.7641 - accuracy: 0.8468 - precision_m: 0.2028 - recall_m: 0.5818 - f1_m: 0.2910 - val_loss: 0.8687 - val_accuracy: 0.7239 - val_precision_m: 0.1312 - val_recall_m: 0.7051 - val_f1_m: 0.2065\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.51620\n",
      "Epoch 5/100\n",
      "32/32 - 1s - loss: 0.7619 - accuracy: 0.8414 - precision_m: 0.2094 - recall_m: 0.5482 - f1_m: 0.2865 - val_loss: 0.8783 - val_accuracy: 0.6894 - val_precision_m: 0.1362 - val_recall_m: 0.9054 - val_f1_m: 0.2289\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.51620\n",
      "Epoch 6/100\n",
      "32/32 - 1s - loss: 0.7914 - accuracy: 0.8797 - precision_m: 0.1923 - recall_m: 0.3705 - f1_m: 0.2415 - val_loss: 0.9350 - val_accuracy: 0.2315 - val_precision_m: 0.0480 - val_recall_m: 0.9933 - val_f1_m: 0.0912\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.51620\n",
      "Epoch 7/100\n",
      "32/32 - 1s - loss: 0.7907 - accuracy: 0.8787 - precision_m: 0.1916 - recall_m: 0.3455 - f1_m: 0.2368 - val_loss: 0.9390 - val_accuracy: 0.1806 - val_precision_m: 0.0433 - val_recall_m: 0.9663 - val_f1_m: 0.0828\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.51620\n",
      "Epoch 8/100\n",
      "32/32 - 1s - loss: 0.7538 - accuracy: 0.8971 - precision_m: 0.2356 - recall_m: 0.3426 - f1_m: 0.2679 - val_loss: 0.9223 - val_accuracy: 0.3833 - val_precision_m: 0.0578 - val_recall_m: 0.9186 - val_f1_m: 0.1083\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.51620\n",
      "Epoch 9/100\n",
      "32/32 - 1s - loss: 0.8111 - accuracy: 0.8235 - precision_m: 0.1567 - recall_m: 0.4843 - f1_m: 0.2245 - val_loss: 0.9220 - val_accuracy: 0.4702 - val_precision_m: 0.0561 - val_recall_m: 0.7839 - val_f1_m: 0.1042\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.51620\n",
      "Epoch 10/100\n",
      "32/32 - 1s - loss: 0.7455 - accuracy: 0.8775 - precision_m: 0.2222 - recall_m: 0.4774 - f1_m: 0.2927 - val_loss: 0.9880 - val_accuracy: 0.9543 - val_precision_m: 2.3507e-04 - val_recall_m: 6.8353e-05 - val_f1_m: 1.0590e-04\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.51620\n",
      "Epoch 11/100\n",
      "32/32 - 1s - loss: 0.7093 - accuracy: 0.8806 - precision_m: 0.2555 - recall_m: 0.5228 - f1_m: 0.3305 - val_loss: 0.9416 - val_accuracy: 0.1895 - val_precision_m: 0.0405 - val_recall_m: 0.8822 - val_f1_m: 0.0772\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.51620\n",
      "Epoch 12/100\n",
      "32/32 - 1s - loss: 0.7145 - accuracy: 0.9018 - precision_m: 0.2745 - recall_m: 0.3954 - f1_m: 0.3064 - val_loss: 0.9639 - val_accuracy: 0.6551 - val_precision_m: 0.0220 - val_recall_m: 0.2045 - val_f1_m: 0.0392\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.51620\n",
      "Epoch 13/100\n",
      "32/32 - 1s - loss: 0.7166 - accuracy: 0.8964 - precision_m: 0.2644 - recall_m: 0.4206 - f1_m: 0.3055 - val_loss: 0.9960 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.51620\n",
      "Epoch 14/100\n",
      "32/32 - 1s - loss: 0.7024 - accuracy: 0.8893 - precision_m: 0.2670 - recall_m: 0.5021 - f1_m: 0.3326 - val_loss: 0.9975 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.51620\n",
      "Epoch 15/100\n",
      "32/32 - 1s - loss: 0.7061 - accuracy: 0.9033 - precision_m: 0.2820 - recall_m: 0.3835 - f1_m: 0.3124 - val_loss: 0.9948 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.51620\n",
      "Epoch 16/100\n",
      "32/32 - 1s - loss: 0.6458 - accuracy: 0.9267 - precision_m: 0.3646 - recall_m: 0.3947 - f1_m: 0.3613 - val_loss: 0.9956 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.51620\n",
      "Epoch 17/100\n",
      "32/32 - 1s - loss: 0.6747 - accuracy: 0.9102 - precision_m: 0.3073 - recall_m: 0.4952 - f1_m: 0.3516 - val_loss: 0.8942 - val_accuracy: 0.9595 - val_precision_m: 0.1412 - val_recall_m: 0.0551 - val_f1_m: 0.0793\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.51620\n",
      "Epoch 18/100\n",
      "32/32 - 1s - loss: 0.6371 - accuracy: 0.9257 - precision_m: 0.3603 - recall_m: 0.4287 - f1_m: 0.3745 - val_loss: 0.9295 - val_accuracy: 0.9379 - val_precision_m: 0.0881 - val_recall_m: 0.0387 - val_f1_m: 0.0538\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.51620\n",
      "Epoch 19/100\n",
      "32/32 - 1s - loss: 0.7088 - accuracy: 0.9180 - precision_m: 0.2933 - recall_m: 0.3350 - f1_m: 0.2980 - val_loss: 0.9970 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.51620\n",
      "Epoch 20/100\n",
      "32/32 - 1s - loss: 0.6831 - accuracy: 0.9245 - precision_m: 0.3206 - recall_m: 0.3630 - f1_m: 0.3235 - val_loss: 0.9977 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.51620\n",
      "Epoch 21/100\n",
      "32/32 - 1s - loss: 0.6208 - accuracy: 0.9312 - precision_m: 0.3904 - recall_m: 0.4201 - f1_m: 0.3833 - val_loss: 0.9986 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.51620\n",
      "Epoch 22/100\n",
      "32/32 - 1s - loss: 0.6112 - accuracy: 0.9296 - precision_m: 0.3778 - recall_m: 0.4602 - f1_m: 0.4016 - val_loss: 0.9990 - val_accuracy: 0.9425 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.51620\n",
      "Epoch 23/100\n",
      "32/32 - 1s - loss: 0.6109 - accuracy: 0.9335 - precision_m: 0.4091 - recall_m: 0.4080 - f1_m: 0.3874 - val_loss: 0.9987 - val_accuracy: 0.9575 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.51620\n",
      "Epoch 24/100\n",
      "32/32 - 1s - loss: 0.6405 - accuracy: 0.9274 - precision_m: 0.3720 - recall_m: 0.3989 - f1_m: 0.3622 - val_loss: 0.9993 - val_accuracy: 0.9319 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.51620\n",
      "Epoch 25/100\n",
      "32/32 - 1s - loss: 0.6028 - accuracy: 0.9333 - precision_m: 0.4078 - recall_m: 0.4261 - f1_m: 0.3988 - val_loss: 0.9991 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.51620\n",
      "Epoch 26/100\n",
      "32/32 - 1s - loss: 0.5758 - accuracy: 0.9352 - precision_m: 0.4396 - recall_m: 0.4628 - f1_m: 0.4267 - val_loss: 0.9990 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.51620\n",
      "Epoch 27/100\n",
      "32/32 - 1s - loss: 0.5572 - accuracy: 0.9426 - precision_m: 0.4803 - recall_m: 0.4235 - f1_m: 0.4317 - val_loss: 0.9992 - val_accuracy: 0.9485 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.51620\n",
      "Epoch 28/100\n",
      "32/32 - 1s - loss: 0.5862 - accuracy: 0.9413 - precision_m: 0.4681 - recall_m: 0.3844 - f1_m: 0.3958 - val_loss: 0.9993 - val_accuracy: 0.9374 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.51620\n",
      "Epoch 29/100\n",
      "32/32 - 1s - loss: 0.5604 - accuracy: 0.9418 - precision_m: 0.4812 - recall_m: 0.4178 - f1_m: 0.4276 - val_loss: 0.9994 - val_accuracy: 0.9403 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.51620\n",
      "Epoch 30/100\n",
      "32/32 - 1s - loss: 0.5615 - accuracy: 0.9428 - precision_m: 0.4966 - recall_m: 0.4004 - f1_m: 0.4206 - val_loss: 0.9994 - val_accuracy: 0.9556 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.51620\n",
      "Epoch 31/100\n",
      "32/32 - 1s - loss: 0.5332 - accuracy: 0.9458 - precision_m: 0.5233 - recall_m: 0.4172 - f1_m: 0.4457 - val_loss: 0.9995 - val_accuracy: 0.9561 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.51620\n",
      "Epoch 32/100\n",
      "32/32 - 1s - loss: 0.6259 - accuracy: 0.9327 - precision_m: 0.4139 - recall_m: 0.3442 - f1_m: 0.3602 - val_loss: 0.9996 - val_accuracy: 0.9325 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.51620\n",
      "Epoch 33/100\n",
      "32/32 - 1s - loss: 0.5864 - accuracy: 0.9366 - precision_m: 0.4353 - recall_m: 0.4094 - f1_m: 0.4083 - val_loss: 0.9996 - val_accuracy: 0.9321 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.51620\n",
      "Epoch 34/100\n",
      "32/32 - 1s - loss: 0.5372 - accuracy: 0.9443 - precision_m: 0.4983 - recall_m: 0.4411 - f1_m: 0.4506 - val_loss: 0.8495 - val_accuracy: 0.9633 - val_precision_m: 0.2912 - val_recall_m: 0.0851 - val_f1_m: 0.1155\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.51620\n",
      "Epoch 35/100\n",
      "32/32 - 1s - loss: 0.4903 - accuracy: 0.9492 - precision_m: 0.5552 - recall_m: 0.4769 - f1_m: 0.4948 - val_loss: 0.9785 - val_accuracy: 0.9614 - val_precision_m: 0.2500 - val_recall_m: 0.0059 - val_f1_m: 0.0116\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.51620\n",
      "Epoch 36/100\n",
      "32/32 - 1s - loss: 0.5443 - accuracy: 0.9441 - precision_m: 0.5043 - recall_m: 0.4149 - f1_m: 0.4364 - val_loss: 0.6634 - val_accuracy: 0.9416 - val_precision_m: 0.4286 - val_recall_m: 0.3545 - val_f1_m: 0.3401\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.51620\n",
      "Epoch 37/100\n",
      "32/32 - 1s - loss: 0.5197 - accuracy: 0.9450 - precision_m: 0.5222 - recall_m: 0.4525 - f1_m: 0.4658 - val_loss: 0.9449 - val_accuracy: 0.9621 - val_precision_m: 0.2500 - val_recall_m: 0.0173 - val_f1_m: 0.0323\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.51620\n",
      "Epoch 38/100\n",
      "32/32 - 1s - loss: 0.4906 - accuracy: 0.9508 - precision_m: 0.5889 - recall_m: 0.4314 - f1_m: 0.4772 - val_loss: 0.8875 - val_accuracy: 0.9615 - val_precision_m: 0.1326 - val_recall_m: 0.0656 - val_f1_m: 0.0878\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.51620\n",
      "Epoch 39/100\n",
      "32/32 - 1s - loss: 0.5341 - accuracy: 0.9461 - precision_m: 0.5314 - recall_m: 0.4090 - f1_m: 0.4422 - val_loss: 0.9997 - val_accuracy: 0.9309 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.51620\n",
      "Epoch 40/100\n",
      "32/32 - 1s - loss: 0.4805 - accuracy: 0.9513 - precision_m: 0.5965 - recall_m: 0.4586 - f1_m: 0.4918 - val_loss: 0.9661 - val_accuracy: 0.9617 - val_precision_m: 0.2500 - val_recall_m: 0.0097 - val_f1_m: 0.0187\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.51620\n",
      "Epoch 41/100\n",
      "32/32 - 1s - loss: 0.5222 - accuracy: 0.9481 - precision_m: 0.5632 - recall_m: 0.3961 - f1_m: 0.4455 - val_loss: 0.9997 - val_accuracy: 0.9454 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.51620\n",
      "Epoch 42/100\n",
      "32/32 - 2s - loss: 0.5089 - accuracy: 0.9502 - precision_m: 0.5718 - recall_m: 0.4152 - f1_m: 0.4601 - val_loss: 0.9998 - val_accuracy: 0.9310 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.51620\n",
      "Epoch 43/100\n",
      "32/32 - 1s - loss: 0.4891 - accuracy: 0.9492 - precision_m: 0.5777 - recall_m: 0.4516 - f1_m: 0.4851 - val_loss: 0.9998 - val_accuracy: 0.9313 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.51620\n",
      "Epoch 44/100\n",
      "32/32 - 1s - loss: 0.4824 - accuracy: 0.9526 - precision_m: 0.5938 - recall_m: 0.4483 - f1_m: 0.4882 - val_loss: 0.9997 - val_accuracy: 0.9304 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.51620\n",
      "Epoch 45/100\n",
      "32/32 - 1s - loss: 0.4500 - accuracy: 0.9543 - precision_m: 0.6275 - recall_m: 0.4784 - f1_m: 0.5208 - val_loss: 0.9998 - val_accuracy: 0.9321 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.51620\n",
      "Epoch 46/100\n",
      "32/32 - 1s - loss: 0.4851 - accuracy: 0.9512 - precision_m: 0.5837 - recall_m: 0.4505 - f1_m: 0.4864 - val_loss: 0.6492 - val_accuracy: 0.9434 - val_precision_m: 0.4300 - val_recall_m: 0.3783 - val_f1_m: 0.3717\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.51620\n",
      "Epoch 47/100\n",
      "32/32 - 1s - loss: 0.4759 - accuracy: 0.9520 - precision_m: 0.6017 - recall_m: 0.4594 - f1_m: 0.4958 - val_loss: 0.6630 - val_accuracy: 0.9478 - val_precision_m: 0.4403 - val_recall_m: 0.3269 - val_f1_m: 0.3355\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.51620\n",
      "Epoch 48/100\n",
      "32/32 - 1s - loss: 0.4449 - accuracy: 0.9547 - precision_m: 0.6442 - recall_m: 0.4688 - f1_m: 0.5208 - val_loss: 0.9564 - val_accuracy: 0.9585 - val_precision_m: 0.2426 - val_recall_m: 0.0134 - val_f1_m: 0.0255\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.51620\n",
      "Epoch 49/100\n",
      "32/32 - 1s - loss: 0.4491 - accuracy: 0.9551 - precision_m: 0.6532 - recall_m: 0.4465 - f1_m: 0.5092 - val_loss: 0.9998 - val_accuracy: 0.9586 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.51620\n",
      "Epoch 50/100\n",
      "32/32 - 1s - loss: 0.4449 - accuracy: 0.9548 - precision_m: 0.6319 - recall_m: 0.4696 - f1_m: 0.5230 - val_loss: 0.9998 - val_accuracy: 0.9309 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.51620\n",
      "Epoch 51/100\n",
      "32/32 - 1s - loss: 0.4820 - accuracy: 0.9521 - precision_m: 0.5983 - recall_m: 0.4284 - f1_m: 0.4835 - val_loss: 0.9998 - val_accuracy: 0.9362 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.51620\n",
      "Epoch 52/100\n",
      "32/32 - 1s - loss: 0.4233 - accuracy: 0.9565 - precision_m: 0.6471 - recall_m: 0.5087 - f1_m: 0.5484 - val_loss: 0.9998 - val_accuracy: 0.9560 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.51620\n",
      "Epoch 53/100\n",
      "32/32 - 1s - loss: 0.4133 - accuracy: 0.9583 - precision_m: 0.7073 - recall_m: 0.4489 - f1_m: 0.5368 - val_loss: 0.9330 - val_accuracy: 0.9460 - val_precision_m: 0.1777 - val_recall_m: 0.0241 - val_f1_m: 0.0424\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.51620\n",
      "Epoch 54/100\n",
      "32/32 - 1s - loss: 0.4184 - accuracy: 0.9563 - precision_m: 0.6530 - recall_m: 0.4828 - f1_m: 0.5470 - val_loss: 0.9998 - val_accuracy: 0.9287 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.51620\n",
      "Epoch 55/100\n",
      "32/32 - 1s - loss: 0.4457 - accuracy: 0.9556 - precision_m: 0.6688 - recall_m: 0.4585 - f1_m: 0.5127 - val_loss: 0.9998 - val_accuracy: 0.9392 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.51620\n",
      "Epoch 56/100\n",
      "32/32 - 1s - loss: 0.4046 - accuracy: 0.9593 - precision_m: 0.6783 - recall_m: 0.5062 - f1_m: 0.5605 - val_loss: 0.9999 - val_accuracy: 0.9290 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.51620\n",
      "Epoch 57/100\n",
      "32/32 - 1s - loss: 0.4513 - accuracy: 0.9549 - precision_m: 0.6508 - recall_m: 0.4474 - f1_m: 0.5074 - val_loss: 0.9999 - val_accuracy: 0.9396 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.51620\n",
      "Epoch 58/100\n",
      "32/32 - 1s - loss: 0.4178 - accuracy: 0.9571 - precision_m: 0.6814 - recall_m: 0.4753 - f1_m: 0.5404 - val_loss: 0.9999 - val_accuracy: 0.9331 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.51620\n",
      "Epoch 59/100\n",
      "32/32 - 1s - loss: 0.4304 - accuracy: 0.9565 - precision_m: 0.6405 - recall_m: 0.4996 - f1_m: 0.5409 - val_loss: 0.9999 - val_accuracy: 0.9312 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.51620\n",
      "Epoch 60/100\n",
      "32/32 - 1s - loss: 0.4300 - accuracy: 0.9554 - precision_m: 0.6561 - recall_m: 0.4760 - f1_m: 0.5339 - val_loss: 0.9999 - val_accuracy: 0.9308 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.51620\n",
      "Epoch 61/100\n",
      "32/32 - 1s - loss: 0.4011 - accuracy: 0.9588 - precision_m: 0.6713 - recall_m: 0.5147 - f1_m: 0.5680 - val_loss: 0.6328 - val_accuracy: 0.9566 - val_precision_m: 0.4653 - val_recall_m: 0.3318 - val_f1_m: 0.3580\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.51620\n",
      "Epoch 62/100\n",
      "32/32 - 1s - loss: 0.4086 - accuracy: 0.9578 - precision_m: 0.6604 - recall_m: 0.5258 - f1_m: 0.5635 - val_loss: 0.7380 - val_accuracy: 0.9182 - val_precision_m: 0.3438 - val_recall_m: 0.2896 - val_f1_m: 0.2473\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.51620\n",
      "Epoch 63/100\n",
      "32/32 - 1s - loss: 0.4260 - accuracy: 0.9560 - precision_m: 0.6391 - recall_m: 0.5099 - f1_m: 0.5474 - val_loss: 0.6961 - val_accuracy: 0.8820 - val_precision_m: 0.2754 - val_recall_m: 0.7294 - val_f1_m: 0.3857\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.51620\n",
      "Epoch 64/100\n",
      "32/32 - 1s - loss: 0.3699 - accuracy: 0.9609 - precision_m: 0.7149 - recall_m: 0.5294 - f1_m: 0.5920 - val_loss: 0.9567 - val_accuracy: 0.9421 - val_precision_m: 0.1276 - val_recall_m: 0.0152 - val_f1_m: 0.0270\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.51620\n",
      "Epoch 65/100\n",
      "32/32 - 1s - loss: 0.4002 - accuracy: 0.9595 - precision_m: 0.6800 - recall_m: 0.5172 - f1_m: 0.5649 - val_loss: 0.6430 - val_accuracy: 0.9357 - val_precision_m: 0.3650 - val_recall_m: 0.4841 - val_f1_m: 0.3877\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.51620\n",
      "Epoch 66/100\n",
      "32/32 - 1s - loss: 0.4011 - accuracy: 0.9599 - precision_m: 0.6987 - recall_m: 0.4911 - f1_m: 0.5570 - val_loss: 0.8920 - val_accuracy: 0.9277 - val_precision_m: 0.0960 - val_recall_m: 0.1048 - val_f1_m: 0.0992\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.51620\n",
      "Epoch 67/100\n",
      "32/32 - 1s - loss: 0.3565 - accuracy: 0.9631 - precision_m: 0.7397 - recall_m: 0.5153 - f1_m: 0.5973 - val_loss: 0.6444 - val_accuracy: 0.9439 - val_precision_m: 0.3879 - val_recall_m: 0.4246 - val_f1_m: 0.3779\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.51620\n",
      "Epoch 68/100\n",
      "32/32 - 1s - loss: 0.3567 - accuracy: 0.9634 - precision_m: 0.7412 - recall_m: 0.5231 - f1_m: 0.5994 - val_loss: 0.9470 - val_accuracy: 0.9338 - val_precision_m: 0.1525 - val_recall_m: 0.0182 - val_f1_m: 0.0325\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.51620\n",
      "Epoch 69/100\n",
      "32/32 - 1s - loss: 0.3433 - accuracy: 0.9638 - precision_m: 0.7381 - recall_m: 0.5623 - f1_m: 0.6213 - val_loss: 0.9839 - val_accuracy: 0.9342 - val_precision_m: 0.1113 - val_recall_m: 0.0049 - val_f1_m: 0.0093\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.51620\n",
      "Epoch 70/100\n",
      "32/32 - 1s - loss: 0.3604 - accuracy: 0.9638 - precision_m: 0.7273 - recall_m: 0.5321 - f1_m: 0.5997 - val_loss: 0.9999 - val_accuracy: 0.9367 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.51620\n",
      "Epoch 71/100\n",
      "32/32 - 1s - loss: 0.3671 - accuracy: 0.9627 - precision_m: 0.7188 - recall_m: 0.5317 - f1_m: 0.5945 - val_loss: 0.9999 - val_accuracy: 0.9349 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.51620\n",
      "Epoch 72/100\n",
      "32/32 - 1s - loss: 0.3722 - accuracy: 0.9614 - precision_m: 0.7049 - recall_m: 0.5381 - f1_m: 0.5933 - val_loss: 0.9999 - val_accuracy: 0.9355 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.51620\n",
      "Epoch 73/100\n",
      "32/32 - 1s - loss: 0.3558 - accuracy: 0.9624 - precision_m: 0.7449 - recall_m: 0.5274 - f1_m: 0.5998 - val_loss: 0.8002 - val_accuracy: 0.9396 - val_precision_m: 0.4085 - val_recall_m: 0.1626 - val_f1_m: 0.1770\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.51620\n",
      "Epoch 74/100\n",
      "32/32 - 1s - loss: 0.3408 - accuracy: 0.9639 - precision_m: 0.7535 - recall_m: 0.5415 - f1_m: 0.6153 - val_loss: 0.8182 - val_accuracy: 0.9283 - val_precision_m: 0.4292 - val_recall_m: 0.1708 - val_f1_m: 0.1704\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.51620\n",
      "Epoch 75/100\n",
      "32/32 - 1s - loss: 0.3146 - accuracy: 0.9668 - precision_m: 0.7640 - recall_m: 0.5937 - f1_m: 0.6502 - val_loss: 0.8400 - val_accuracy: 0.9352 - val_precision_m: 0.4173 - val_recall_m: 0.1127 - val_f1_m: 0.1343\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.51620\n",
      "Epoch 76/100\n",
      "32/32 - 1s - loss: 0.3584 - accuracy: 0.9639 - precision_m: 0.7147 - recall_m: 0.5531 - f1_m: 0.6086 - val_loss: 0.6373 - val_accuracy: 0.9562 - val_precision_m: 0.4665 - val_recall_m: 0.3113 - val_f1_m: 0.3372\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.51620\n",
      "Epoch 77/100\n",
      "32/32 - 2s - loss: 0.3233 - accuracy: 0.9657 - precision_m: 0.7691 - recall_m: 0.5631 - f1_m: 0.6343 - val_loss: 0.8814 - val_accuracy: 0.9419 - val_precision_m: 0.1285 - val_recall_m: 0.0782 - val_f1_m: 0.0959\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.51620\n",
      "Epoch 78/100\n",
      "32/32 - 1s - loss: 0.3068 - accuracy: 0.9674 - precision_m: 0.7809 - recall_m: 0.5894 - f1_m: 0.6535 - val_loss: 0.8617 - val_accuracy: 0.9397 - val_precision_m: 0.2573 - val_recall_m: 0.1183 - val_f1_m: 0.1242\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.51620\n",
      "Epoch 79/100\n",
      "32/32 - 1s - loss: 0.3063 - accuracy: 0.9675 - precision_m: 0.7812 - recall_m: 0.5784 - f1_m: 0.6537 - val_loss: 0.8554 - val_accuracy: 0.9354 - val_precision_m: 0.4852 - val_recall_m: 0.1222 - val_f1_m: 0.1299\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.51620\n",
      "Epoch 80/100\n",
      "32/32 - 1s - loss: 0.2888 - accuracy: 0.9696 - precision_m: 0.7926 - recall_m: 0.5948 - f1_m: 0.6714 - val_loss: 0.9191 - val_accuracy: 0.9393 - val_precision_m: 0.1633 - val_recall_m: 0.0320 - val_f1_m: 0.0535\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.51620\n",
      "Epoch 81/100\n",
      "32/32 - 1s - loss: 0.2921 - accuracy: 0.9696 - precision_m: 0.8122 - recall_m: 0.5741 - f1_m: 0.6598 - val_loss: 0.8870 - val_accuracy: 0.9394 - val_precision_m: 0.1594 - val_recall_m: 0.0567 - val_f1_m: 0.0828\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.51620\n",
      "Epoch 82/100\n",
      "32/32 - 1s - loss: 0.2925 - accuracy: 0.9691 - precision_m: 0.7909 - recall_m: 0.5999 - f1_m: 0.6688 - val_loss: 0.9953 - val_accuracy: 0.9349 - val_precision_m: 0.0705 - val_recall_m: 0.0011 - val_f1_m: 0.0022\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.51620\n",
      "Epoch 83/100\n",
      "32/32 - 1s - loss: 0.2880 - accuracy: 0.9694 - precision_m: 0.8071 - recall_m: 0.5808 - f1_m: 0.6672 - val_loss: 0.9999 - val_accuracy: 0.9302 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.51620\n",
      "Epoch 84/100\n",
      "32/32 - 1s - loss: 0.2911 - accuracy: 0.9689 - precision_m: 0.7903 - recall_m: 0.6101 - f1_m: 0.6711 - val_loss: 0.9999 - val_accuracy: 0.9274 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.51620\n",
      "Epoch 85/100\n",
      "32/32 - 1s - loss: 0.2669 - accuracy: 0.9713 - precision_m: 0.8163 - recall_m: 0.6168 - f1_m: 0.6913 - val_loss: 0.9999 - val_accuracy: 0.9361 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.51620\n",
      "Epoch 86/100\n",
      "32/32 - 1s - loss: 0.2767 - accuracy: 0.9711 - precision_m: 0.8101 - recall_m: 0.6061 - f1_m: 0.6822 - val_loss: 0.9999 - val_accuracy: 0.9290 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.51620\n",
      "Epoch 87/100\n",
      "32/32 - 1s - loss: 0.2709 - accuracy: 0.9709 - precision_m: 0.8170 - recall_m: 0.6073 - f1_m: 0.6874 - val_loss: 0.9999 - val_accuracy: 0.9365 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.51620\n",
      "Epoch 88/100\n",
      "32/32 - 1s - loss: 0.2982 - accuracy: 0.9668 - precision_m: 0.7841 - recall_m: 0.5887 - f1_m: 0.6629 - val_loss: 0.9999 - val_accuracy: 0.9371 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.51620\n",
      "Epoch 89/100\n",
      "32/32 - 1s - loss: 0.2667 - accuracy: 0.9710 - precision_m: 0.8179 - recall_m: 0.6167 - f1_m: 0.6926 - val_loss: 0.9999 - val_accuracy: 0.9368 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.51620\n",
      "Epoch 90/100\n",
      "32/32 - 1s - loss: 0.2543 - accuracy: 0.9717 - precision_m: 0.8511 - recall_m: 0.6008 - f1_m: 0.6959 - val_loss: 0.9999 - val_accuracy: 0.9387 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.51620\n",
      "Epoch 91/100\n",
      "32/32 - 1s - loss: 0.2842 - accuracy: 0.9706 - precision_m: 0.8105 - recall_m: 0.5984 - f1_m: 0.6724 - val_loss: 0.9999 - val_accuracy: 0.9320 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.51620\n",
      "Epoch 92/100\n",
      "32/32 - 1s - loss: 0.2922 - accuracy: 0.9691 - precision_m: 0.7962 - recall_m: 0.5840 - f1_m: 0.6628 - val_loss: 0.8963 - val_accuracy: 0.9423 - val_precision_m: 0.1611 - val_recall_m: 0.0477 - val_f1_m: 0.0737\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.51620\n",
      "Epoch 93/100\n",
      "32/32 - 1s - loss: 0.2853 - accuracy: 0.9690 - precision_m: 0.8105 - recall_m: 0.5947 - f1_m: 0.6724 - val_loss: 0.8857 - val_accuracy: 0.9418 - val_precision_m: 0.1476 - val_recall_m: 0.0638 - val_f1_m: 0.0878\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.51620\n",
      "Epoch 94/100\n",
      "32/32 - 1s - loss: 0.2609 - accuracy: 0.9724 - precision_m: 0.8308 - recall_m: 0.6078 - f1_m: 0.6938 - val_loss: 0.9142 - val_accuracy: 0.9415 - val_precision_m: 0.1679 - val_recall_m: 0.0343 - val_f1_m: 0.0570\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.51620\n",
      "Epoch 95/100\n",
      "32/32 - 1s - loss: 0.2370 - accuracy: 0.9731 - precision_m: 0.8525 - recall_m: 0.6259 - f1_m: 0.7172 - val_loss: 0.8502 - val_accuracy: 0.9414 - val_precision_m: 0.4167 - val_recall_m: 0.1079 - val_f1_m: 0.1320\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.51620\n",
      "Epoch 96/100\n",
      "32/32 - 1s - loss: 0.2423 - accuracy: 0.9723 - precision_m: 0.8664 - recall_m: 0.6088 - f1_m: 0.7057 - val_loss: 0.8834 - val_accuracy: 0.9440 - val_precision_m: 0.1850 - val_recall_m: 0.0688 - val_f1_m: 0.0931\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.51620\n",
      "Epoch 97/100\n",
      "32/32 - 1s - loss: 0.2664 - accuracy: 0.9712 - precision_m: 0.8395 - recall_m: 0.5872 - f1_m: 0.6827 - val_loss: 0.8674 - val_accuracy: 0.9367 - val_precision_m: 0.3392 - val_recall_m: 0.0894 - val_f1_m: 0.1126\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.51620\n",
      "Epoch 98/100\n",
      "32/32 - 1s - loss: 0.2372 - accuracy: 0.9731 - precision_m: 0.8610 - recall_m: 0.6213 - f1_m: 0.7146 - val_loss: 0.8314 - val_accuracy: 0.9348 - val_precision_m: 0.3679 - val_recall_m: 0.1297 - val_f1_m: 0.1521\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.51620\n",
      "Epoch 99/100\n",
      "32/32 - 1s - loss: 0.2554 - accuracy: 0.9725 - precision_m: 0.8378 - recall_m: 0.6149 - f1_m: 0.7001 - val_loss: 0.8977 - val_accuracy: 0.9371 - val_precision_m: 0.1473 - val_recall_m: 0.0498 - val_f1_m: 0.0744\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.51620\n",
      "Epoch 100/100\n",
      "32/32 - 1s - loss: 0.2230 - accuracy: 0.9744 - precision_m: 0.8708 - recall_m: 0.6343 - f1_m: 0.7291 - val_loss: 0.8722 - val_accuracy: 0.9356 - val_precision_m: 0.3523 - val_recall_m: 0.0877 - val_f1_m: 0.1097\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.51620\n",
      "5/5 - 0s - loss: 0.6521 - accuracy: 0.9363 - precision_m: 0.4961 - recall_m: 0.2892 - f1_m: 0.3339\n",
      "The metrics for the test set with loss tversky, learning rate 0.001,  filters 32, and batch size 8 is [0.6521031856536865, 0.9363242983818054, 0.49610430002212524, 0.2891850769519806, 0.3339446187019348]\n",
      "__________________________________________________________________________________________________________________________________________________________________________\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 128, 128, 5) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 128, 128, 8)  368         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 128, 128, 8)  32          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 128, 128, 8)  0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 8)  584         activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 8)  32          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, 128, 8)  0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 64, 64, 8)    0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 16)   1168        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64, 64, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 64, 64, 16)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 64, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64, 64, 16)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 16)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 32)   4640        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 32)   128         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 32)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 32)   9248        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 32)   128         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 32)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 32)   0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 16, 16, 64)   18496       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 16, 16, 64)   256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 16, 16, 64)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 16, 16, 64)   36928       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 16, 16, 64)   256         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16, 16, 64)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 64)     0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 8, 8, 128)    73856       max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 8, 8, 128)    512         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 8, 8, 128)    0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 8, 8, 128)    147584      activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 8, 8, 128)    512         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 8, 8, 128)    0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 16, 16, 128)  65664       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 16, 16, 192)  0           conv2d_transpose[0][0]           \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 64)   110656      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 64)   256         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 64)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 64)   36928       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 64)   256         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 64)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 32, 32, 64)   16448       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 96)   0           conv2d_transpose_1[0][0]         \n",
      "                                                                 activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 32)   27680       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 32, 32, 32)   128         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 32, 32, 32)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 32, 32)   9248        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 32, 32, 32)   128         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 32, 32, 32)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 64, 64, 32)   4128        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 64, 64, 48)   0           conv2d_transpose_2[0][0]         \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 16)   6928        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64, 64, 16)   64          conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 64, 64, 16)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 16)   2320        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 64, 64, 16)   64          conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 64, 64, 16)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 128, 128, 16) 1040        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 128, 128, 24) 0           conv2d_transpose_3[0][0]         \n",
      "                                                                 activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 128, 128, 8)  1736        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 128, 128, 8)  32          conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 128, 128, 8)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 128, 128, 8)  584         activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 128, 128, 8)  32          conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 128, 128, 8)  0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 128, 128, 1)  9           activation_17[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 581,505\n",
      "Trainable params: 580,033\n",
      "Non-trainable params: 1,472\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "Initialising in 3..........2..........1......................................\n",
      "Training on 284 images with learning rate 0.001, batch size 16, and number of filters 8\n",
      "Epoch 1/100\n",
      "16/16 - 37s - loss: 0.9152 - accuracy: 0.5478 - precision_m: 0.0660 - recall_m: 0.5924 - f1_m: 0.1138 - val_loss: 0.9399 - val_accuracy: 0.4882 - val_precision_m: 0.0486 - val_recall_m: 0.6846 - val_f1_m: 0.0906\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.51620\n",
      "Epoch 2/100\n",
      "16/16 - 0s - loss: 0.8955 - accuracy: 0.4824 - precision_m: 0.0836 - recall_m: 0.7981 - f1_m: 0.1496 - val_loss: 0.9391 - val_accuracy: 0.3780 - val_precision_m: 0.0472 - val_recall_m: 0.8023 - val_f1_m: 0.0890\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.51620\n",
      "Epoch 3/100\n",
      "16/16 - 0s - loss: 0.8916 - accuracy: 0.5708 - precision_m: 0.0857 - recall_m: 0.6840 - f1_m: 0.1510 - val_loss: 0.9332 - val_accuracy: 0.4290 - val_precision_m: 0.0494 - val_recall_m: 0.7746 - val_f1_m: 0.0928\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.51620\n",
      "Epoch 4/100\n",
      "16/16 - 0s - loss: 0.8968 - accuracy: 0.6607 - precision_m: 0.0833 - recall_m: 0.5099 - f1_m: 0.1401 - val_loss: 0.9369 - val_accuracy: 0.3947 - val_precision_m: 0.0488 - val_recall_m: 0.8076 - val_f1_m: 0.0919\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.51620\n",
      "Epoch 5/100\n",
      "16/16 - 0s - loss: 0.8865 - accuracy: 0.6167 - precision_m: 0.0903 - recall_m: 0.6303 - f1_m: 0.1564 - val_loss: 0.9305 - val_accuracy: 0.6927 - val_precision_m: 0.0499 - val_recall_m: 0.3900 - val_f1_m: 0.0884\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.51620\n",
      "Epoch 6/100\n",
      "16/16 - 0s - loss: 0.8808 - accuracy: 0.6434 - precision_m: 0.0957 - recall_m: 0.6261 - f1_m: 0.1639 - val_loss: 0.9346 - val_accuracy: 0.4936 - val_precision_m: 0.0461 - val_recall_m: 0.6216 - val_f1_m: 0.0858\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.51620\n",
      "Epoch 7/100\n",
      "16/16 - 0s - loss: 0.8776 - accuracy: 0.6470 - precision_m: 0.0974 - recall_m: 0.6191 - f1_m: 0.1664 - val_loss: 0.9371 - val_accuracy: 0.5266 - val_precision_m: 0.0462 - val_recall_m: 0.5760 - val_f1_m: 0.0855\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.51620\n",
      "Epoch 8/100\n",
      "16/16 - 0s - loss: 0.8737 - accuracy: 0.6785 - precision_m: 0.1011 - recall_m: 0.5854 - f1_m: 0.1708 - val_loss: 0.9387 - val_accuracy: 0.7194 - val_precision_m: 0.0456 - val_recall_m: 0.3192 - val_f1_m: 0.0798\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.51620\n",
      "Epoch 9/100\n",
      "16/16 - 0s - loss: 0.8833 - accuracy: 0.7185 - precision_m: 0.0951 - recall_m: 0.4690 - f1_m: 0.1564 - val_loss: 0.9403 - val_accuracy: 0.6069 - val_precision_m: 0.0411 - val_recall_m: 0.4148 - val_f1_m: 0.0747\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.51620\n",
      "Epoch 10/100\n",
      "16/16 - 0s - loss: 0.8689 - accuracy: 0.7117 - precision_m: 0.1069 - recall_m: 0.5513 - f1_m: 0.1744 - val_loss: 0.9361 - val_accuracy: 0.4837 - val_precision_m: 0.0455 - val_recall_m: 0.6239 - val_f1_m: 0.0848\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.51620\n",
      "Epoch 11/100\n",
      "16/16 - 0s - loss: 0.8362 - accuracy: 0.7756 - precision_m: 0.1442 - recall_m: 0.6033 - f1_m: 0.2294 - val_loss: 0.9338 - val_accuracy: 0.3442 - val_precision_m: 0.0480 - val_recall_m: 0.8719 - val_f1_m: 0.0910\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.51620\n",
      "Epoch 12/100\n",
      "16/16 - 0s - loss: 0.8095 - accuracy: 0.7917 - precision_m: 0.1660 - recall_m: 0.6752 - f1_m: 0.2627 - val_loss: 0.9251 - val_accuracy: 0.5114 - val_precision_m: 0.0574 - val_recall_m: 0.7639 - val_f1_m: 0.1067\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.51620\n",
      "Epoch 13/100\n",
      "16/16 - 0s - loss: 0.8124 - accuracy: 0.7992 - precision_m: 0.1668 - recall_m: 0.6396 - f1_m: 0.2602 - val_loss: 0.9436 - val_accuracy: 0.0798 - val_precision_m: 0.0396 - val_recall_m: 0.9932 - val_f1_m: 0.0760\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.51620\n",
      "Epoch 14/100\n",
      "16/16 - 0s - loss: 0.7698 - accuracy: 0.8359 - precision_m: 0.2084 - recall_m: 0.6873 - f1_m: 0.3172 - val_loss: 0.9375 - val_accuracy: 0.2190 - val_precision_m: 0.0449 - val_recall_m: 0.9618 - val_f1_m: 0.0856\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.51620\n",
      "Epoch 15/100\n",
      "16/16 - 0s - loss: 0.7678 - accuracy: 0.8421 - precision_m: 0.2073 - recall_m: 0.6621 - f1_m: 0.3121 - val_loss: 0.9135 - val_accuracy: 0.6161 - val_precision_m: 0.0677 - val_recall_m: 0.7086 - val_f1_m: 0.1234\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.51620\n",
      "Epoch 16/100\n",
      "16/16 - 0s - loss: 0.7516 - accuracy: 0.8365 - precision_m: 0.2104 - recall_m: 0.6916 - f1_m: 0.3192 - val_loss: 0.9239 - val_accuracy: 0.4719 - val_precision_m: 0.0568 - val_recall_m: 0.8234 - val_f1_m: 0.1063\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.51620\n",
      "Epoch 17/100\n",
      "16/16 - 0s - loss: 0.7276 - accuracy: 0.8649 - precision_m: 0.2438 - recall_m: 0.6754 - f1_m: 0.3507 - val_loss: 0.9082 - val_accuracy: 0.5595 - val_precision_m: 0.0689 - val_recall_m: 0.8326 - val_f1_m: 0.1273\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.51620\n",
      "Epoch 18/100\n",
      "16/16 - 0s - loss: 0.7374 - accuracy: 0.8684 - precision_m: 0.2340 - recall_m: 0.5904 - f1_m: 0.3300 - val_loss: 0.9741 - val_accuracy: 0.8935 - val_precision_m: 0.0182 - val_recall_m: 0.0222 - val_f1_m: 0.0199\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.51620\n",
      "Epoch 19/100\n",
      "16/16 - 0s - loss: 0.7072 - accuracy: 0.8795 - precision_m: 0.2646 - recall_m: 0.6143 - f1_m: 0.3623 - val_loss: 0.9727 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.51620\n",
      "Epoch 20/100\n",
      "16/16 - 0s - loss: 0.7161 - accuracy: 0.8935 - precision_m: 0.2651 - recall_m: 0.5449 - f1_m: 0.3486 - val_loss: 0.9766 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.51620\n",
      "Epoch 21/100\n",
      "16/16 - 1s - loss: 0.6866 - accuracy: 0.8980 - precision_m: 0.2903 - recall_m: 0.5577 - f1_m: 0.3748 - val_loss: 0.9483 - val_accuracy: 0.9298 - val_precision_m: 0.0325 - val_recall_m: 0.0467 - val_f1_m: 0.0383\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.51620\n",
      "Epoch 22/100\n",
      "16/16 - 0s - loss: 0.6861 - accuracy: 0.8982 - precision_m: 0.2886 - recall_m: 0.5466 - f1_m: 0.3697 - val_loss: 0.9876 - val_accuracy: 0.9322 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.51620\n",
      "Epoch 23/100\n",
      "16/16 - 0s - loss: 0.6664 - accuracy: 0.9110 - precision_m: 0.3216 - recall_m: 0.5235 - f1_m: 0.3882 - val_loss: 0.8330 - val_accuracy: 0.9261 - val_precision_m: 0.1808 - val_recall_m: 0.2483 - val_f1_m: 0.2092\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.51620\n",
      "Epoch 24/100\n",
      "16/16 - 0s - loss: 0.6907 - accuracy: 0.9078 - precision_m: 0.2975 - recall_m: 0.4708 - f1_m: 0.3521 - val_loss: 0.9729 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.51620\n",
      "Epoch 25/100\n",
      "16/16 - 0s - loss: 0.7079 - accuracy: 0.9013 - precision_m: 0.2700 - recall_m: 0.4640 - f1_m: 0.3327 - val_loss: 0.9844 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.51620\n",
      "Epoch 26/100\n",
      "16/16 - 0s - loss: 0.6687 - accuracy: 0.9003 - precision_m: 0.2968 - recall_m: 0.5736 - f1_m: 0.3805 - val_loss: 0.9874 - val_accuracy: 0.9527 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.51620\n",
      "Epoch 27/100\n",
      "16/16 - 0s - loss: 0.6474 - accuracy: 0.9175 - precision_m: 0.3447 - recall_m: 0.4931 - f1_m: 0.3912 - val_loss: 0.9881 - val_accuracy: 0.9440 - val_precision_m: 0.0026 - val_recall_m: 0.0011 - val_f1_m: 0.0015\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.51620\n",
      "Epoch 28/100\n",
      "16/16 - 0s - loss: 0.6052 - accuracy: 0.9212 - precision_m: 0.3713 - recall_m: 0.5741 - f1_m: 0.4421 - val_loss: 0.9869 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.51620\n",
      "Epoch 29/100\n",
      "16/16 - 0s - loss: 0.6156 - accuracy: 0.9240 - precision_m: 0.3809 - recall_m: 0.4808 - f1_m: 0.4161 - val_loss: 0.9896 - val_accuracy: 0.9567 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.51620\n",
      "Epoch 30/100\n",
      "16/16 - 0s - loss: 0.6139 - accuracy: 0.9304 - precision_m: 0.4036 - recall_m: 0.4533 - f1_m: 0.4081 - val_loss: 0.9899 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.51620\n",
      "Epoch 31/100\n",
      "16/16 - 0s - loss: 0.5596 - accuracy: 0.9396 - precision_m: 0.4681 - recall_m: 0.4712 - f1_m: 0.4557 - val_loss: 0.9923 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.51620\n",
      "Epoch 32/100\n",
      "16/16 - 0s - loss: 0.5716 - accuracy: 0.9355 - precision_m: 0.4326 - recall_m: 0.4918 - f1_m: 0.4494 - val_loss: 0.9934 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.51620\n",
      "Epoch 33/100\n",
      "16/16 - 0s - loss: 0.5534 - accuracy: 0.9432 - precision_m: 0.5024 - recall_m: 0.4319 - f1_m: 0.4452 - val_loss: 0.9941 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.51620\n",
      "Epoch 34/100\n",
      "16/16 - 0s - loss: 0.5365 - accuracy: 0.9427 - precision_m: 0.5039 - recall_m: 0.4637 - f1_m: 0.4676 - val_loss: 0.8463 - val_accuracy: 0.9605 - val_precision_m: 0.2325 - val_recall_m: 0.0825 - val_f1_m: 0.1217\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.51620\n",
      "Epoch 35/100\n",
      "16/16 - 0s - loss: 0.4949 - accuracy: 0.9478 - precision_m: 0.5429 - recall_m: 0.4985 - f1_m: 0.5076 - val_loss: 0.9873 - val_accuracy: 0.9569 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.51620\n",
      "Epoch 36/100\n",
      "16/16 - 0s - loss: 0.5005 - accuracy: 0.9488 - precision_m: 0.5669 - recall_m: 0.4685 - f1_m: 0.4906 - val_loss: 0.9957 - val_accuracy: 0.9558 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.51620\n",
      "Epoch 37/100\n",
      "16/16 - 0s - loss: 0.4757 - accuracy: 0.9527 - precision_m: 0.6255 - recall_m: 0.4341 - f1_m: 0.4993 - val_loss: 0.9530 - val_accuracy: 0.9114 - val_precision_m: 0.0414 - val_recall_m: 0.0512 - val_f1_m: 0.0453\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.51620\n",
      "Epoch 38/100\n",
      "16/16 - 0s - loss: 0.4688 - accuracy: 0.9516 - precision_m: 0.5886 - recall_m: 0.4828 - f1_m: 0.5174 - val_loss: 0.9919 - val_accuracy: 0.8649 - val_precision_m: 0.0054 - val_recall_m: 0.0141 - val_f1_m: 0.0076\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.51620\n",
      "Epoch 39/100\n",
      "16/16 - 0s - loss: 0.4593 - accuracy: 0.9542 - precision_m: 0.6390 - recall_m: 0.4437 - f1_m: 0.5104 - val_loss: 0.9975 - val_accuracy: 0.8839 - val_precision_m: 0.0012 - val_recall_m: 0.0038 - val_f1_m: 0.0018\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.51620\n",
      "Epoch 40/100\n",
      "16/16 - 0s - loss: 0.4113 - accuracy: 0.9580 - precision_m: 0.6664 - recall_m: 0.4977 - f1_m: 0.5642 - val_loss: 0.7828 - val_accuracy: 0.9440 - val_precision_m: 0.2330 - val_recall_m: 0.1983 - val_f1_m: 0.2142\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.51620\n",
      "Epoch 41/100\n",
      "16/16 - 0s - loss: 0.3840 - accuracy: 0.9604 - precision_m: 0.6966 - recall_m: 0.5310 - f1_m: 0.5913 - val_loss: 0.6785 - val_accuracy: 0.9417 - val_precision_m: 0.3085 - val_recall_m: 0.3811 - val_f1_m: 0.3405\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.51620\n",
      "Epoch 42/100\n",
      "16/16 - 0s - loss: 0.4167 - accuracy: 0.9559 - precision_m: 0.6546 - recall_m: 0.5013 - f1_m: 0.5601 - val_loss: 0.8647 - val_accuracy: 0.9600 - val_precision_m: 0.4637 - val_recall_m: 0.0480 - val_f1_m: 0.0849\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.51620\n",
      "Epoch 43/100\n",
      "16/16 - 0s - loss: 0.3690 - accuracy: 0.9615 - precision_m: 0.7099 - recall_m: 0.5434 - f1_m: 0.6062 - val_loss: 0.9740 - val_accuracy: 0.9606 - val_precision_m: 0.1093 - val_recall_m: 0.0039 - val_f1_m: 0.0075\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.51620\n",
      "Epoch 44/100\n",
      "16/16 - 0s - loss: 0.3525 - accuracy: 0.9635 - precision_m: 0.7545 - recall_m: 0.5212 - f1_m: 0.6106 - val_loss: 0.7573 - val_accuracy: 0.9604 - val_precision_m: 0.4761 - val_recall_m: 0.1135 - val_f1_m: 0.1804\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.51620\n",
      "Epoch 45/100\n",
      "16/16 - 0s - loss: 0.3715 - accuracy: 0.9617 - precision_m: 0.7055 - recall_m: 0.5409 - f1_m: 0.6038 - val_loss: 0.7646 - val_accuracy: 0.9474 - val_precision_m: 0.2644 - val_recall_m: 0.2095 - val_f1_m: 0.2300\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.51620\n",
      "Epoch 46/100\n",
      "16/16 - 0s - loss: 0.4306 - accuracy: 0.9566 - precision_m: 0.6413 - recall_m: 0.5020 - f1_m: 0.5469 - val_loss: 0.9252 - val_accuracy: 0.8148 - val_precision_m: 0.0569 - val_recall_m: 0.2567 - val_f1_m: 0.0917\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.51620\n",
      "Epoch 47/100\n",
      "16/16 - 0s - loss: 0.3335 - accuracy: 0.9646 - precision_m: 0.7630 - recall_m: 0.5436 - f1_m: 0.6312 - val_loss: 0.7206 - val_accuracy: 0.9590 - val_precision_m: 0.4353 - val_recall_m: 0.1715 - val_f1_m: 0.2454\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.51620\n",
      "Epoch 48/100\n",
      "16/16 - 0s - loss: 0.3757 - accuracy: 0.9611 - precision_m: 0.7204 - recall_m: 0.5228 - f1_m: 0.5913 - val_loss: 0.6566 - val_accuracy: 0.9602 - val_precision_m: 0.4896 - val_recall_m: 0.2180 - val_f1_m: 0.3016\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.51620\n",
      "Epoch 49/100\n",
      "16/16 - 0s - loss: 0.3370 - accuracy: 0.9651 - precision_m: 0.7577 - recall_m: 0.5654 - f1_m: 0.6318 - val_loss: 0.8633 - val_accuracy: 0.8740 - val_precision_m: 0.1099 - val_recall_m: 0.3373 - val_f1_m: 0.1631\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.51620\n",
      "Epoch 50/100\n",
      "16/16 - 0s - loss: 0.3270 - accuracy: 0.9653 - precision_m: 0.7571 - recall_m: 0.5691 - f1_m: 0.6435 - val_loss: 0.7445 - val_accuracy: 0.9607 - val_precision_m: 0.4862 - val_recall_m: 0.1329 - val_f1_m: 0.2086\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.51620\n",
      "Epoch 51/100\n",
      "16/16 - 0s - loss: 0.3046 - accuracy: 0.9674 - precision_m: 0.7917 - recall_m: 0.5711 - f1_m: 0.6596 - val_loss: 0.6573 - val_accuracy: 0.9613 - val_precision_m: 0.5108 - val_recall_m: 0.2029 - val_f1_m: 0.2901\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.51620\n",
      "Epoch 52/100\n",
      "16/16 - 0s - loss: 0.3048 - accuracy: 0.9673 - precision_m: 0.7843 - recall_m: 0.5899 - f1_m: 0.6640 - val_loss: 0.7270 - val_accuracy: 0.9621 - val_precision_m: 0.6330 - val_recall_m: 0.1230 - val_f1_m: 0.1976\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.51620\n",
      "Epoch 53/100\n",
      "16/16 - 0s - loss: 0.3421 - accuracy: 0.9639 - precision_m: 0.7571 - recall_m: 0.5425 - f1_m: 0.6223 - val_loss: 0.9976 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.51620\n",
      "Epoch 54/100\n",
      "16/16 - 0s - loss: 0.3135 - accuracy: 0.9669 - precision_m: 0.7849 - recall_m: 0.5643 - f1_m: 0.6485 - val_loss: 0.9935 - val_accuracy: 0.9582 - val_precision_m: 0.0040 - val_recall_m: 4.6355e-04 - val_f1_m: 8.3037e-04\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.51620\n",
      "Epoch 55/100\n",
      "16/16 - 0s - loss: 0.3178 - accuracy: 0.9665 - precision_m: 0.7851 - recall_m: 0.5576 - f1_m: 0.6437 - val_loss: 0.9983 - val_accuracy: 0.9590 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.51620\n",
      "Epoch 56/100\n",
      "16/16 - 0s - loss: 0.2907 - accuracy: 0.9685 - precision_m: 0.8065 - recall_m: 0.5931 - f1_m: 0.6744 - val_loss: 0.9927 - val_accuracy: 0.9608 - val_precision_m: 0.0342 - val_recall_m: 4.2141e-04 - val_f1_m: 8.3257e-04\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.51620\n",
      "Epoch 57/100\n",
      "16/16 - 0s - loss: 0.3285 - accuracy: 0.9656 - precision_m: 0.7692 - recall_m: 0.5583 - f1_m: 0.6352 - val_loss: 0.9991 - val_accuracy: 0.9377 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.51620\n",
      "Epoch 58/100\n",
      "16/16 - 0s - loss: 0.2906 - accuracy: 0.9682 - precision_m: 0.8283 - recall_m: 0.5614 - f1_m: 0.6632 - val_loss: 0.9989 - val_accuracy: 0.9353 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.51620\n",
      "Epoch 59/100\n",
      "16/16 - 0s - loss: 0.3217 - accuracy: 0.9657 - precision_m: 0.7723 - recall_m: 0.5674 - f1_m: 0.6436 - val_loss: 0.9867 - val_accuracy: 0.9607 - val_precision_m: 0.1027 - val_recall_m: 0.0026 - val_f1_m: 0.0050\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.51620\n",
      "Epoch 60/100\n",
      "16/16 - 0s - loss: 0.2899 - accuracy: 0.9691 - precision_m: 0.7991 - recall_m: 0.6018 - f1_m: 0.6780 - val_loss: 0.9710 - val_accuracy: 0.9608 - val_precision_m: 0.1821 - val_recall_m: 0.0084 - val_f1_m: 0.0161\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.51620\n",
      "Epoch 61/100\n",
      "16/16 - 0s - loss: 0.2800 - accuracy: 0.9693 - precision_m: 0.8314 - recall_m: 0.5710 - f1_m: 0.6736 - val_loss: 0.9986 - val_accuracy: 0.9555 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.51620\n",
      "Epoch 62/100\n",
      "16/16 - 0s - loss: 0.2875 - accuracy: 0.9688 - precision_m: 0.8044 - recall_m: 0.6043 - f1_m: 0.6782 - val_loss: 0.8589 - val_accuracy: 0.9617 - val_precision_m: 0.7001 - val_recall_m: 0.0508 - val_f1_m: 0.0942\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.51620\n",
      "Epoch 63/100\n",
      "16/16 - 1s - loss: 0.2957 - accuracy: 0.9682 - precision_m: 0.8059 - recall_m: 0.5806 - f1_m: 0.6650 - val_loss: 0.6737 - val_accuracy: 0.9616 - val_precision_m: 0.5598 - val_recall_m: 0.1734 - val_f1_m: 0.2567\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.51620\n",
      "Epoch 64/100\n",
      "16/16 - 0s - loss: 0.2714 - accuracy: 0.9705 - precision_m: 0.8300 - recall_m: 0.6004 - f1_m: 0.6881 - val_loss: 0.6872 - val_accuracy: 0.9626 - val_precision_m: 0.7272 - val_recall_m: 0.1539 - val_f1_m: 0.2375\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.51620\n",
      "Epoch 65/100\n",
      "16/16 - 0s - loss: 0.2552 - accuracy: 0.9718 - precision_m: 0.8609 - recall_m: 0.5887 - f1_m: 0.6957 - val_loss: 0.6394 - val_accuracy: 0.9616 - val_precision_m: 0.6037 - val_recall_m: 0.2089 - val_f1_m: 0.3001\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.51620\n",
      "Epoch 66/100\n",
      "16/16 - 0s - loss: 0.2581 - accuracy: 0.9716 - precision_m: 0.8428 - recall_m: 0.6052 - f1_m: 0.7003 - val_loss: 0.6375 - val_accuracy: 0.9635 - val_precision_m: 0.7339 - val_recall_m: 0.1907 - val_f1_m: 0.2938\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.51620\n",
      "Epoch 67/100\n",
      "16/16 - 0s - loss: 0.2591 - accuracy: 0.9716 - precision_m: 0.8434 - recall_m: 0.6068 - f1_m: 0.7003 - val_loss: 0.6345 - val_accuracy: 0.9638 - val_precision_m: 0.6738 - val_recall_m: 0.1919 - val_f1_m: 0.2943\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.51620\n",
      "Epoch 68/100\n",
      "16/16 - 0s - loss: 0.2464 - accuracy: 0.9721 - precision_m: 0.8554 - recall_m: 0.6141 - f1_m: 0.7110 - val_loss: 0.7852 - val_accuracy: 0.9628 - val_precision_m: 0.7570 - val_recall_m: 0.0842 - val_f1_m: 0.1497\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.51620\n",
      "Epoch 69/100\n",
      "16/16 - 0s - loss: 0.2664 - accuracy: 0.9713 - precision_m: 0.8332 - recall_m: 0.6058 - f1_m: 0.6948 - val_loss: 0.7125 - val_accuracy: 0.9636 - val_precision_m: 0.7576 - val_recall_m: 0.1217 - val_f1_m: 0.2051\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.51620\n",
      "Epoch 70/100\n",
      "16/16 - 0s - loss: 0.2488 - accuracy: 0.9730 - precision_m: 0.8358 - recall_m: 0.6402 - f1_m: 0.7176 - val_loss: 0.6728 - val_accuracy: 0.9637 - val_precision_m: 0.7276 - val_recall_m: 0.1533 - val_f1_m: 0.2481\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.51620\n",
      "Epoch 71/100\n",
      "16/16 - 0s - loss: 0.2516 - accuracy: 0.9729 - precision_m: 0.8388 - recall_m: 0.6347 - f1_m: 0.7140 - val_loss: 0.6419 - val_accuracy: 0.9636 - val_precision_m: 0.7218 - val_recall_m: 0.1828 - val_f1_m: 0.2882\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.51620\n",
      "Epoch 72/100\n",
      "16/16 - 0s - loss: 0.2732 - accuracy: 0.9696 - precision_m: 0.8150 - recall_m: 0.6225 - f1_m: 0.6926 - val_loss: 0.6491 - val_accuracy: 0.9614 - val_precision_m: 0.6044 - val_recall_m: 0.1931 - val_f1_m: 0.2890\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.51620\n",
      "Epoch 73/100\n",
      "16/16 - 0s - loss: 0.2423 - accuracy: 0.9733 - precision_m: 0.8406 - recall_m: 0.6451 - f1_m: 0.7241 - val_loss: 0.6316 - val_accuracy: 0.9616 - val_precision_m: 0.6023 - val_recall_m: 0.2122 - val_f1_m: 0.3091\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.51620\n",
      "Epoch 74/100\n",
      "16/16 - 0s - loss: 0.2439 - accuracy: 0.9733 - precision_m: 0.8529 - recall_m: 0.6275 - f1_m: 0.7174 - val_loss: 0.6406 - val_accuracy: 0.9638 - val_precision_m: 0.6906 - val_recall_m: 0.1823 - val_f1_m: 0.2871\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.51620\n",
      "Epoch 75/100\n",
      "16/16 - 0s - loss: 0.2632 - accuracy: 0.9717 - precision_m: 0.8397 - recall_m: 0.6102 - f1_m: 0.6955 - val_loss: 0.6013 - val_accuracy: 0.9630 - val_precision_m: 0.6158 - val_recall_m: 0.2320 - val_f1_m: 0.3354\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.51620\n",
      "Epoch 76/100\n",
      "16/16 - 0s - loss: 0.2521 - accuracy: 0.9725 - precision_m: 0.8419 - recall_m: 0.6197 - f1_m: 0.7091 - val_loss: 0.6196 - val_accuracy: 0.9645 - val_precision_m: 0.7294 - val_recall_m: 0.1959 - val_f1_m: 0.3071\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.51620\n",
      "Epoch 77/100\n",
      "16/16 - 0s - loss: 0.2509 - accuracy: 0.9729 - precision_m: 0.8428 - recall_m: 0.6216 - f1_m: 0.7109 - val_loss: 0.6459 - val_accuracy: 0.9643 - val_precision_m: 0.7469 - val_recall_m: 0.1699 - val_f1_m: 0.2760\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.51620\n",
      "Epoch 78/100\n",
      "16/16 - 0s - loss: 0.2388 - accuracy: 0.9736 - precision_m: 0.8582 - recall_m: 0.6259 - f1_m: 0.7203 - val_loss: 0.6046 - val_accuracy: 0.9647 - val_precision_m: 0.7333 - val_recall_m: 0.2075 - val_f1_m: 0.3210\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.51620\n",
      "Epoch 79/100\n",
      "16/16 - 0s - loss: 0.2401 - accuracy: 0.9737 - precision_m: 0.8634 - recall_m: 0.6175 - f1_m: 0.7163 - val_loss: 0.6418 - val_accuracy: 0.9644 - val_precision_m: 0.7431 - val_recall_m: 0.1742 - val_f1_m: 0.2812\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.51620\n",
      "Epoch 80/100\n",
      "16/16 - 0s - loss: 0.2276 - accuracy: 0.9742 - precision_m: 0.8729 - recall_m: 0.6317 - f1_m: 0.7294 - val_loss: 0.6214 - val_accuracy: 0.9646 - val_precision_m: 0.7427 - val_recall_m: 0.1909 - val_f1_m: 0.3028\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.51620\n",
      "Epoch 81/100\n",
      "16/16 - 0s - loss: 0.2557 - accuracy: 0.9722 - precision_m: 0.8353 - recall_m: 0.6270 - f1_m: 0.7076 - val_loss: 0.6272 - val_accuracy: 0.9649 - val_precision_m: 0.7637 - val_recall_m: 0.1794 - val_f1_m: 0.2899\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.51620\n",
      "Epoch 82/100\n",
      "16/16 - 0s - loss: 0.2619 - accuracy: 0.9726 - precision_m: 0.8249 - recall_m: 0.6296 - f1_m: 0.7066 - val_loss: 0.6066 - val_accuracy: 0.9652 - val_precision_m: 0.7629 - val_recall_m: 0.1967 - val_f1_m: 0.3117\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.51620\n",
      "Epoch 83/100\n",
      "16/16 - 0s - loss: 0.2360 - accuracy: 0.9735 - precision_m: 0.8624 - recall_m: 0.6261 - f1_m: 0.7211 - val_loss: 0.6325 - val_accuracy: 0.9649 - val_precision_m: 0.7663 - val_recall_m: 0.1771 - val_f1_m: 0.2876\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.51620\n",
      "Epoch 84/100\n",
      "16/16 - 0s - loss: 0.2520 - accuracy: 0.9718 - precision_m: 0.8395 - recall_m: 0.6227 - f1_m: 0.7089 - val_loss: 0.6331 - val_accuracy: 0.9648 - val_precision_m: 0.7506 - val_recall_m: 0.1790 - val_f1_m: 0.2889\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.51620\n",
      "Epoch 85/100\n",
      "16/16 - 0s - loss: 0.2318 - accuracy: 0.9739 - precision_m: 0.8614 - recall_m: 0.6403 - f1_m: 0.7288 - val_loss: 0.6094 - val_accuracy: 0.9651 - val_precision_m: 0.7503 - val_recall_m: 0.1969 - val_f1_m: 0.3119\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.51620\n",
      "Epoch 86/100\n",
      "16/16 - 0s - loss: 0.2424 - accuracy: 0.9736 - precision_m: 0.8523 - recall_m: 0.6274 - f1_m: 0.7184 - val_loss: 0.6143 - val_accuracy: 0.9656 - val_precision_m: 0.7854 - val_recall_m: 0.1871 - val_f1_m: 0.3019\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.51620\n",
      "Epoch 87/100\n",
      "16/16 - 0s - loss: 0.2180 - accuracy: 0.9750 - precision_m: 0.8740 - recall_m: 0.6551 - f1_m: 0.7442 - val_loss: 0.6198 - val_accuracy: 0.9650 - val_precision_m: 0.7462 - val_recall_m: 0.1918 - val_f1_m: 0.3049\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.51620\n",
      "Epoch 88/100\n",
      "16/16 - 0s - loss: 0.2287 - accuracy: 0.9746 - precision_m: 0.8720 - recall_m: 0.6315 - f1_m: 0.7292 - val_loss: 0.6805 - val_accuracy: 0.9639 - val_precision_m: 0.7388 - val_recall_m: 0.1462 - val_f1_m: 0.2440\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.51620\n",
      "Epoch 89/100\n",
      "16/16 - 0s - loss: 0.2493 - accuracy: 0.9733 - precision_m: 0.8476 - recall_m: 0.6201 - f1_m: 0.7113 - val_loss: 0.6131 - val_accuracy: 0.9644 - val_precision_m: 0.7153 - val_recall_m: 0.2021 - val_f1_m: 0.3149\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.51620\n",
      "Epoch 90/100\n",
      "16/16 - 0s - loss: 0.2341 - accuracy: 0.9741 - precision_m: 0.8682 - recall_m: 0.6272 - f1_m: 0.7235 - val_loss: 0.5859 - val_accuracy: 0.9643 - val_precision_m: 0.6712 - val_recall_m: 0.2392 - val_f1_m: 0.3527\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.51620\n",
      "Epoch 91/100\n",
      "16/16 - 0s - loss: 0.2107 - accuracy: 0.9761 - precision_m: 0.8717 - recall_m: 0.6690 - f1_m: 0.7535 - val_loss: 0.6040 - val_accuracy: 0.9641 - val_precision_m: 0.7000 - val_recall_m: 0.2148 - val_f1_m: 0.3286\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.51620\n",
      "Epoch 92/100\n",
      "16/16 - 0s - loss: 0.2212 - accuracy: 0.9755 - precision_m: 0.8749 - recall_m: 0.6439 - f1_m: 0.7379 - val_loss: 0.5941 - val_accuracy: 0.9640 - val_precision_m: 0.7113 - val_recall_m: 0.2222 - val_f1_m: 0.3354\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.51620\n",
      "Epoch 93/100\n",
      "16/16 - 0s - loss: 0.2106 - accuracy: 0.9762 - precision_m: 0.8768 - recall_m: 0.6641 - f1_m: 0.7521 - val_loss: 0.5754 - val_accuracy: 0.9642 - val_precision_m: 0.6990 - val_recall_m: 0.2447 - val_f1_m: 0.3609\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.51620\n",
      "Epoch 94/100\n",
      "16/16 - 0s - loss: 0.2169 - accuracy: 0.9757 - precision_m: 0.8720 - recall_m: 0.6542 - f1_m: 0.7441 - val_loss: 0.5846 - val_accuracy: 0.9643 - val_precision_m: 0.6923 - val_recall_m: 0.2364 - val_f1_m: 0.3522\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.51620\n",
      "Epoch 95/100\n",
      "16/16 - 0s - loss: 0.2112 - accuracy: 0.9758 - precision_m: 0.8811 - recall_m: 0.6554 - f1_m: 0.7486 - val_loss: 0.5928 - val_accuracy: 0.9655 - val_precision_m: 0.7506 - val_recall_m: 0.2102 - val_f1_m: 0.3284\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.51620\n",
      "Epoch 96/100\n",
      "16/16 - 0s - loss: 0.2118 - accuracy: 0.9757 - precision_m: 0.8730 - recall_m: 0.6664 - f1_m: 0.7516 - val_loss: 0.5706 - val_accuracy: 0.9658 - val_precision_m: 0.7516 - val_recall_m: 0.2286 - val_f1_m: 0.3501\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.51620\n",
      "Epoch 97/100\n",
      "16/16 - 1s - loss: 0.2124 - accuracy: 0.9766 - precision_m: 0.8680 - recall_m: 0.6697 - f1_m: 0.7532 - val_loss: 0.6165 - val_accuracy: 0.9643 - val_precision_m: 0.7119 - val_recall_m: 0.2000 - val_f1_m: 0.3122\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.51620\n",
      "Epoch 98/100\n",
      "16/16 - 0s - loss: 0.2403 - accuracy: 0.9739 - precision_m: 0.8602 - recall_m: 0.6262 - f1_m: 0.7186 - val_loss: 0.6333 - val_accuracy: 0.9640 - val_precision_m: 0.6928 - val_recall_m: 0.1918 - val_f1_m: 0.3003\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.51620\n",
      "Epoch 99/100\n",
      "16/16 - 0s - loss: 0.2248 - accuracy: 0.9750 - precision_m: 0.8826 - recall_m: 0.6325 - f1_m: 0.7308 - val_loss: 0.5503 - val_accuracy: 0.9635 - val_precision_m: 0.5955 - val_recall_m: 0.3111 - val_f1_m: 0.4087\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.51620\n",
      "Epoch 100/100\n",
      "16/16 - 0s - loss: 0.2289 - accuracy: 0.9747 - precision_m: 0.8566 - recall_m: 0.6515 - f1_m: 0.7349 - val_loss: 0.5771 - val_accuracy: 0.9641 - val_precision_m: 0.6513 - val_recall_m: 0.2513 - val_f1_m: 0.3623\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.51620\n",
      "5/5 - 0s - loss: 0.3938 - accuracy: 0.9559 - precision_m: 0.6346 - recall_m: 0.5950 - f1_m: 0.6063\n",
      "The metrics for the test set with loss tversky, learning rate 0.001,  filters 8, and batch size 16 is [0.39383718371391296, 0.955917477607727, 0.6345664262771606, 0.5949615240097046, 0.6062560677528381]\n",
      "__________________________________________________________________________________________________________________________________________________________________________\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 128, 128, 5) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 128, 128, 16) 736         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 128, 128, 16) 64          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 128, 128, 16) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 16) 2320        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 16) 64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, 128, 16) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 64, 64, 16)   0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 32)   4640        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64, 64, 32)   128         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 64, 64, 32)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 64, 32)   9248        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 32)   128         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64, 64, 32)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 32)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 64)   18496       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 64)   256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 64)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 64)   36928       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 64)   256         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 64)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 64)   0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 16, 16, 128)  73856       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 16, 16, 128)  512         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 16, 16, 128)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 16, 16, 128)  147584      activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 16, 16, 128)  512         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16, 16, 128)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 128)    0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 8, 8, 256)    295168      max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 8, 8, 256)    1024        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 8, 8, 256)    0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 8, 8, 256)    590080      activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 8, 8, 256)    1024        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 8, 8, 256)    0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 16, 16, 128)  131200      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 16, 16, 256)  0           conv2d_transpose[0][0]           \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 128)  295040      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 128)  512         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 128)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 128)  147584      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 128)  512         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 128)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 32, 32, 64)   32832       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 128)  0           conv2d_transpose_1[0][0]         \n",
      "                                                                 activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 64)   73792       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 32, 32, 64)   256         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 32, 32, 64)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 32, 64)   36928       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 32, 32, 64)   256         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 32, 32, 64)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 64, 64, 32)   8224        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 64, 64, 64)   0           conv2d_transpose_2[0][0]         \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 32)   18464       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64, 64, 32)   128         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 64, 64, 32)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 32)   9248        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 64, 64, 32)   128         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 64, 64, 32)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 128, 128, 16) 2064        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 128, 128, 32) 0           conv2d_transpose_3[0][0]         \n",
      "                                                                 activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 128, 128, 16) 4624        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 128, 128, 16) 64          conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 128, 128, 16) 0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 128, 128, 16) 2320        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 128, 128, 16) 64          conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 128, 128, 16) 0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 128, 128, 1)  17          activation_17[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,947,281\n",
      "Trainable params: 1,944,337\n",
      "Non-trainable params: 2,944\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "Initialising in 3..........2..........1......................................\n",
      "Training on 284 images with learning rate 0.001, batch size 16, and number of filters 16\n",
      "Epoch 1/100\n",
      "16/16 - 39s - loss: 0.9090 - accuracy: 0.3383 - precision_m: 0.0712 - recall_m: 0.8724 - f1_m: 0.1305 - val_loss: 0.9358 - val_accuracy: 0.5458 - val_precision_m: 0.0478 - val_recall_m: 0.6047 - val_f1_m: 0.0885\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.51620\n",
      "Epoch 2/100\n",
      "16/16 - 1s - loss: 0.8934 - accuracy: 0.5308 - precision_m: 0.0842 - recall_m: 0.7283 - f1_m: 0.1498 - val_loss: 0.9355 - val_accuracy: 0.4922 - val_precision_m: 0.0486 - val_recall_m: 0.6773 - val_f1_m: 0.0906\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.51620\n",
      "Epoch 3/100\n",
      "16/16 - 1s - loss: 0.8890 - accuracy: 0.6140 - precision_m: 0.0881 - recall_m: 0.6236 - f1_m: 0.1531 - val_loss: 0.9342 - val_accuracy: 0.4769 - val_precision_m: 0.0479 - val_recall_m: 0.6870 - val_f1_m: 0.0895\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.51620\n",
      "Epoch 4/100\n",
      "16/16 - 1s - loss: 0.8706 - accuracy: 0.6949 - precision_m: 0.1095 - recall_m: 0.6048 - f1_m: 0.1842 - val_loss: 0.9380 - val_accuracy: 0.3687 - val_precision_m: 0.0455 - val_recall_m: 0.7884 - val_f1_m: 0.0859\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.51620\n",
      "Epoch 5/100\n",
      "16/16 - 1s - loss: 0.8460 - accuracy: 0.7314 - precision_m: 0.1333 - recall_m: 0.6687 - f1_m: 0.2199 - val_loss: 0.9385 - val_accuracy: 0.2364 - val_precision_m: 0.0436 - val_recall_m: 0.9162 - val_f1_m: 0.0831\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.51620\n",
      "Epoch 6/100\n",
      "16/16 - 1s - loss: 0.8544 - accuracy: 0.7566 - precision_m: 0.1250 - recall_m: 0.5712 - f1_m: 0.1998 - val_loss: 0.9371 - val_accuracy: 0.2859 - val_precision_m: 0.0450 - val_recall_m: 0.8921 - val_f1_m: 0.0855\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.51620\n",
      "Epoch 7/100\n",
      "16/16 - 1s - loss: 0.8364 - accuracy: 0.8055 - precision_m: 0.1461 - recall_m: 0.5075 - f1_m: 0.2252 - val_loss: 0.9220 - val_accuracy: 0.4790 - val_precision_m: 0.0597 - val_recall_m: 0.8654 - val_f1_m: 0.1116\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.51620\n",
      "Epoch 8/100\n",
      "16/16 - 1s - loss: 0.8066 - accuracy: 0.8048 - precision_m: 0.1753 - recall_m: 0.6507 - f1_m: 0.2707 - val_loss: 0.9559 - val_accuracy: 0.6931 - val_precision_m: 0.0254 - val_recall_m: 0.1731 - val_f1_m: 0.0443\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.51620\n",
      "Epoch 9/100\n",
      "16/16 - 1s - loss: 0.8143 - accuracy: 0.7776 - precision_m: 0.1556 - recall_m: 0.6396 - f1_m: 0.2436 - val_loss: 0.9130 - val_accuracy: 0.5269 - val_precision_m: 0.0647 - val_recall_m: 0.8405 - val_f1_m: 0.1201\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.51620\n",
      "Epoch 10/100\n",
      "16/16 - 1s - loss: 0.8030 - accuracy: 0.8368 - precision_m: 0.1802 - recall_m: 0.5530 - f1_m: 0.2614 - val_loss: 0.9441 - val_accuracy: 0.8764 - val_precision_m: 0.0318 - val_recall_m: 0.1041 - val_f1_m: 0.0487\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.51620\n",
      "Epoch 11/100\n",
      "16/16 - 1s - loss: 0.7590 - accuracy: 0.8473 - precision_m: 0.2141 - recall_m: 0.6435 - f1_m: 0.3154 - val_loss: 0.9187 - val_accuracy: 0.9385 - val_precision_m: 0.0973 - val_recall_m: 0.1003 - val_f1_m: 0.0988\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.51620\n",
      "Epoch 12/100\n",
      "16/16 - 1s - loss: 0.7474 - accuracy: 0.8499 - precision_m: 0.2212 - recall_m: 0.6381 - f1_m: 0.3221 - val_loss: 0.9255 - val_accuracy: 0.9526 - val_precision_m: 0.1532 - val_recall_m: 0.0886 - val_f1_m: 0.1123\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.51620\n",
      "Epoch 13/100\n",
      "16/16 - 1s - loss: 0.7379 - accuracy: 0.8420 - precision_m: 0.2206 - recall_m: 0.7153 - f1_m: 0.3307 - val_loss: 0.9377 - val_accuracy: 0.9573 - val_precision_m: 0.1433 - val_recall_m: 0.0425 - val_f1_m: 0.0655\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.51620\n",
      "Epoch 14/100\n",
      "16/16 - 1s - loss: 0.7466 - accuracy: 0.8611 - precision_m: 0.2206 - recall_m: 0.5795 - f1_m: 0.3169 - val_loss: 0.9617 - val_accuracy: 0.9604 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.51620\n",
      "Epoch 15/100\n",
      "16/16 - 1s - loss: 0.7573 - accuracy: 0.8396 - precision_m: 0.2005 - recall_m: 0.6475 - f1_m: 0.3032 - val_loss: 0.9115 - val_accuracy: 0.9472 - val_precision_m: 0.1295 - val_recall_m: 0.0980 - val_f1_m: 0.1116\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.51620\n",
      "Epoch 16/100\n",
      "16/16 - 1s - loss: 0.7372 - accuracy: 0.8619 - precision_m: 0.2248 - recall_m: 0.5939 - f1_m: 0.3204 - val_loss: 0.9637 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.51620\n",
      "Epoch 17/100\n",
      "16/16 - 1s - loss: 0.7405 - accuracy: 0.8650 - precision_m: 0.2222 - recall_m: 0.5628 - f1_m: 0.3115 - val_loss: 0.9379 - val_accuracy: 0.9588 - val_precision_m: 0.1819 - val_recall_m: 0.0266 - val_f1_m: 0.0465\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.51620\n",
      "Epoch 18/100\n",
      "16/16 - 1s - loss: 0.7602 - accuracy: 0.8565 - precision_m: 0.2056 - recall_m: 0.5469 - f1_m: 0.2904 - val_loss: 0.9754 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.51620\n",
      "Epoch 19/100\n",
      "16/16 - 1s - loss: 0.7464 - accuracy: 0.8665 - precision_m: 0.2198 - recall_m: 0.5319 - f1_m: 0.3013 - val_loss: 0.9729 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.51620\n",
      "Epoch 20/100\n",
      "16/16 - 1s - loss: 0.7350 - accuracy: 0.8814 - precision_m: 0.2300 - recall_m: 0.4943 - f1_m: 0.3109 - val_loss: 0.9737 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.51620\n",
      "Epoch 21/100\n",
      "16/16 - 1s - loss: 0.7146 - accuracy: 0.8758 - precision_m: 0.2472 - recall_m: 0.5619 - f1_m: 0.3359 - val_loss: 0.9833 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.51620\n",
      "Epoch 22/100\n",
      "16/16 - 1s - loss: 0.7185 - accuracy: 0.8718 - precision_m: 0.2392 - recall_m: 0.5776 - f1_m: 0.3325 - val_loss: 0.9833 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.51620\n",
      "Epoch 23/100\n",
      "16/16 - 1s - loss: 0.6864 - accuracy: 0.8850 - precision_m: 0.2756 - recall_m: 0.5913 - f1_m: 0.3654 - val_loss: 0.9827 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.51620\n",
      "Epoch 24/100\n",
      "16/16 - 1s - loss: 0.7188 - accuracy: 0.8915 - precision_m: 0.2551 - recall_m: 0.4488 - f1_m: 0.3182 - val_loss: 0.9944 - val_accuracy: 0.9357 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.51620\n",
      "Epoch 25/100\n",
      "16/16 - 1s - loss: 0.7444 - accuracy: 0.8622 - precision_m: 0.2145 - recall_m: 0.5191 - f1_m: 0.2985 - val_loss: 0.9922 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.51620\n",
      "Epoch 26/100\n",
      "16/16 - 1s - loss: 0.6901 - accuracy: 0.8896 - precision_m: 0.2698 - recall_m: 0.5529 - f1_m: 0.3553 - val_loss: 0.9917 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.51620\n",
      "Epoch 27/100\n",
      "16/16 - 1s - loss: 0.8238 - accuracy: 0.9215 - precision_m: 0.2344 - recall_m: 0.1546 - f1_m: 0.1731 - val_loss: 0.9064 - val_accuracy: 0.9241 - val_precision_m: 0.0647 - val_recall_m: 0.1228 - val_f1_m: 0.0835\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.51620\n",
      "Epoch 28/100\n",
      "16/16 - 1s - loss: 0.7421 - accuracy: 0.9018 - precision_m: 0.2504 - recall_m: 0.3555 - f1_m: 0.2807 - val_loss: 0.9882 - val_accuracy: 0.9180 - val_precision_m: 0.0062 - val_recall_m: 0.0107 - val_f1_m: 0.0078\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.51620\n",
      "Epoch 29/100\n",
      "16/16 - 1s - loss: 0.6922 - accuracy: 0.8933 - precision_m: 0.2719 - recall_m: 0.5220 - f1_m: 0.3485 - val_loss: 0.9907 - val_accuracy: 0.9315 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.51620\n",
      "Epoch 30/100\n",
      "16/16 - 1s - loss: 0.6436 - accuracy: 0.9180 - precision_m: 0.3472 - recall_m: 0.4879 - f1_m: 0.3815 - val_loss: 0.9937 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.51620\n",
      "Epoch 31/100\n",
      "16/16 - 1s - loss: 0.6504 - accuracy: 0.9213 - precision_m: 0.3418 - recall_m: 0.4293 - f1_m: 0.3690 - val_loss: 0.9918 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.51620\n",
      "Epoch 32/100\n",
      "16/16 - 1s - loss: 0.6681 - accuracy: 0.9243 - precision_m: 0.3298 - recall_m: 0.3824 - f1_m: 0.3457 - val_loss: 0.9952 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.51620\n",
      "Epoch 33/100\n",
      "16/16 - 1s - loss: 0.5859 - accuracy: 0.9273 - precision_m: 0.3927 - recall_m: 0.5339 - f1_m: 0.4437 - val_loss: 0.9952 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.51620\n",
      "Epoch 34/100\n",
      "16/16 - 1s - loss: 0.6207 - accuracy: 0.9297 - precision_m: 0.3819 - recall_m: 0.4154 - f1_m: 0.3898 - val_loss: 0.9101 - val_accuracy: 0.9593 - val_precision_m: 0.1814 - val_recall_m: 0.0356 - val_f1_m: 0.0595\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.51620\n",
      "Epoch 35/100\n",
      "16/16 - 1s - loss: 0.5938 - accuracy: 0.9247 - precision_m: 0.3834 - recall_m: 0.5238 - f1_m: 0.4333 - val_loss: 0.9183 - val_accuracy: 0.9572 - val_precision_m: 0.1340 - val_recall_m: 0.0340 - val_f1_m: 0.0543\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.51620\n",
      "Epoch 36/100\n",
      "16/16 - 1s - loss: 0.5996 - accuracy: 0.9298 - precision_m: 0.4000 - recall_m: 0.4720 - f1_m: 0.4135 - val_loss: 0.9962 - val_accuracy: 0.9609 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.51620\n",
      "Epoch 37/100\n",
      "16/16 - 1s - loss: 0.5628 - accuracy: 0.9414 - precision_m: 0.4754 - recall_m: 0.4193 - f1_m: 0.4301 - val_loss: 0.9867 - val_accuracy: 0.9611 - val_precision_m: 0.2766 - val_recall_m: 0.0022 - val_f1_m: 0.0043\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.51620\n",
      "Epoch 38/100\n",
      "16/16 - 1s - loss: 0.5583 - accuracy: 0.9409 - precision_m: 0.4770 - recall_m: 0.4087 - f1_m: 0.4329 - val_loss: 0.9979 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.51620\n",
      "Epoch 39/100\n",
      "16/16 - 1s - loss: 0.5282 - accuracy: 0.9422 - precision_m: 0.4860 - recall_m: 0.4712 - f1_m: 0.4731 - val_loss: 0.7002 - val_accuracy: 0.9408 - val_precision_m: 0.2833 - val_recall_m: 0.3512 - val_f1_m: 0.3135\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.51620\n",
      "Epoch 40/100\n",
      "16/16 - 1s - loss: 0.5647 - accuracy: 0.9401 - precision_m: 0.4743 - recall_m: 0.4141 - f1_m: 0.4269 - val_loss: 0.9165 - val_accuracy: 0.9615 - val_precision_m: 0.3001 - val_recall_m: 0.0273 - val_f1_m: 0.0500\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.51620\n",
      "Epoch 41/100\n",
      "16/16 - 1s - loss: 0.5276 - accuracy: 0.9430 - precision_m: 0.4938 - recall_m: 0.4523 - f1_m: 0.4664 - val_loss: 0.9980 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.51620\n",
      "Epoch 42/100\n",
      "16/16 - 1s - loss: 0.5278 - accuracy: 0.9457 - precision_m: 0.5310 - recall_m: 0.4257 - f1_m: 0.4545 - val_loss: 0.9979 - val_accuracy: 0.9609 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.51620\n",
      "Epoch 43/100\n",
      "16/16 - 1s - loss: 0.5559 - accuracy: 0.9376 - precision_m: 0.4475 - recall_m: 0.4743 - f1_m: 0.4502 - val_loss: 0.8098 - val_accuracy: 0.9578 - val_precision_m: 0.5562 - val_recall_m: 0.1095 - val_f1_m: 0.1500\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.51620\n",
      "Epoch 44/100\n",
      "16/16 - 1s - loss: 0.5028 - accuracy: 0.9474 - precision_m: 0.5434 - recall_m: 0.4437 - f1_m: 0.4795 - val_loss: 0.9283 - val_accuracy: 0.9595 - val_precision_m: 0.1603 - val_recall_m: 0.0230 - val_f1_m: 0.0402\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.51620\n",
      "Epoch 45/100\n",
      "16/16 - 1s - loss: 0.5164 - accuracy: 0.9465 - precision_m: 0.5431 - recall_m: 0.4253 - f1_m: 0.4633 - val_loss: 0.8230 - val_accuracy: 0.9623 - val_precision_m: 0.7956 - val_recall_m: 0.0731 - val_f1_m: 0.1210\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.51620\n",
      "Epoch 46/100\n",
      "16/16 - 1s - loss: 0.4966 - accuracy: 0.9495 - precision_m: 0.5657 - recall_m: 0.4227 - f1_m: 0.4764 - val_loss: 0.9989 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.51620\n",
      "Epoch 47/100\n",
      "16/16 - 1s - loss: 0.5180 - accuracy: 0.9505 - precision_m: 0.5974 - recall_m: 0.3652 - f1_m: 0.4378 - val_loss: 0.9995 - val_accuracy: 0.9342 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.51620\n",
      "Epoch 48/100\n",
      "16/16 - 1s - loss: 0.5808 - accuracy: 0.9400 - precision_m: 0.4602 - recall_m: 0.3671 - f1_m: 0.4002 - val_loss: 0.8395 - val_accuracy: 0.9604 - val_precision_m: 0.2303 - val_recall_m: 0.0788 - val_f1_m: 0.1175\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.51620\n",
      "Epoch 49/100\n",
      "16/16 - 1s - loss: 0.5251 - accuracy: 0.9417 - precision_m: 0.4856 - recall_m: 0.4901 - f1_m: 0.4761 - val_loss: 0.9914 - val_accuracy: 0.9608 - val_precision_m: 0.1122 - val_recall_m: 0.0019 - val_f1_m: 0.0038\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.51620\n",
      "Epoch 50/100\n",
      "16/16 - 1s - loss: 0.5253 - accuracy: 0.9467 - precision_m: 0.5349 - recall_m: 0.4135 - f1_m: 0.4520 - val_loss: 0.9995 - val_accuracy: 0.9514 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.51620\n",
      "Epoch 51/100\n",
      "16/16 - 1s - loss: 0.5072 - accuracy: 0.9504 - precision_m: 0.5874 - recall_m: 0.3890 - f1_m: 0.4550 - val_loss: 0.8333 - val_accuracy: 0.9628 - val_precision_m: 0.3350 - val_recall_m: 0.0665 - val_f1_m: 0.1109\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.51620\n",
      "Epoch 52/100\n",
      "16/16 - 1s - loss: 0.4945 - accuracy: 0.9501 - precision_m: 0.5776 - recall_m: 0.4378 - f1_m: 0.4790 - val_loss: 0.9994 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.51620\n",
      "Epoch 53/100\n",
      "16/16 - 1s - loss: 0.5809 - accuracy: 0.9439 - precision_m: 0.4945 - recall_m: 0.3372 - f1_m: 0.3890 - val_loss: 0.6775 - val_accuracy: 0.9453 - val_precision_m: 0.3307 - val_recall_m: 0.3396 - val_f1_m: 0.3203\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.51620\n",
      "Epoch 54/100\n",
      "16/16 - 1s - loss: 0.5270 - accuracy: 0.9474 - precision_m: 0.5431 - recall_m: 0.3966 - f1_m: 0.4440 - val_loss: 0.9996 - val_accuracy: 0.9299 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.51620\n",
      "Epoch 55/100\n",
      "16/16 - 1s - loss: 0.4729 - accuracy: 0.9529 - precision_m: 0.6323 - recall_m: 0.4079 - f1_m: 0.4835 - val_loss: 0.6745 - val_accuracy: 0.9282 - val_precision_m: 0.2877 - val_recall_m: 0.5051 - val_f1_m: 0.3601\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.51620\n",
      "Epoch 56/100\n",
      "16/16 - 1s - loss: 0.4823 - accuracy: 0.9529 - precision_m: 0.6411 - recall_m: 0.3933 - f1_m: 0.4724 - val_loss: 0.6410 - val_accuracy: 0.9578 - val_precision_m: 0.4350 - val_recall_m: 0.2694 - val_f1_m: 0.3228\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.51620\n",
      "Epoch 57/100\n",
      "16/16 - 1s - loss: 0.4713 - accuracy: 0.9522 - precision_m: 0.6041 - recall_m: 0.4316 - f1_m: 0.4947 - val_loss: 0.7340 - val_accuracy: 0.9567 - val_precision_m: 0.3624 - val_recall_m: 0.1615 - val_f1_m: 0.2221\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.51620\n",
      "Epoch 58/100\n",
      "16/16 - 1s - loss: 0.4562 - accuracy: 0.9539 - precision_m: 0.6249 - recall_m: 0.4614 - f1_m: 0.5115 - val_loss: 0.9548 - val_accuracy: 0.5446 - val_precision_m: 0.0327 - val_recall_m: 0.3997 - val_f1_m: 0.0600\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.51620\n",
      "Epoch 59/100\n",
      "16/16 - 1s - loss: 0.4636 - accuracy: 0.9539 - precision_m: 0.6495 - recall_m: 0.4176 - f1_m: 0.4924 - val_loss: 0.9744 - val_accuracy: 0.5334 - val_precision_m: 0.0190 - val_recall_m: 0.2476 - val_f1_m: 0.0351\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.51620\n",
      "Epoch 60/100\n",
      "16/16 - 1s - loss: 0.4441 - accuracy: 0.9536 - precision_m: 0.6096 - recall_m: 0.4823 - f1_m: 0.5314 - val_loss: 0.9996 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.51620\n",
      "Epoch 61/100\n",
      "16/16 - 1s - loss: 0.4314 - accuracy: 0.9566 - precision_m: 0.6882 - recall_m: 0.4260 - f1_m: 0.5173 - val_loss: 0.9995 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.51620\n",
      "Epoch 62/100\n",
      "16/16 - 1s - loss: 0.4513 - accuracy: 0.9549 - precision_m: 0.6656 - recall_m: 0.4222 - f1_m: 0.5026 - val_loss: 0.9997 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.51620\n",
      "Epoch 63/100\n",
      "16/16 - 1s - loss: 0.3867 - accuracy: 0.9595 - precision_m: 0.7099 - recall_m: 0.4856 - f1_m: 0.5689 - val_loss: 0.9997 - val_accuracy: 0.9519 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.51620\n",
      "Epoch 64/100\n",
      "16/16 - 1s - loss: 0.3900 - accuracy: 0.9592 - precision_m: 0.7009 - recall_m: 0.5035 - f1_m: 0.5710 - val_loss: 0.9997 - val_accuracy: 0.9581 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.51620\n",
      "Epoch 65/100\n",
      "16/16 - 1s - loss: 0.4593 - accuracy: 0.9546 - precision_m: 0.6574 - recall_m: 0.4149 - f1_m: 0.4926 - val_loss: 0.9997 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.51620\n",
      "Epoch 66/100\n",
      "16/16 - 1s - loss: 0.4053 - accuracy: 0.9583 - precision_m: 0.6765 - recall_m: 0.4845 - f1_m: 0.5573 - val_loss: 0.9997 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.51620\n",
      "Epoch 67/100\n",
      "16/16 - 1s - loss: 0.3825 - accuracy: 0.9597 - precision_m: 0.6934 - recall_m: 0.5115 - f1_m: 0.5819 - val_loss: 0.9998 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.51620\n",
      "Epoch 68/100\n",
      "16/16 - 1s - loss: 0.3506 - accuracy: 0.9629 - precision_m: 0.7428 - recall_m: 0.5287 - f1_m: 0.6071 - val_loss: 0.9998 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.51620\n",
      "Epoch 69/100\n",
      "16/16 - 1s - loss: 0.3775 - accuracy: 0.9604 - precision_m: 0.7029 - recall_m: 0.5186 - f1_m: 0.5865 - val_loss: 0.9998 - val_accuracy: 0.9440 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.51620\n",
      "Epoch 70/100\n",
      "16/16 - 1s - loss: 0.3456 - accuracy: 0.9630 - precision_m: 0.7294 - recall_m: 0.5474 - f1_m: 0.6181 - val_loss: 0.9998 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.51620\n",
      "Epoch 71/100\n",
      "16/16 - 1s - loss: 0.3937 - accuracy: 0.9596 - precision_m: 0.7498 - recall_m: 0.4292 - f1_m: 0.5422 - val_loss: 0.9998 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.51620\n",
      "Epoch 72/100\n",
      "16/16 - 1s - loss: 0.4100 - accuracy: 0.9578 - precision_m: 0.6973 - recall_m: 0.4716 - f1_m: 0.5441 - val_loss: 0.9998 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.51620\n",
      "Epoch 73/100\n",
      "16/16 - 1s - loss: 0.3538 - accuracy: 0.9630 - precision_m: 0.7549 - recall_m: 0.5114 - f1_m: 0.5987 - val_loss: 0.9998 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.51620\n",
      "Epoch 74/100\n",
      "16/16 - 1s - loss: 0.3393 - accuracy: 0.9639 - precision_m: 0.7598 - recall_m: 0.5244 - f1_m: 0.6137 - val_loss: 0.9999 - val_accuracy: 0.9321 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.51620\n",
      "Epoch 75/100\n",
      "16/16 - 1s - loss: 0.3119 - accuracy: 0.9667 - precision_m: 0.7870 - recall_m: 0.5715 - f1_m: 0.6455 - val_loss: 0.9999 - val_accuracy: 0.9371 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.51620\n",
      "Epoch 76/100\n",
      "16/16 - 1s - loss: 0.3188 - accuracy: 0.9655 - precision_m: 0.7925 - recall_m: 0.5374 - f1_m: 0.6314 - val_loss: 0.9998 - val_accuracy: 0.9470 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.51620\n",
      "Epoch 77/100\n",
      "16/16 - 1s - loss: 0.2954 - accuracy: 0.9678 - precision_m: 0.8152 - recall_m: 0.5572 - f1_m: 0.6533 - val_loss: 0.9999 - val_accuracy: 0.9490 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.51620\n",
      "Epoch 78/100\n",
      "16/16 - 1s - loss: 0.3063 - accuracy: 0.9672 - precision_m: 0.8026 - recall_m: 0.5535 - f1_m: 0.6437 - val_loss: 0.9998 - val_accuracy: 0.9526 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.51620\n",
      "Epoch 79/100\n",
      "16/16 - 1s - loss: 0.2657 - accuracy: 0.9701 - precision_m: 0.8437 - recall_m: 0.5803 - f1_m: 0.6829 - val_loss: 0.9998 - val_accuracy: 0.9561 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.51620\n",
      "Epoch 80/100\n",
      "16/16 - 1s - loss: 0.3180 - accuracy: 0.9661 - precision_m: 0.7689 - recall_m: 0.5624 - f1_m: 0.6416 - val_loss: 0.7422 - val_accuracy: 0.9618 - val_precision_m: 0.6030 - val_recall_m: 0.1324 - val_f1_m: 0.1947\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.51620\n",
      "Epoch 81/100\n",
      "16/16 - 1s - loss: 0.2798 - accuracy: 0.9690 - precision_m: 0.8125 - recall_m: 0.5879 - f1_m: 0.6764 - val_loss: 0.7771 - val_accuracy: 0.9627 - val_precision_m: 0.7322 - val_recall_m: 0.0931 - val_f1_m: 0.1549\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.51620\n",
      "Epoch 82/100\n",
      "16/16 - 1s - loss: 0.2851 - accuracy: 0.9688 - precision_m: 0.8322 - recall_m: 0.5509 - f1_m: 0.6591 - val_loss: 0.7792 - val_accuracy: 0.9644 - val_precision_m: 0.8527 - val_recall_m: 0.0870 - val_f1_m: 0.1478\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.51620\n",
      "Epoch 83/100\n",
      "16/16 - 1s - loss: 0.2769 - accuracy: 0.9697 - precision_m: 0.8232 - recall_m: 0.5791 - f1_m: 0.6747 - val_loss: 0.7711 - val_accuracy: 0.9621 - val_precision_m: 0.6250 - val_recall_m: 0.1008 - val_f1_m: 0.1634\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.51620\n",
      "Epoch 84/100\n",
      "16/16 - 1s - loss: 0.2718 - accuracy: 0.9700 - precision_m: 0.8308 - recall_m: 0.5779 - f1_m: 0.6780 - val_loss: 0.7267 - val_accuracy: 0.9624 - val_precision_m: 0.7536 - val_recall_m: 0.1242 - val_f1_m: 0.1986\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.51620\n",
      "Epoch 85/100\n",
      "16/16 - 1s - loss: 0.2601 - accuracy: 0.9705 - precision_m: 0.8426 - recall_m: 0.5921 - f1_m: 0.6896 - val_loss: 0.5941 - val_accuracy: 0.9617 - val_precision_m: 0.6228 - val_recall_m: 0.2634 - val_f1_m: 0.3447\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.51620\n",
      "Epoch 86/100\n",
      "16/16 - 1s - loss: 0.2528 - accuracy: 0.9716 - precision_m: 0.8458 - recall_m: 0.5996 - f1_m: 0.6982 - val_loss: 0.7449 - val_accuracy: 0.9622 - val_precision_m: 0.7499 - val_recall_m: 0.1210 - val_f1_m: 0.1875\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.51620\n",
      "Epoch 87/100\n",
      "16/16 - 1s - loss: 0.2641 - accuracy: 0.9705 - precision_m: 0.8360 - recall_m: 0.5908 - f1_m: 0.6873 - val_loss: 0.9506 - val_accuracy: 0.5928 - val_precision_m: 0.0357 - val_recall_m: 0.3818 - val_f1_m: 0.0648\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.51620\n",
      "Epoch 88/100\n",
      "16/16 - 1s - loss: 0.2481 - accuracy: 0.9718 - precision_m: 0.8428 - recall_m: 0.6171 - f1_m: 0.7070 - val_loss: 0.9936 - val_accuracy: 0.7605 - val_precision_m: 0.0049 - val_recall_m: 0.0339 - val_f1_m: 0.0084\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.51620\n",
      "Epoch 89/100\n",
      "16/16 - 1s - loss: 0.2484 - accuracy: 0.9717 - precision_m: 0.8499 - recall_m: 0.6082 - f1_m: 0.7039 - val_loss: 0.9878 - val_accuracy: 0.6114 - val_precision_m: 0.0087 - val_recall_m: 0.0859 - val_f1_m: 0.0156\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.51620\n",
      "Epoch 90/100\n",
      "16/16 - 1s - loss: 0.2969 - accuracy: 0.9672 - precision_m: 0.7919 - recall_m: 0.5897 - f1_m: 0.6633 - val_loss: 0.9998 - val_accuracy: 0.7451 - val_precision_m: 1.0037e-04 - val_recall_m: 5.5450e-04 - val_f1_m: 1.6672e-04\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.51620\n",
      "Epoch 91/100\n",
      "16/16 - 1s - loss: 0.2459 - accuracy: 0.9718 - precision_m: 0.8456 - recall_m: 0.6205 - f1_m: 0.7099 - val_loss: 0.9999 - val_accuracy: 0.7689 - val_precision_m: 2.4041e-05 - val_recall_m: 8.4281e-05 - val_f1_m: 3.7394e-05\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.51620\n",
      "Epoch 92/100\n",
      "16/16 - 1s - loss: 0.2766 - accuracy: 0.9700 - precision_m: 0.8331 - recall_m: 0.5689 - f1_m: 0.6700 - val_loss: 0.9912 - val_accuracy: 0.6381 - val_precision_m: 0.0064 - val_recall_m: 0.0597 - val_f1_m: 0.0115\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.51620\n",
      "Epoch 93/100\n",
      "16/16 - 1s - loss: 0.2600 - accuracy: 0.9710 - precision_m: 0.8350 - recall_m: 0.6067 - f1_m: 0.6950 - val_loss: 0.9503 - val_accuracy: 0.4994 - val_precision_m: 0.0357 - val_recall_m: 0.4817 - val_f1_m: 0.0660\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.51620\n",
      "Epoch 94/100\n",
      "16/16 - 1s - loss: 0.2415 - accuracy: 0.9729 - precision_m: 0.8449 - recall_m: 0.6290 - f1_m: 0.7169 - val_loss: 0.9815 - val_accuracy: 0.5993 - val_precision_m: 0.0132 - val_recall_m: 0.1309 - val_f1_m: 0.0237\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.51620\n",
      "Epoch 95/100\n",
      "16/16 - 1s - loss: 0.2212 - accuracy: 0.9743 - precision_m: 0.8749 - recall_m: 0.6312 - f1_m: 0.7309 - val_loss: 0.9596 - val_accuracy: 0.5558 - val_precision_m: 0.0290 - val_recall_m: 0.3319 - val_f1_m: 0.0528\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.51620\n",
      "Epoch 96/100\n",
      "16/16 - 1s - loss: 0.2437 - accuracy: 0.9724 - precision_m: 0.8546 - recall_m: 0.6159 - f1_m: 0.7086 - val_loss: 0.8709 - val_accuracy: 0.8779 - val_precision_m: 0.1044 - val_recall_m: 0.2861 - val_f1_m: 0.1506\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.51620\n",
      "Epoch 97/100\n",
      "16/16 - 1s - loss: 0.2473 - accuracy: 0.9730 - precision_m: 0.8287 - recall_m: 0.6363 - f1_m: 0.7155 - val_loss: 0.5690 - val_accuracy: 0.9628 - val_precision_m: 0.5469 - val_recall_m: 0.3014 - val_f1_m: 0.3886\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.51620\n",
      "Epoch 98/100\n",
      "16/16 - 1s - loss: 0.2534 - accuracy: 0.9726 - precision_m: 0.8314 - recall_m: 0.6252 - f1_m: 0.7054 - val_loss: 0.7962 - val_accuracy: 0.9581 - val_precision_m: 0.3249 - val_recall_m: 0.1014 - val_f1_m: 0.1542\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.51620\n",
      "Epoch 99/100\n",
      "16/16 - 1s - loss: 0.2675 - accuracy: 0.9720 - precision_m: 0.8094 - recall_m: 0.6223 - f1_m: 0.6953 - val_loss: 0.8739 - val_accuracy: 0.9609 - val_precision_m: 0.2440 - val_recall_m: 0.0518 - val_f1_m: 0.0855\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.51620\n",
      "Epoch 100/100\n",
      "16/16 - 1s - loss: 0.2364 - accuracy: 0.9721 - precision_m: 0.8611 - recall_m: 0.6318 - f1_m: 0.7190 - val_loss: 0.9998 - val_accuracy: 0.9575 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.51620\n",
      "5/5 - 0s - loss: 0.9793 - accuracy: 0.9372 - precision_m: 0.1333 - recall_m: 0.0063 - f1_m: 0.0116\n",
      "The metrics for the test set with loss tversky, learning rate 0.001,  filters 16, and batch size 16 is [0.979296088218689, 0.937186598777771, 0.13331183791160583, 0.006264598574489355, 0.011589957401156425]\n",
      "__________________________________________________________________________________________________________________________________________________________________________\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 128, 128, 5) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 128, 128, 32) 1472        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 128, 128, 32) 128         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 128, 128, 32) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 32) 9248        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 32) 128         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, 128, 32) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 64, 64, 32)   0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 64)   18496       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64, 64, 64)   256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 64, 64, 64)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 64, 64)   36928       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 64)   256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64, 64, 64)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 64)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 128)  73856       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 128)  512         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 128)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 128)  147584      activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 128)  512         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 128)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 128)  0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 16, 16, 256)  295168      max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 16, 16, 256)  1024        conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 16, 16, 256)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 16, 16, 256)  590080      activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 16, 16, 256)  1024        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16, 16, 256)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 256)    0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 8, 8, 512)    1180160     max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 8, 8, 512)    2048        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 8, 8, 512)    0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 8, 8, 512)    2359808     activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 8, 8, 512)    2048        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 8, 8, 512)    0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 16, 16, 128)  262272      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 16, 16, 384)  0           conv2d_transpose[0][0]           \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 256)  884992      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 256)  1024        conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 256)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 256)  590080      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 256)  1024        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 256)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 32, 32, 64)   65600       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 192)  0           conv2d_transpose_1[0][0]         \n",
      "                                                                 activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 128)  221312      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 32, 32, 128)  512         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 32, 32, 128)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 32, 128)  147584      activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 32, 32, 128)  512         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 32, 32, 128)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 64, 64, 32)   16416       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 64, 64, 96)   0           conv2d_transpose_2[0][0]         \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 64)   55360       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64, 64, 64)   256         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 64, 64, 64)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 64)   36928       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 64, 64, 64)   256         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 64, 64, 64)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 128, 128, 16) 4112        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 128, 128, 48) 0           conv2d_transpose_3[0][0]         \n",
      "                                                                 activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 128, 128, 32) 13856       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 128, 128, 32) 128         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 128, 128, 32) 0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 128, 128, 32) 9248        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 128, 128, 32) 128         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 128, 128, 32) 0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 128, 128, 1)  33          activation_17[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 7,032,369\n",
      "Trainable params: 7,026,481\n",
      "Non-trainable params: 5,888\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "Initialising in 3..........2..........1......................................\n",
      "Training on 284 images with learning rate 0.001, batch size 16, and number of filters 32\n",
      "Epoch 1/100\n",
      "16/16 - 40s - loss: 0.9042 - accuracy: 0.4369 - precision_m: 0.0728 - recall_m: 0.7689 - f1_m: 0.1318 - val_loss: 0.9408 - val_accuracy: 0.4093 - val_precision_m: 0.0463 - val_recall_m: 0.7557 - val_f1_m: 0.0871\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.51620\n",
      "Epoch 2/100\n",
      "16/16 - 1s - loss: 0.8603 - accuracy: 0.6800 - precision_m: 0.1180 - recall_m: 0.6768 - f1_m: 0.1986 - val_loss: 0.9362 - val_accuracy: 0.8878 - val_precision_m: 0.0512 - val_recall_m: 0.1136 - val_f1_m: 0.0703\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.51620\n",
      "Epoch 3/100\n",
      "16/16 - 1s - loss: 0.8212 - accuracy: 0.7221 - precision_m: 0.1491 - recall_m: 0.8356 - f1_m: 0.2495 - val_loss: 0.9298 - val_accuracy: 0.7528 - val_precision_m: 0.0591 - val_recall_m: 0.3874 - val_f1_m: 0.1021\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.51620\n",
      "Epoch 4/100\n",
      "16/16 - 1s - loss: 0.7844 - accuracy: 0.7760 - precision_m: 0.1839 - recall_m: 0.8645 - f1_m: 0.3002 - val_loss: 0.9169 - val_accuracy: 0.6165 - val_precision_m: 0.0704 - val_recall_m: 0.7626 - val_f1_m: 0.1287\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.51620\n",
      "Epoch 5/100\n",
      "16/16 - 1s - loss: 0.8075 - accuracy: 0.7395 - precision_m: 0.1584 - recall_m: 0.8001 - f1_m: 0.2588 - val_loss: 0.9271 - val_accuracy: 0.4696 - val_precision_m: 0.0560 - val_recall_m: 0.8315 - val_f1_m: 0.1049\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.51620\n",
      "Epoch 6/100\n",
      "16/16 - 1s - loss: 0.7819 - accuracy: 0.8028 - precision_m: 0.1900 - recall_m: 0.7766 - f1_m: 0.2977 - val_loss: 0.9690 - val_accuracy: 0.9158 - val_precision_m: 1.9074e-04 - val_recall_m: 3.3713e-04 - val_f1_m: 2.4361e-04\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.51620\n",
      "Epoch 7/100\n",
      "16/16 - 1s - loss: 0.7775 - accuracy: 0.7937 - precision_m: 0.1831 - recall_m: 0.7911 - f1_m: 0.2946 - val_loss: 0.9746 - val_accuracy: 0.9064 - val_precision_m: 6.7912e-04 - val_recall_m: 0.0014 - val_f1_m: 9.1256e-04\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.51620\n",
      "Epoch 8/100\n",
      "16/16 - 1s - loss: 0.7580 - accuracy: 0.8105 - precision_m: 0.2017 - recall_m: 0.7970 - f1_m: 0.3157 - val_loss: 0.9580 - val_accuracy: 0.9598 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.51620\n",
      "Epoch 9/100\n",
      "16/16 - 1s - loss: 0.7369 - accuracy: 0.8525 - precision_m: 0.2305 - recall_m: 0.6918 - f1_m: 0.3419 - val_loss: 0.9672 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.51620\n",
      "Epoch 10/100\n",
      "16/16 - 1s - loss: 0.7389 - accuracy: 0.8306 - precision_m: 0.2151 - recall_m: 0.7533 - f1_m: 0.3299 - val_loss: 0.9241 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.51620\n",
      "Epoch 11/100\n",
      "16/16 - 1s - loss: 0.7580 - accuracy: 0.8755 - precision_m: 0.2233 - recall_m: 0.5122 - f1_m: 0.3038 - val_loss: 0.9878 - val_accuracy: 0.9253 - val_precision_m: 8.8152e-05 - val_recall_m: 1.2642e-04 - val_f1_m: 1.0385e-04\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.51620\n",
      "Epoch 12/100\n",
      "16/16 - 1s - loss: 0.7177 - accuracy: 0.8559 - precision_m: 0.2400 - recall_m: 0.7119 - f1_m: 0.3523 - val_loss: 0.9766 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.51620\n",
      "Epoch 13/100\n",
      "16/16 - 1s - loss: 0.6980 - accuracy: 0.8954 - precision_m: 0.2846 - recall_m: 0.5678 - f1_m: 0.3682 - val_loss: 0.9885 - val_accuracy: 0.9514 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.51620\n",
      "Epoch 14/100\n",
      "16/16 - 1s - loss: 0.8006 - accuracy: 0.8628 - precision_m: 0.1698 - recall_m: 0.3787 - f1_m: 0.2311 - val_loss: 0.9796 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.51620\n",
      "Epoch 15/100\n",
      "16/16 - 1s - loss: 0.7334 - accuracy: 0.8778 - precision_m: 0.2326 - recall_m: 0.5048 - f1_m: 0.3128 - val_loss: 0.9903 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.51620\n",
      "Epoch 16/100\n",
      "16/16 - 1s - loss: 0.6538 - accuracy: 0.9047 - precision_m: 0.3139 - recall_m: 0.5981 - f1_m: 0.4050 - val_loss: 0.9922 - val_accuracy: 0.9244 - val_precision_m: 1.7219e-04 - val_recall_m: 2.5284e-04 - val_f1_m: 2.0484e-04\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.51620\n",
      "Epoch 17/100\n",
      "16/16 - 1s - loss: 0.7015 - accuracy: 0.9025 - precision_m: 0.2775 - recall_m: 0.4479 - f1_m: 0.3347 - val_loss: 0.9887 - val_accuracy: 0.9092 - val_precision_m: 0.0061 - val_recall_m: 0.0125 - val_f1_m: 0.0082\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.51620\n",
      "Epoch 18/100\n",
      "16/16 - 1s - loss: 0.6637 - accuracy: 0.9034 - precision_m: 0.3034 - recall_m: 0.5273 - f1_m: 0.3795 - val_loss: 0.9811 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.51620\n",
      "Epoch 19/100\n",
      "16/16 - 1s - loss: 0.6482 - accuracy: 0.9121 - precision_m: 0.3264 - recall_m: 0.5425 - f1_m: 0.3961 - val_loss: 0.9926 - val_accuracy: 0.9266 - val_precision_m: 3.0479e-05 - val_recall_m: 4.2141e-05 - val_f1_m: 3.5349e-05\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.51620\n",
      "Epoch 20/100\n",
      "16/16 - 1s - loss: 0.5663 - accuracy: 0.9336 - precision_m: 0.4253 - recall_m: 0.5432 - f1_m: 0.4705 - val_loss: 0.9950 - val_accuracy: 0.9333 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.51620\n",
      "Epoch 21/100\n",
      "16/16 - 1s - loss: 0.5977 - accuracy: 0.9343 - precision_m: 0.4111 - recall_m: 0.4596 - f1_m: 0.4274 - val_loss: 0.9946 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.51620\n",
      "Epoch 22/100\n",
      "16/16 - 1s - loss: 0.5999 - accuracy: 0.9239 - precision_m: 0.3756 - recall_m: 0.5307 - f1_m: 0.4339 - val_loss: 0.9950 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.51620\n",
      "Epoch 23/100\n",
      "16/16 - 1s - loss: 0.5806 - accuracy: 0.9312 - precision_m: 0.4101 - recall_m: 0.5104 - f1_m: 0.4490 - val_loss: 0.9955 - val_accuracy: 0.9571 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.51620\n",
      "Epoch 24/100\n",
      "16/16 - 1s - loss: 0.5618 - accuracy: 0.9347 - precision_m: 0.4323 - recall_m: 0.5175 - f1_m: 0.4657 - val_loss: 0.9964 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.51620\n",
      "Epoch 25/100\n",
      "16/16 - 1s - loss: 0.5637 - accuracy: 0.9345 - precision_m: 0.4311 - recall_m: 0.5277 - f1_m: 0.4631 - val_loss: 0.9960 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.51620\n",
      "Epoch 26/100\n",
      "16/16 - 1s - loss: 0.5455 - accuracy: 0.9392 - precision_m: 0.4600 - recall_m: 0.5262 - f1_m: 0.4805 - val_loss: 0.9966 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.51620\n",
      "Epoch 27/100\n",
      "16/16 - 1s - loss: 0.5528 - accuracy: 0.9353 - precision_m: 0.4372 - recall_m: 0.5445 - f1_m: 0.4748 - val_loss: 0.9949 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.51620\n",
      "Epoch 28/100\n",
      "16/16 - 1s - loss: 0.5556 - accuracy: 0.9383 - precision_m: 0.4487 - recall_m: 0.4994 - f1_m: 0.4634 - val_loss: 0.9966 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.51620\n",
      "Epoch 29/100\n",
      "16/16 - 1s - loss: 0.5433 - accuracy: 0.9398 - precision_m: 0.4658 - recall_m: 0.5397 - f1_m: 0.4762 - val_loss: 0.9837 - val_accuracy: 0.9611 - val_precision_m: 0.5000 - val_recall_m: 8.4281e-05 - val_f1_m: 1.6853e-04\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.51620\n",
      "Epoch 30/100\n",
      "16/16 - 1s - loss: 0.5773 - accuracy: 0.9320 - precision_m: 0.4177 - recall_m: 0.4924 - f1_m: 0.4409 - val_loss: 0.9776 - val_accuracy: 0.9332 - val_precision_m: 0.0153 - val_recall_m: 0.0182 - val_f1_m: 0.0166\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.51620\n",
      "Epoch 31/100\n",
      "16/16 - 1s - loss: 0.5596 - accuracy: 0.9374 - precision_m: 0.4551 - recall_m: 0.4820 - f1_m: 0.4543 - val_loss: 0.9947 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.51620\n",
      "Epoch 32/100\n",
      "16/16 - 1s - loss: 0.5402 - accuracy: 0.9414 - precision_m: 0.4768 - recall_m: 0.4967 - f1_m: 0.4717 - val_loss: 0.9972 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.51620\n",
      "Epoch 33/100\n",
      "16/16 - 1s - loss: 0.4998 - accuracy: 0.9466 - precision_m: 0.5394 - recall_m: 0.4850 - f1_m: 0.5035 - val_loss: 0.9971 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.51620\n",
      "Epoch 34/100\n",
      "16/16 - 1s - loss: 0.5322 - accuracy: 0.9432 - precision_m: 0.5096 - recall_m: 0.4476 - f1_m: 0.4649 - val_loss: 0.9975 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.51620\n",
      "Epoch 35/100\n",
      "16/16 - 1s - loss: 0.5177 - accuracy: 0.9477 - precision_m: 0.5371 - recall_m: 0.4543 - f1_m: 0.4774 - val_loss: 0.9968 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.51620\n",
      "Epoch 36/100\n",
      "16/16 - 1s - loss: 0.4520 - accuracy: 0.9531 - precision_m: 0.6052 - recall_m: 0.5000 - f1_m: 0.5391 - val_loss: 0.9964 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.51620\n",
      "Epoch 37/100\n",
      "16/16 - 1s - loss: 0.4748 - accuracy: 0.9516 - precision_m: 0.5842 - recall_m: 0.4943 - f1_m: 0.5190 - val_loss: 0.9958 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.51620\n",
      "Epoch 38/100\n",
      "16/16 - 1s - loss: 0.4831 - accuracy: 0.9521 - precision_m: 0.6066 - recall_m: 0.4460 - f1_m: 0.4967 - val_loss: 0.9960 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.51620\n",
      "Epoch 39/100\n",
      "16/16 - 1s - loss: 0.4590 - accuracy: 0.9541 - precision_m: 0.6287 - recall_m: 0.4770 - f1_m: 0.5219 - val_loss: 0.9945 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.51620\n",
      "Epoch 40/100\n",
      "16/16 - 1s - loss: 0.4773 - accuracy: 0.9526 - precision_m: 0.6057 - recall_m: 0.4519 - f1_m: 0.5023 - val_loss: 0.9513 - val_accuracy: 0.9614 - val_precision_m: 0.9946 - val_recall_m: 0.0105 - val_f1_m: 0.0208\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.51620\n",
      "Epoch 41/100\n",
      "16/16 - 1s - loss: 0.4389 - accuracy: 0.9550 - precision_m: 0.6208 - recall_m: 0.5033 - f1_m: 0.5491 - val_loss: 0.8825 - val_accuracy: 0.9622 - val_precision_m: 0.3630 - val_recall_m: 0.0359 - val_f1_m: 0.0654\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.51620\n",
      "Epoch 42/100\n",
      "16/16 - 1s - loss: 0.4730 - accuracy: 0.9523 - precision_m: 0.6017 - recall_m: 0.4671 - f1_m: 0.5116 - val_loss: 0.7432 - val_accuracy: 0.9577 - val_precision_m: 0.6499 - val_recall_m: 0.1918 - val_f1_m: 0.2207\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.51620\n",
      "Epoch 43/100\n",
      "16/16 - 1s - loss: 0.4362 - accuracy: 0.9561 - precision_m: 0.6568 - recall_m: 0.4771 - f1_m: 0.5396 - val_loss: 0.8629 - val_accuracy: 0.9631 - val_precision_m: 0.4565 - val_recall_m: 0.0446 - val_f1_m: 0.0813\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.51620\n",
      "Epoch 44/100\n",
      "16/16 - 1s - loss: 0.3833 - accuracy: 0.9606 - precision_m: 0.7137 - recall_m: 0.5151 - f1_m: 0.5897 - val_loss: 0.9956 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.51620\n",
      "Epoch 45/100\n",
      "16/16 - 1s - loss: 0.4178 - accuracy: 0.9583 - precision_m: 0.6872 - recall_m: 0.4745 - f1_m: 0.5528 - val_loss: 0.7922 - val_accuracy: 0.9633 - val_precision_m: 0.3243 - val_recall_m: 0.0988 - val_f1_m: 0.1514\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.51620\n",
      "Epoch 46/100\n",
      "16/16 - 1s - loss: 0.4076 - accuracy: 0.9585 - precision_m: 0.6779 - recall_m: 0.5043 - f1_m: 0.5703 - val_loss: 0.8271 - val_accuracy: 0.9628 - val_precision_m: 0.3280 - val_recall_m: 0.0710 - val_f1_m: 0.1168\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.51620\n",
      "Epoch 47/100\n",
      "16/16 - 1s - loss: 0.3864 - accuracy: 0.9605 - precision_m: 0.7174 - recall_m: 0.5041 - f1_m: 0.5851 - val_loss: 0.9115 - val_accuracy: 0.9616 - val_precision_m: 0.4285 - val_recall_m: 0.0265 - val_f1_m: 0.0499\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.51620\n",
      "Epoch 48/100\n",
      "16/16 - 1s - loss: 0.4476 - accuracy: 0.9557 - precision_m: 0.6466 - recall_m: 0.4623 - f1_m: 0.5270 - val_loss: 0.8268 - val_accuracy: 0.9628 - val_precision_m: 0.3283 - val_recall_m: 0.0721 - val_f1_m: 0.1182\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.51620\n",
      "Epoch 49/100\n",
      "16/16 - 1s - loss: 0.4195 - accuracy: 0.9567 - precision_m: 0.6705 - recall_m: 0.4834 - f1_m: 0.5553 - val_loss: 0.7609 - val_accuracy: 0.9610 - val_precision_m: 0.3142 - val_recall_m: 0.1311 - val_f1_m: 0.1832\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.51620\n",
      "Epoch 50/100\n",
      "16/16 - 1s - loss: 0.4259 - accuracy: 0.9565 - precision_m: 0.6468 - recall_m: 0.5077 - f1_m: 0.5551 - val_loss: 0.8773 - val_accuracy: 0.9612 - val_precision_m: 0.3308 - val_recall_m: 0.0405 - val_f1_m: 0.0722\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.51620\n",
      "Epoch 51/100\n",
      "16/16 - 1s - loss: 0.4016 - accuracy: 0.9597 - precision_m: 0.7103 - recall_m: 0.4794 - f1_m: 0.5647 - val_loss: 0.6896 - val_accuracy: 0.9603 - val_precision_m: 0.6088 - val_recall_m: 0.2130 - val_f1_m: 0.2654\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.51620\n",
      "Epoch 52/100\n",
      "16/16 - 1s - loss: 0.4145 - accuracy: 0.9579 - precision_m: 0.6739 - recall_m: 0.4952 - f1_m: 0.5603 - val_loss: 0.8239 - val_accuracy: 0.9621 - val_precision_m: 0.3942 - val_recall_m: 0.0681 - val_f1_m: 0.1159\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.51620\n",
      "Epoch 53/100\n",
      "16/16 - 1s - loss: 0.3953 - accuracy: 0.9594 - precision_m: 0.6866 - recall_m: 0.5292 - f1_m: 0.5812 - val_loss: 0.8476 - val_accuracy: 0.9603 - val_precision_m: 0.3246 - val_recall_m: 0.0615 - val_f1_m: 0.1029\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.51620\n",
      "Epoch 54/100\n",
      "16/16 - 1s - loss: 0.3911 - accuracy: 0.9606 - precision_m: 0.7028 - recall_m: 0.5240 - f1_m: 0.5828 - val_loss: 0.9460 - val_accuracy: 0.9567 - val_precision_m: 0.0674 - val_recall_m: 0.0232 - val_f1_m: 0.0345\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.51620\n",
      "Epoch 55/100\n",
      "16/16 - 1s - loss: 0.3759 - accuracy: 0.9619 - precision_m: 0.7290 - recall_m: 0.5158 - f1_m: 0.5917 - val_loss: 0.8105 - val_accuracy: 0.9613 - val_precision_m: 0.5183 - val_recall_m: 0.0809 - val_f1_m: 0.1371\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.51620\n",
      "Epoch 56/100\n",
      "16/16 - 1s - loss: 0.3449 - accuracy: 0.9645 - precision_m: 0.7513 - recall_m: 0.5411 - f1_m: 0.6250 - val_loss: 0.9934 - val_accuracy: 0.7800 - val_precision_m: 0.0039 - val_recall_m: 0.0278 - val_f1_m: 0.0068\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.51620\n",
      "Epoch 57/100\n",
      "16/16 - 1s - loss: 0.3602 - accuracy: 0.9634 - precision_m: 0.7335 - recall_m: 0.5395 - f1_m: 0.6111 - val_loss: 0.9991 - val_accuracy: 0.7300 - val_precision_m: 4.9215e-04 - val_recall_m: 0.0048 - val_f1_m: 8.9113e-04\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.51620\n",
      "Epoch 58/100\n",
      "16/16 - 1s - loss: 0.3323 - accuracy: 0.9651 - precision_m: 0.7815 - recall_m: 0.5297 - f1_m: 0.6274 - val_loss: 0.7240 - val_accuracy: 0.9536 - val_precision_m: 0.3411 - val_recall_m: 0.2149 - val_f1_m: 0.2625\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.51620\n",
      "Epoch 59/100\n",
      "16/16 - 1s - loss: 0.3179 - accuracy: 0.9666 - precision_m: 0.7878 - recall_m: 0.5504 - f1_m: 0.6440 - val_loss: 0.7486 - val_accuracy: 0.9573 - val_precision_m: 0.3258 - val_recall_m: 0.1512 - val_f1_m: 0.2054\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.51620\n",
      "Epoch 60/100\n",
      "16/16 - 1s - loss: 0.3543 - accuracy: 0.9642 - precision_m: 0.7308 - recall_m: 0.5460 - f1_m: 0.6185 - val_loss: 0.9686 - val_accuracy: 0.9600 - val_precision_m: 0.2310 - val_recall_m: 0.0056 - val_f1_m: 0.0110\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.51620\n",
      "Epoch 61/100\n",
      "16/16 - 1s - loss: 0.3452 - accuracy: 0.9645 - precision_m: 0.7515 - recall_m: 0.5447 - f1_m: 0.6245 - val_loss: 0.7871 - val_accuracy: 0.9631 - val_precision_m: 0.8145 - val_recall_m: 0.0973 - val_f1_m: 0.1511\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.51620\n",
      "Epoch 62/100\n",
      "16/16 - 1s - loss: 0.3615 - accuracy: 0.9638 - precision_m: 0.7686 - recall_m: 0.4930 - f1_m: 0.5950 - val_loss: 0.6205 - val_accuracy: 0.9594 - val_precision_m: 0.5444 - val_recall_m: 0.2876 - val_f1_m: 0.3388\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.51620\n",
      "Epoch 63/100\n",
      "16/16 - 1s - loss: 0.3622 - accuracy: 0.9623 - precision_m: 0.7185 - recall_m: 0.5462 - f1_m: 0.6120 - val_loss: 0.6154 - val_accuracy: 0.9639 - val_precision_m: 0.6119 - val_recall_m: 0.2346 - val_f1_m: 0.3221\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.51620\n",
      "Epoch 64/100\n",
      "16/16 - 1s - loss: 0.3305 - accuracy: 0.9653 - precision_m: 0.7768 - recall_m: 0.5425 - f1_m: 0.6317 - val_loss: 0.6934 - val_accuracy: 0.9626 - val_precision_m: 0.5994 - val_recall_m: 0.1675 - val_f1_m: 0.2426\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.51620\n",
      "Epoch 65/100\n",
      "16/16 - 1s - loss: 0.3184 - accuracy: 0.9664 - precision_m: 0.7772 - recall_m: 0.5610 - f1_m: 0.6460 - val_loss: 0.7394 - val_accuracy: 0.9632 - val_precision_m: 0.7848 - val_recall_m: 0.1382 - val_f1_m: 0.1976\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.51620\n",
      "Epoch 66/100\n",
      "16/16 - 1s - loss: 0.3383 - accuracy: 0.9645 - precision_m: 0.7513 - recall_m: 0.5644 - f1_m: 0.6318 - val_loss: 0.6020 - val_accuracy: 0.9632 - val_precision_m: 0.6347 - val_recall_m: 0.2528 - val_f1_m: 0.3387\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.51620\n",
      "Epoch 67/100\n",
      "16/16 - 1s - loss: 0.3211 - accuracy: 0.9668 - precision_m: 0.7873 - recall_m: 0.5497 - f1_m: 0.6396 - val_loss: 0.6155 - val_accuracy: 0.9625 - val_precision_m: 0.6215 - val_recall_m: 0.2435 - val_f1_m: 0.3274\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.51620\n",
      "Epoch 68/100\n",
      "16/16 - 1s - loss: 0.3074 - accuracy: 0.9680 - precision_m: 0.7850 - recall_m: 0.5809 - f1_m: 0.6608 - val_loss: 0.6527 - val_accuracy: 0.9614 - val_precision_m: 0.6272 - val_recall_m: 0.2221 - val_f1_m: 0.2883\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.51620\n",
      "Epoch 69/100\n",
      "16/16 - 1s - loss: 0.3370 - accuracy: 0.9662 - precision_m: 0.7479 - recall_m: 0.5760 - f1_m: 0.6364 - val_loss: 0.6047 - val_accuracy: 0.9580 - val_precision_m: 0.6168 - val_recall_m: 0.2954 - val_f1_m: 0.3546\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.51620\n",
      "Epoch 70/100\n",
      "16/16 - 1s - loss: 0.3174 - accuracy: 0.9664 - precision_m: 0.7748 - recall_m: 0.5612 - f1_m: 0.6474 - val_loss: 0.6880 - val_accuracy: 0.9621 - val_precision_m: 0.5642 - val_recall_m: 0.1906 - val_f1_m: 0.2531\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.51620\n",
      "Epoch 71/100\n",
      "16/16 - 1s - loss: 0.3370 - accuracy: 0.9649 - precision_m: 0.7701 - recall_m: 0.5460 - f1_m: 0.6261 - val_loss: 0.6736 - val_accuracy: 0.9609 - val_precision_m: 0.5584 - val_recall_m: 0.2107 - val_f1_m: 0.2746\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.51620\n",
      "Epoch 72/100\n",
      "16/16 - 1s - loss: 0.3132 - accuracy: 0.9673 - precision_m: 0.7953 - recall_m: 0.5525 - f1_m: 0.6452 - val_loss: 0.5849 - val_accuracy: 0.9579 - val_precision_m: 0.5502 - val_recall_m: 0.3356 - val_f1_m: 0.3811\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.51620\n",
      "Epoch 73/100\n",
      "16/16 - 1s - loss: 0.3219 - accuracy: 0.9665 - precision_m: 0.7608 - recall_m: 0.5717 - f1_m: 0.6465 - val_loss: 0.6547 - val_accuracy: 0.9612 - val_precision_m: 0.5418 - val_recall_m: 0.2238 - val_f1_m: 0.2935\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.51620\n",
      "Epoch 74/100\n",
      "16/16 - 1s - loss: 0.3125 - accuracy: 0.9672 - precision_m: 0.7799 - recall_m: 0.5674 - f1_m: 0.6512 - val_loss: 0.6563 - val_accuracy: 0.9562 - val_precision_m: 0.4848 - val_recall_m: 0.2716 - val_f1_m: 0.3102\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.51620\n",
      "Epoch 75/100\n",
      "16/16 - 1s - loss: 0.3054 - accuracy: 0.9673 - precision_m: 0.7945 - recall_m: 0.5778 - f1_m: 0.6572 - val_loss: 0.6423 - val_accuracy: 0.9602 - val_precision_m: 0.5130 - val_recall_m: 0.2436 - val_f1_m: 0.3092\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.51620\n",
      "Epoch 76/100\n",
      "16/16 - 1s - loss: 0.2832 - accuracy: 0.9692 - precision_m: 0.8309 - recall_m: 0.5703 - f1_m: 0.6717 - val_loss: 0.8181 - val_accuracy: 0.9547 - val_precision_m: 0.4408 - val_recall_m: 0.1242 - val_f1_m: 0.1513\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.51620\n",
      "Epoch 77/100\n",
      "16/16 - 1s - loss: 0.3120 - accuracy: 0.9672 - precision_m: 0.7759 - recall_m: 0.5763 - f1_m: 0.6560 - val_loss: 0.5669 - val_accuracy: 0.9633 - val_precision_m: 0.6594 - val_recall_m: 0.2802 - val_f1_m: 0.3744\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.51620\n",
      "Epoch 78/100\n",
      "16/16 - 1s - loss: 0.3023 - accuracy: 0.9682 - precision_m: 0.8024 - recall_m: 0.5667 - f1_m: 0.6576 - val_loss: 0.5511 - val_accuracy: 0.9622 - val_precision_m: 0.6567 - val_recall_m: 0.3140 - val_f1_m: 0.3972\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.51620\n",
      "Epoch 79/100\n",
      "16/16 - 1s - loss: 0.3000 - accuracy: 0.9674 - precision_m: 0.7941 - recall_m: 0.5755 - f1_m: 0.6616 - val_loss: 0.6986 - val_accuracy: 0.9363 - val_precision_m: 0.3627 - val_recall_m: 0.3331 - val_f1_m: 0.2950\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.51620\n",
      "Epoch 80/100\n",
      "16/16 - 1s - loss: 0.2661 - accuracy: 0.9704 - precision_m: 0.8306 - recall_m: 0.6062 - f1_m: 0.6953 - val_loss: 0.8017 - val_accuracy: 0.9412 - val_precision_m: 0.3353 - val_recall_m: 0.1790 - val_f1_m: 0.1765\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.51620\n",
      "Epoch 81/100\n",
      "16/16 - 1s - loss: 0.2791 - accuracy: 0.9698 - precision_m: 0.8189 - recall_m: 0.5894 - f1_m: 0.6813 - val_loss: 0.6878 - val_accuracy: 0.9457 - val_precision_m: 0.4657 - val_recall_m: 0.2672 - val_f1_m: 0.2849\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.51620\n",
      "Epoch 82/100\n",
      "16/16 - 1s - loss: 0.2673 - accuracy: 0.9711 - precision_m: 0.8285 - recall_m: 0.6052 - f1_m: 0.6935 - val_loss: 0.6768 - val_accuracy: 0.9567 - val_precision_m: 0.5228 - val_recall_m: 0.2212 - val_f1_m: 0.2783\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.51620\n",
      "Epoch 83/100\n",
      "16/16 - 1s - loss: 0.3140 - accuracy: 0.9661 - precision_m: 0.7874 - recall_m: 0.5659 - f1_m: 0.6471 - val_loss: 0.5977 - val_accuracy: 0.9547 - val_precision_m: 0.4626 - val_recall_m: 0.3740 - val_f1_m: 0.3904\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.51620\n",
      "Epoch 84/100\n",
      "16/16 - 1s - loss: 0.2594 - accuracy: 0.9710 - precision_m: 0.8408 - recall_m: 0.6105 - f1_m: 0.7003 - val_loss: 0.5433 - val_accuracy: 0.9626 - val_precision_m: 0.6514 - val_recall_m: 0.3269 - val_f1_m: 0.4087\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.51620\n",
      "Epoch 85/100\n",
      "16/16 - 1s - loss: 0.2812 - accuracy: 0.9703 - precision_m: 0.8246 - recall_m: 0.5843 - f1_m: 0.6768 - val_loss: 0.6533 - val_accuracy: 0.9394 - val_precision_m: 0.4391 - val_recall_m: 0.3555 - val_f1_m: 0.3387\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.51620\n",
      "Epoch 86/100\n",
      "16/16 - 1s - loss: 0.2715 - accuracy: 0.9705 - precision_m: 0.8263 - recall_m: 0.5968 - f1_m: 0.6879 - val_loss: 0.5936 - val_accuracy: 0.9535 - val_precision_m: 0.5888 - val_recall_m: 0.3368 - val_f1_m: 0.3771\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.51620\n",
      "Epoch 87/100\n",
      "16/16 - 1s - loss: 0.2793 - accuracy: 0.9699 - precision_m: 0.8038 - recall_m: 0.6114 - f1_m: 0.6875 - val_loss: 0.5668 - val_accuracy: 0.9567 - val_precision_m: 0.5216 - val_recall_m: 0.3808 - val_f1_m: 0.4128\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.51620\n",
      "Epoch 88/100\n",
      "16/16 - 1s - loss: 0.2769 - accuracy: 0.9710 - precision_m: 0.8229 - recall_m: 0.5882 - f1_m: 0.6822 - val_loss: 0.5433 - val_accuracy: 0.9607 - val_precision_m: 0.6073 - val_recall_m: 0.3622 - val_f1_m: 0.4196\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.51620\n",
      "Epoch 89/100\n",
      "16/16 - 1s - loss: 0.2690 - accuracy: 0.9705 - precision_m: 0.8304 - recall_m: 0.5945 - f1_m: 0.6895 - val_loss: 0.6278 - val_accuracy: 0.9588 - val_precision_m: 0.6000 - val_recall_m: 0.2733 - val_f1_m: 0.3266\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.51620\n",
      "Epoch 90/100\n",
      "16/16 - 1s - loss: 0.2668 - accuracy: 0.9709 - precision_m: 0.8350 - recall_m: 0.6038 - f1_m: 0.6921 - val_loss: 0.7121 - val_accuracy: 0.9558 - val_precision_m: 0.4828 - val_recall_m: 0.2267 - val_f1_m: 0.2534\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.51620\n",
      "Epoch 91/100\n",
      "16/16 - 1s - loss: 0.2991 - accuracy: 0.9683 - precision_m: 0.7951 - recall_m: 0.5768 - f1_m: 0.6644 - val_loss: 0.6426 - val_accuracy: 0.9405 - val_precision_m: 0.4150 - val_recall_m: 0.3790 - val_f1_m: 0.3538\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.51620\n",
      "Epoch 92/100\n",
      "16/16 - 1s - loss: 0.2970 - accuracy: 0.9683 - precision_m: 0.8119 - recall_m: 0.5633 - f1_m: 0.6582 - val_loss: 0.5831 - val_accuracy: 0.9613 - val_precision_m: 0.7036 - val_recall_m: 0.2811 - val_f1_m: 0.3574\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.51620\n",
      "Epoch 93/100\n",
      "16/16 - 1s - loss: 0.3082 - accuracy: 0.9675 - precision_m: 0.7999 - recall_m: 0.5581 - f1_m: 0.6495 - val_loss: 0.7180 - val_accuracy: 0.9543 - val_precision_m: 0.3613 - val_recall_m: 0.2247 - val_f1_m: 0.2515\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.51620\n",
      "Epoch 94/100\n",
      "16/16 - 1s - loss: 0.2905 - accuracy: 0.9686 - precision_m: 0.7901 - recall_m: 0.5929 - f1_m: 0.6730 - val_loss: 0.5591 - val_accuracy: 0.9667 - val_precision_m: 0.7866 - val_recall_m: 0.2531 - val_f1_m: 0.3586\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.51620\n",
      "Epoch 95/100\n",
      "16/16 - 1s - loss: 0.3020 - accuracy: 0.9682 - precision_m: 0.8106 - recall_m: 0.5523 - f1_m: 0.6510 - val_loss: 0.5681 - val_accuracy: 0.9592 - val_precision_m: 0.5249 - val_recall_m: 0.3465 - val_f1_m: 0.4000\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.51620\n",
      "Epoch 96/100\n",
      "16/16 - 1s - loss: 0.2460 - accuracy: 0.9722 - precision_m: 0.8499 - recall_m: 0.6199 - f1_m: 0.7131 - val_loss: 0.8162 - val_accuracy: 0.9426 - val_precision_m: 0.3376 - val_recall_m: 0.1717 - val_f1_m: 0.1646\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.51620\n",
      "Epoch 97/100\n",
      "16/16 - 1s - loss: 0.2609 - accuracy: 0.9720 - precision_m: 0.8209 - recall_m: 0.6295 - f1_m: 0.7055 - val_loss: 0.7007 - val_accuracy: 0.9447 - val_precision_m: 0.4126 - val_recall_m: 0.2569 - val_f1_m: 0.2772\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.51620\n",
      "Epoch 98/100\n",
      "16/16 - 1s - loss: 0.2491 - accuracy: 0.9726 - precision_m: 0.8435 - recall_m: 0.6210 - f1_m: 0.7116 - val_loss: 0.6779 - val_accuracy: 0.9484 - val_precision_m: 0.4004 - val_recall_m: 0.2835 - val_f1_m: 0.3055\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.51620\n",
      "Epoch 99/100\n",
      "16/16 - 1s - loss: 0.2527 - accuracy: 0.9716 - precision_m: 0.8457 - recall_m: 0.6120 - f1_m: 0.7052 - val_loss: 0.6441 - val_accuracy: 0.9573 - val_precision_m: 0.4821 - val_recall_m: 0.2686 - val_f1_m: 0.3205\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.51620\n",
      "Epoch 100/100\n",
      "16/16 - 1s - loss: 0.2444 - accuracy: 0.9730 - precision_m: 0.8538 - recall_m: 0.6170 - f1_m: 0.7128 - val_loss: 0.6108 - val_accuracy: 0.9588 - val_precision_m: 0.5385 - val_recall_m: 0.2908 - val_f1_m: 0.3519\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.51620\n",
      "5/5 - 0s - loss: 0.4223 - accuracy: 0.9545 - precision_m: 0.6643 - recall_m: 0.4970 - f1_m: 0.5649\n",
      "The metrics for the test set with loss tversky, learning rate 0.001,  filters 32, and batch size 16 is [0.422314316034317, 0.9544781446456909, 0.6642940640449524, 0.4970399737358093, 0.564864993095398]\n",
      "__________________________________________________________________________________________________________________________________________________________________________\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 128, 128, 5) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 128, 128, 8)  368         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 128, 128, 8)  32          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 128, 128, 8)  0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 8)  584         activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 8)  32          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, 128, 8)  0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 64, 64, 8)    0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 16)   1168        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64, 64, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 64, 64, 16)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 64, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64, 64, 16)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 16)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 32)   4640        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 32)   128         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 32)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 32)   9248        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 32)   128         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 32)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 32)   0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 16, 16, 64)   18496       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 16, 16, 64)   256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 16, 16, 64)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 16, 16, 64)   36928       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 16, 16, 64)   256         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16, 16, 64)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 64)     0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 8, 8, 128)    73856       max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 8, 8, 128)    512         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 8, 8, 128)    0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 8, 8, 128)    147584      activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 8, 8, 128)    512         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 8, 8, 128)    0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 16, 16, 128)  65664       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 16, 16, 192)  0           conv2d_transpose[0][0]           \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 64)   110656      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 64)   256         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 64)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 64)   36928       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 64)   256         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 64)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 32, 32, 64)   16448       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 96)   0           conv2d_transpose_1[0][0]         \n",
      "                                                                 activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 32)   27680       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 32, 32, 32)   128         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 32, 32, 32)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 32, 32)   9248        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 32, 32, 32)   128         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 32, 32, 32)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 64, 64, 32)   4128        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 64, 64, 48)   0           conv2d_transpose_2[0][0]         \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 16)   6928        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64, 64, 16)   64          conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 64, 64, 16)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 16)   2320        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 64, 64, 16)   64          conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 64, 64, 16)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 128, 128, 16) 1040        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 128, 128, 24) 0           conv2d_transpose_3[0][0]         \n",
      "                                                                 activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 128, 128, 8)  1736        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 128, 128, 8)  32          conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 128, 128, 8)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 128, 128, 8)  584         activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 128, 128, 8)  32          conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 128, 128, 8)  0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 128, 128, 1)  9           activation_17[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 581,505\n",
      "Trainable params: 580,033\n",
      "Non-trainable params: 1,472\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "Initialising in 3..........2..........1......................................\n",
      "Training on 284 images with learning rate 0.0001, batch size 8, and number of filters 8\n",
      "Epoch 1/100\n",
      "32/32 - 3s - loss: 0.9230 - accuracy: 0.4239 - precision_m: 0.0600 - recall_m: 0.6333 - f1_m: 0.1077 - val_loss: 0.9461 - val_accuracy: 0.3118 - val_precision_m: 0.0405 - val_recall_m: 0.7656 - val_f1_m: 0.0766\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.51620\n",
      "Epoch 2/100\n",
      "32/32 - 1s - loss: 0.9204 - accuracy: 0.3006 - precision_m: 0.0658 - recall_m: 0.8636 - f1_m: 0.1212 - val_loss: 0.9456 - val_accuracy: 0.2536 - val_precision_m: 0.0433 - val_recall_m: 0.8953 - val_f1_m: 0.0822\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.51620\n",
      "Epoch 3/100\n",
      "32/32 - 1s - loss: 0.9174 - accuracy: 0.3601 - precision_m: 0.0706 - recall_m: 0.8469 - f1_m: 0.1286 - val_loss: 0.9450 - val_accuracy: 0.2328 - val_precision_m: 0.0440 - val_recall_m: 0.9256 - val_f1_m: 0.0835\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.51620\n",
      "Epoch 4/100\n",
      "32/32 - 1s - loss: 0.9116 - accuracy: 0.4151 - precision_m: 0.0781 - recall_m: 0.8458 - f1_m: 0.1417 - val_loss: 0.9438 - val_accuracy: 0.2178 - val_precision_m: 0.0447 - val_recall_m: 0.9535 - val_f1_m: 0.0850\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.51620\n",
      "Epoch 5/100\n",
      "32/32 - 1s - loss: 0.9074 - accuracy: 0.4692 - precision_m: 0.0809 - recall_m: 0.8036 - f1_m: 0.1452 - val_loss: 0.9428 - val_accuracy: 0.2038 - val_precision_m: 0.0443 - val_recall_m: 0.9600 - val_f1_m: 0.0843\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.51620\n",
      "Epoch 6/100\n",
      "32/32 - 1s - loss: 0.9013 - accuracy: 0.4876 - precision_m: 0.0881 - recall_m: 0.8556 - f1_m: 0.1568 - val_loss: 0.9383 - val_accuracy: 0.3060 - val_precision_m: 0.0507 - val_recall_m: 0.9517 - val_f1_m: 0.0957\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.51620\n",
      "Epoch 7/100\n",
      "32/32 - 1s - loss: 0.8945 - accuracy: 0.5251 - precision_m: 0.0960 - recall_m: 0.8553 - f1_m: 0.1701 - val_loss: 0.9394 - val_accuracy: 0.2683 - val_precision_m: 0.0477 - val_recall_m: 0.9492 - val_f1_m: 0.0904\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.51620\n",
      "Epoch 8/100\n",
      "32/32 - 1s - loss: 0.8905 - accuracy: 0.5707 - precision_m: 0.1009 - recall_m: 0.8445 - f1_m: 0.1773 - val_loss: 0.9383 - val_accuracy: 0.2889 - val_precision_m: 0.0475 - val_recall_m: 0.9235 - val_f1_m: 0.0899\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.51620\n",
      "Epoch 9/100\n",
      "32/32 - 1s - loss: 0.8874 - accuracy: 0.5999 - precision_m: 0.1028 - recall_m: 0.7891 - f1_m: 0.1793 - val_loss: 0.9341 - val_accuracy: 0.3784 - val_precision_m: 0.0516 - val_recall_m: 0.8820 - val_f1_m: 0.0967\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.51620\n",
      "Epoch 10/100\n",
      "32/32 - 1s - loss: 0.8831 - accuracy: 0.6169 - precision_m: 0.1085 - recall_m: 0.7959 - f1_m: 0.1865 - val_loss: 0.9360 - val_accuracy: 0.2924 - val_precision_m: 0.0502 - val_recall_m: 0.9595 - val_f1_m: 0.0949\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.51620\n",
      "Epoch 11/100\n",
      "32/32 - 1s - loss: 0.8741 - accuracy: 0.6612 - precision_m: 0.1210 - recall_m: 0.7846 - f1_m: 0.2049 - val_loss: 0.9350 - val_accuracy: 0.3759 - val_precision_m: 0.0520 - val_recall_m: 0.8693 - val_f1_m: 0.0974\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.51620\n",
      "Epoch 12/100\n",
      "32/32 - 1s - loss: 0.8690 - accuracy: 0.6652 - precision_m: 0.1263 - recall_m: 0.7881 - f1_m: 0.2123 - val_loss: 0.9348 - val_accuracy: 0.3873 - val_precision_m: 0.0504 - val_recall_m: 0.8526 - val_f1_m: 0.0944\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.51620\n",
      "Epoch 13/100\n",
      "32/32 - 1s - loss: 0.8636 - accuracy: 0.6802 - precision_m: 0.1301 - recall_m: 0.7919 - f1_m: 0.2198 - val_loss: 0.9341 - val_accuracy: 0.3786 - val_precision_m: 0.0510 - val_recall_m: 0.8668 - val_f1_m: 0.0956\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.51620\n",
      "Epoch 14/100\n",
      "32/32 - 1s - loss: 0.8558 - accuracy: 0.7144 - precision_m: 0.1469 - recall_m: 0.8004 - f1_m: 0.2388 - val_loss: 0.9355 - val_accuracy: 0.2965 - val_precision_m: 0.0487 - val_recall_m: 0.9362 - val_f1_m: 0.0921\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.51620\n",
      "Epoch 15/100\n",
      "32/32 - 1s - loss: 0.8537 - accuracy: 0.7338 - precision_m: 0.1480 - recall_m: 0.7980 - f1_m: 0.2429 - val_loss: 0.9370 - val_accuracy: 0.2132 - val_precision_m: 0.0462 - val_recall_m: 0.9833 - val_f1_m: 0.0879\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.51620\n",
      "Epoch 16/100\n",
      "32/32 - 1s - loss: 0.8398 - accuracy: 0.7588 - precision_m: 0.1709 - recall_m: 0.8219 - f1_m: 0.2753 - val_loss: 0.9396 - val_accuracy: 0.1536 - val_precision_m: 0.0433 - val_recall_m: 0.9923 - val_f1_m: 0.0827\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.51620\n",
      "Epoch 17/100\n",
      "32/32 - 1s - loss: 0.8358 - accuracy: 0.7800 - precision_m: 0.1824 - recall_m: 0.8174 - f1_m: 0.2923 - val_loss: 0.9352 - val_accuracy: 0.2691 - val_precision_m: 0.0483 - val_recall_m: 0.9570 - val_f1_m: 0.0914\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.51620\n",
      "Epoch 18/100\n",
      "32/32 - 1s - loss: 0.8397 - accuracy: 0.7668 - precision_m: 0.1706 - recall_m: 0.8027 - f1_m: 0.2753 - val_loss: 0.9402 - val_accuracy: 0.1381 - val_precision_m: 0.0427 - val_recall_m: 0.9926 - val_f1_m: 0.0815\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.51620\n",
      "Epoch 19/100\n",
      "32/32 - 1s - loss: 0.8297 - accuracy: 0.7760 - precision_m: 0.1801 - recall_m: 0.8393 - f1_m: 0.2911 - val_loss: 0.9378 - val_accuracy: 0.1814 - val_precision_m: 0.0451 - val_recall_m: 0.9916 - val_f1_m: 0.0858\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.51620\n",
      "Epoch 20/100\n",
      "32/32 - 1s - loss: 0.8230 - accuracy: 0.8170 - precision_m: 0.2102 - recall_m: 0.8154 - f1_m: 0.3227 - val_loss: 0.9338 - val_accuracy: 0.2975 - val_precision_m: 0.0498 - val_recall_m: 0.9481 - val_f1_m: 0.0940\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.51620\n",
      "Epoch 21/100\n",
      "32/32 - 1s - loss: 0.8258 - accuracy: 0.8114 - precision_m: 0.2023 - recall_m: 0.8064 - f1_m: 0.3091 - val_loss: 0.9318 - val_accuracy: 0.2777 - val_precision_m: 0.0511 - val_recall_m: 0.9884 - val_f1_m: 0.0966\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.51620\n",
      "Epoch 22/100\n",
      "32/32 - 1s - loss: 0.8194 - accuracy: 0.8156 - precision_m: 0.2088 - recall_m: 0.8218 - f1_m: 0.3224 - val_loss: 0.9363 - val_accuracy: 0.2065 - val_precision_m: 0.0464 - val_recall_m: 0.9896 - val_f1_m: 0.0882\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.51620\n",
      "Epoch 23/100\n",
      "32/32 - 1s - loss: 0.8175 - accuracy: 0.8184 - precision_m: 0.2150 - recall_m: 0.8133 - f1_m: 0.3262 - val_loss: 0.9299 - val_accuracy: 0.3313 - val_precision_m: 0.0532 - val_recall_m: 0.9567 - val_f1_m: 0.1002\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.51620\n",
      "Epoch 24/100\n",
      "32/32 - 1s - loss: 0.8126 - accuracy: 0.8215 - precision_m: 0.2181 - recall_m: 0.8305 - f1_m: 0.3312 - val_loss: 0.9276 - val_accuracy: 0.3500 - val_precision_m: 0.0559 - val_recall_m: 0.9708 - val_f1_m: 0.1050\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.51620\n",
      "Epoch 25/100\n",
      "32/32 - 1s - loss: 0.8035 - accuracy: 0.8245 - precision_m: 0.2322 - recall_m: 0.8410 - f1_m: 0.3530 - val_loss: 0.9291 - val_accuracy: 0.3484 - val_precision_m: 0.0540 - val_recall_m: 0.9477 - val_f1_m: 0.1014\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.51620\n",
      "Epoch 26/100\n",
      "32/32 - 1s - loss: 0.8037 - accuracy: 0.8372 - precision_m: 0.2388 - recall_m: 0.8332 - f1_m: 0.3527 - val_loss: 0.9263 - val_accuracy: 0.3522 - val_precision_m: 0.0565 - val_recall_m: 0.9727 - val_f1_m: 0.1061\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.51620\n",
      "Epoch 27/100\n",
      "32/32 - 1s - loss: 0.7938 - accuracy: 0.8474 - precision_m: 0.2550 - recall_m: 0.8367 - f1_m: 0.3792 - val_loss: 0.9306 - val_accuracy: 0.3187 - val_precision_m: 0.0523 - val_recall_m: 0.9562 - val_f1_m: 0.0985\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.51620\n",
      "Epoch 28/100\n",
      "32/32 - 1s - loss: 0.7911 - accuracy: 0.8506 - precision_m: 0.2563 - recall_m: 0.8381 - f1_m: 0.3779 - val_loss: 0.9249 - val_accuracy: 0.4051 - val_precision_m: 0.0583 - val_recall_m: 0.9357 - val_f1_m: 0.1090\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.51620\n",
      "Epoch 29/100\n",
      "32/32 - 1s - loss: 0.8012 - accuracy: 0.8522 - precision_m: 0.2443 - recall_m: 0.7946 - f1_m: 0.3586 - val_loss: 0.9241 - val_accuracy: 0.4188 - val_precision_m: 0.0590 - val_recall_m: 0.9241 - val_f1_m: 0.1101\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.51620\n",
      "Epoch 30/100\n",
      "32/32 - 1s - loss: 0.7803 - accuracy: 0.8635 - precision_m: 0.2795 - recall_m: 0.8617 - f1_m: 0.4054 - val_loss: 0.9223 - val_accuracy: 0.3998 - val_precision_m: 0.0610 - val_recall_m: 0.9696 - val_f1_m: 0.1140\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.51620\n",
      "Epoch 31/100\n",
      "32/32 - 1s - loss: 0.7832 - accuracy: 0.8763 - precision_m: 0.2862 - recall_m: 0.8057 - f1_m: 0.4091 - val_loss: 0.9166 - val_accuracy: 0.4919 - val_precision_m: 0.0681 - val_recall_m: 0.9166 - val_f1_m: 0.1258\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.51620\n",
      "Epoch 32/100\n",
      "32/32 - 1s - loss: 0.7857 - accuracy: 0.8717 - precision_m: 0.2760 - recall_m: 0.7970 - f1_m: 0.3926 - val_loss: 0.9143 - val_accuracy: 0.5520 - val_precision_m: 0.0716 - val_recall_m: 0.8512 - val_f1_m: 0.1308\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.51620\n",
      "Epoch 33/100\n",
      "32/32 - 1s - loss: 0.7896 - accuracy: 0.8595 - precision_m: 0.2574 - recall_m: 0.8067 - f1_m: 0.3705 - val_loss: 0.9129 - val_accuracy: 0.5247 - val_precision_m: 0.0716 - val_recall_m: 0.8956 - val_f1_m: 0.1314\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.51620\n",
      "Epoch 34/100\n",
      "32/32 - 1s - loss: 0.7724 - accuracy: 0.8767 - precision_m: 0.2919 - recall_m: 0.8244 - f1_m: 0.4180 - val_loss: 0.9024 - val_accuracy: 0.6504 - val_precision_m: 0.0879 - val_recall_m: 0.8244 - val_f1_m: 0.1574\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.51620\n",
      "Epoch 35/100\n",
      "32/32 - 1s - loss: 0.7740 - accuracy: 0.8847 - precision_m: 0.2991 - recall_m: 0.7831 - f1_m: 0.4186 - val_loss: 0.9143 - val_accuracy: 0.5149 - val_precision_m: 0.0696 - val_recall_m: 0.8844 - val_f1_m: 0.1280\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.51620\n",
      "Epoch 36/100\n",
      "32/32 - 1s - loss: 0.7694 - accuracy: 0.8859 - precision_m: 0.3059 - recall_m: 0.8125 - f1_m: 0.4259 - val_loss: 0.9046 - val_accuracy: 0.6385 - val_precision_m: 0.0837 - val_recall_m: 0.7778 - val_f1_m: 0.1496\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.51620\n",
      "Epoch 37/100\n",
      "32/32 - 1s - loss: 0.7667 - accuracy: 0.8817 - precision_m: 0.3091 - recall_m: 0.7881 - f1_m: 0.4259 - val_loss: 0.9026 - val_accuracy: 0.7772 - val_precision_m: 0.0937 - val_recall_m: 0.5439 - val_f1_m: 0.1593\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.51620\n",
      "Epoch 38/100\n",
      "32/32 - 1s - loss: 0.7557 - accuracy: 0.9070 - precision_m: 0.3519 - recall_m: 0.8055 - f1_m: 0.4737 - val_loss: 0.9090 - val_accuracy: 0.7890 - val_precision_m: 0.0878 - val_recall_m: 0.4543 - val_f1_m: 0.1457\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.51620\n",
      "Epoch 39/100\n",
      "32/32 - 1s - loss: 0.7587 - accuracy: 0.9047 - precision_m: 0.3429 - recall_m: 0.7642 - f1_m: 0.4537 - val_loss: 0.8940 - val_accuracy: 0.7989 - val_precision_m: 0.1126 - val_recall_m: 0.5862 - val_f1_m: 0.1869\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.51620\n",
      "Epoch 40/100\n",
      "32/32 - 1s - loss: 0.7471 - accuracy: 0.9095 - precision_m: 0.3618 - recall_m: 0.7853 - f1_m: 0.4789 - val_loss: 0.8858 - val_accuracy: 0.8201 - val_precision_m: 0.1298 - val_recall_m: 0.5791 - val_f1_m: 0.2108\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.51620\n",
      "Epoch 41/100\n",
      "32/32 - 1s - loss: 0.7355 - accuracy: 0.9085 - precision_m: 0.3705 - recall_m: 0.8234 - f1_m: 0.4920 - val_loss: 0.8880 - val_accuracy: 0.8131 - val_precision_m: 0.1217 - val_recall_m: 0.5756 - val_f1_m: 0.1996\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.51620\n",
      "Epoch 42/100\n",
      "32/32 - 1s - loss: 0.7372 - accuracy: 0.9182 - precision_m: 0.3883 - recall_m: 0.7981 - f1_m: 0.5005 - val_loss: 0.9037 - val_accuracy: 0.8423 - val_precision_m: 0.1008 - val_recall_m: 0.3950 - val_f1_m: 0.1583\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.51620\n",
      "Epoch 43/100\n",
      "32/32 - 1s - loss: 0.7335 - accuracy: 0.9103 - precision_m: 0.3698 - recall_m: 0.8016 - f1_m: 0.4912 - val_loss: 0.9208 - val_accuracy: 0.9097 - val_precision_m: 0.1030 - val_recall_m: 0.1568 - val_f1_m: 0.1217\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.51620\n",
      "Epoch 44/100\n",
      "32/32 - 1s - loss: 0.7405 - accuracy: 0.9229 - precision_m: 0.4002 - recall_m: 0.7435 - f1_m: 0.4882 - val_loss: 0.8953 - val_accuracy: 0.8192 - val_precision_m: 0.1084 - val_recall_m: 0.4807 - val_f1_m: 0.1750\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.51620\n",
      "Epoch 45/100\n",
      "32/32 - 1s - loss: 0.7218 - accuracy: 0.9175 - precision_m: 0.3970 - recall_m: 0.7907 - f1_m: 0.5128 - val_loss: 0.8793 - val_accuracy: 0.7649 - val_precision_m: 0.1220 - val_recall_m: 0.7239 - val_f1_m: 0.2068\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.51620\n",
      "Epoch 46/100\n",
      "32/32 - 1s - loss: 0.7246 - accuracy: 0.9193 - precision_m: 0.3991 - recall_m: 0.8061 - f1_m: 0.4972 - val_loss: 0.8888 - val_accuracy: 0.7437 - val_precision_m: 0.1059 - val_recall_m: 0.7085 - val_f1_m: 0.1830\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.51620\n",
      "Epoch 47/100\n",
      "32/32 - 1s - loss: 0.7166 - accuracy: 0.9281 - precision_m: 0.4258 - recall_m: 0.7696 - f1_m: 0.5280 - val_loss: 0.8918 - val_accuracy: 0.7940 - val_precision_m: 0.1100 - val_recall_m: 0.5867 - val_f1_m: 0.1829\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.51620\n",
      "Epoch 48/100\n",
      "32/32 - 1s - loss: 0.7074 - accuracy: 0.9335 - precision_m: 0.4428 - recall_m: 0.7760 - f1_m: 0.5460 - val_loss: 0.8959 - val_accuracy: 0.6885 - val_precision_m: 0.0932 - val_recall_m: 0.7356 - val_f1_m: 0.1639\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.51620\n",
      "Epoch 49/100\n",
      "32/32 - 1s - loss: 0.7199 - accuracy: 0.9244 - precision_m: 0.4107 - recall_m: 0.7598 - f1_m: 0.5078 - val_loss: 0.8893 - val_accuracy: 0.7443 - val_precision_m: 0.1058 - val_recall_m: 0.6986 - val_f1_m: 0.1816\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.51620\n",
      "Epoch 50/100\n",
      "32/32 - 1s - loss: 0.7147 - accuracy: 0.9161 - precision_m: 0.3846 - recall_m: 0.7833 - f1_m: 0.4948 - val_loss: 0.8795 - val_accuracy: 0.7743 - val_precision_m: 0.1198 - val_recall_m: 0.6904 - val_f1_m: 0.2021\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.51620\n",
      "Epoch 51/100\n",
      "32/32 - 1s - loss: 0.7268 - accuracy: 0.9176 - precision_m: 0.3846 - recall_m: 0.7222 - f1_m: 0.4738 - val_loss: 0.8612 - val_accuracy: 0.8299 - val_precision_m: 0.1816 - val_recall_m: 0.6435 - val_f1_m: 0.2770\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.51620\n",
      "Epoch 52/100\n",
      "32/32 - 1s - loss: 0.7019 - accuracy: 0.9344 - precision_m: 0.4508 - recall_m: 0.7576 - f1_m: 0.5387 - val_loss: 0.8664 - val_accuracy: 0.8491 - val_precision_m: 0.1505 - val_recall_m: 0.5819 - val_f1_m: 0.2375\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.51620\n",
      "Epoch 53/100\n",
      "32/32 - 1s - loss: 0.6833 - accuracy: 0.9386 - precision_m: 0.4717 - recall_m: 0.7907 - f1_m: 0.5673 - val_loss: 0.9057 - val_accuracy: 0.9360 - val_precision_m: 0.1585 - val_recall_m: 0.1634 - val_f1_m: 0.1554\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.51620\n",
      "Epoch 54/100\n",
      "32/32 - 1s - loss: 0.6877 - accuracy: 0.9352 - precision_m: 0.4550 - recall_m: 0.7733 - f1_m: 0.5474 - val_loss: 0.8907 - val_accuracy: 0.9059 - val_precision_m: 0.1539 - val_recall_m: 0.2808 - val_f1_m: 0.1939\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.51620\n",
      "Epoch 55/100\n",
      "32/32 - 1s - loss: 0.6643 - accuracy: 0.9461 - precision_m: 0.5144 - recall_m: 0.8025 - f1_m: 0.6090 - val_loss: 0.9396 - val_accuracy: 0.9568 - val_precision_m: 0.0892 - val_recall_m: 0.0241 - val_f1_m: 0.0378\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.51620\n",
      "Epoch 56/100\n",
      "32/32 - 1s - loss: 0.6684 - accuracy: 0.9449 - precision_m: 0.5023 - recall_m: 0.7758 - f1_m: 0.5894 - val_loss: 0.9023 - val_accuracy: 0.9284 - val_precision_m: 0.1890 - val_recall_m: 0.1871 - val_f1_m: 0.1703\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.51620\n",
      "Epoch 57/100\n",
      "32/32 - 1s - loss: 0.6614 - accuracy: 0.9505 - precision_m: 0.5401 - recall_m: 0.7658 - f1_m: 0.6070 - val_loss: 0.8857 - val_accuracy: 0.9045 - val_precision_m: 0.1707 - val_recall_m: 0.3308 - val_f1_m: 0.2101\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.51620\n",
      "Epoch 58/100\n",
      "32/32 - 1s - loss: 0.6611 - accuracy: 0.9412 - precision_m: 0.4854 - recall_m: 0.7744 - f1_m: 0.5736 - val_loss: 0.8185 - val_accuracy: 0.9183 - val_precision_m: 0.3249 - val_recall_m: 0.5293 - val_f1_m: 0.3908\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.51620\n",
      "Epoch 59/100\n",
      "32/32 - 1s - loss: 0.6479 - accuracy: 0.9498 - precision_m: 0.5418 - recall_m: 0.7502 - f1_m: 0.6155 - val_loss: 0.8254 - val_accuracy: 0.9162 - val_precision_m: 0.2650 - val_recall_m: 0.5248 - val_f1_m: 0.3480\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.51620\n",
      "Epoch 60/100\n",
      "32/32 - 1s - loss: 0.6430 - accuracy: 0.9486 - precision_m: 0.5222 - recall_m: 0.8120 - f1_m: 0.6040 - val_loss: 0.8368 - val_accuracy: 0.9199 - val_precision_m: 0.2650 - val_recall_m: 0.4337 - val_f1_m: 0.3165\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.51620\n",
      "Epoch 61/100\n",
      "32/32 - 1s - loss: 0.6478 - accuracy: 0.9440 - precision_m: 0.4997 - recall_m: 0.7826 - f1_m: 0.5874 - val_loss: 0.8231 - val_accuracy: 0.8961 - val_precision_m: 0.2706 - val_recall_m: 0.5682 - val_f1_m: 0.3536\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.51620\n",
      "Epoch 62/100\n",
      "32/32 - 1s - loss: 0.6339 - accuracy: 0.9536 - precision_m: 0.5595 - recall_m: 0.7752 - f1_m: 0.6266 - val_loss: 0.8358 - val_accuracy: 0.8981 - val_precision_m: 0.2308 - val_recall_m: 0.5027 - val_f1_m: 0.3080\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.51620\n",
      "Epoch 63/100\n",
      "32/32 - 1s - loss: 0.6117 - accuracy: 0.9620 - precision_m: 0.6268 - recall_m: 0.7760 - f1_m: 0.6793 - val_loss: 0.8407 - val_accuracy: 0.9116 - val_precision_m: 0.2525 - val_recall_m: 0.4622 - val_f1_m: 0.3125\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.51620\n",
      "Epoch 64/100\n",
      "32/32 - 1s - loss: 0.6174 - accuracy: 0.9591 - precision_m: 0.5987 - recall_m: 0.7877 - f1_m: 0.6579 - val_loss: 0.8376 - val_accuracy: 0.8661 - val_precision_m: 0.1836 - val_recall_m: 0.6102 - val_f1_m: 0.2788\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.51620\n",
      "Epoch 65/100\n",
      "32/32 - 1s - loss: 0.6039 - accuracy: 0.9596 - precision_m: 0.6028 - recall_m: 0.7991 - f1_m: 0.6626 - val_loss: 0.8272 - val_accuracy: 0.9112 - val_precision_m: 0.2562 - val_recall_m: 0.4773 - val_f1_m: 0.3206\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.51620\n",
      "Epoch 66/100\n",
      "32/32 - 1s - loss: 0.6045 - accuracy: 0.9614 - precision_m: 0.6239 - recall_m: 0.7472 - f1_m: 0.6601 - val_loss: 0.8067 - val_accuracy: 0.9136 - val_precision_m: 0.2850 - val_recall_m: 0.5372 - val_f1_m: 0.3659\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.51620\n",
      "Epoch 67/100\n",
      "32/32 - 1s - loss: 0.5933 - accuracy: 0.9596 - precision_m: 0.6030 - recall_m: 0.7939 - f1_m: 0.6665 - val_loss: 0.8057 - val_accuracy: 0.9150 - val_precision_m: 0.2744 - val_recall_m: 0.5512 - val_f1_m: 0.3630\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.51620\n",
      "Epoch 68/100\n",
      "32/32 - 1s - loss: 0.5915 - accuracy: 0.9605 - precision_m: 0.6072 - recall_m: 0.7733 - f1_m: 0.6620 - val_loss: 0.8165 - val_accuracy: 0.9598 - val_precision_m: 0.5564 - val_recall_m: 0.2954 - val_f1_m: 0.3772\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.51620\n",
      "Epoch 69/100\n",
      "32/32 - 1s - loss: 0.5706 - accuracy: 0.9654 - precision_m: 0.6576 - recall_m: 0.7820 - f1_m: 0.6984 - val_loss: 0.8032 - val_accuracy: 0.9196 - val_precision_m: 0.2783 - val_recall_m: 0.5162 - val_f1_m: 0.3556\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.51620\n",
      "Epoch 70/100\n",
      "32/32 - 1s - loss: 0.5844 - accuracy: 0.9608 - precision_m: 0.6246 - recall_m: 0.7620 - f1_m: 0.6615 - val_loss: 0.8211 - val_accuracy: 0.8755 - val_precision_m: 0.2117 - val_recall_m: 0.5949 - val_f1_m: 0.3058\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.51620\n",
      "Epoch 71/100\n",
      "32/32 - 1s - loss: 0.5628 - accuracy: 0.9663 - precision_m: 0.6613 - recall_m: 0.7945 - f1_m: 0.6978 - val_loss: 0.7922 - val_accuracy: 0.9250 - val_precision_m: 0.2826 - val_recall_m: 0.5373 - val_f1_m: 0.3673\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.51620\n",
      "Epoch 72/100\n",
      "32/32 - 1s - loss: 0.5487 - accuracy: 0.9684 - precision_m: 0.6878 - recall_m: 0.7945 - f1_m: 0.7185 - val_loss: 0.7956 - val_accuracy: 0.9526 - val_precision_m: 0.3969 - val_recall_m: 0.3969 - val_f1_m: 0.3903\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.51620\n",
      "Epoch 73/100\n",
      "32/32 - 1s - loss: 0.5480 - accuracy: 0.9696 - precision_m: 0.7036 - recall_m: 0.7889 - f1_m: 0.7208 - val_loss: 0.8312 - val_accuracy: 0.9503 - val_precision_m: 0.3180 - val_recall_m: 0.3128 - val_f1_m: 0.2985\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.51620\n",
      "Epoch 74/100\n",
      "32/32 - 1s - loss: 0.5654 - accuracy: 0.9646 - precision_m: 0.6412 - recall_m: 0.7681 - f1_m: 0.6777 - val_loss: 0.7804 - val_accuracy: 0.9400 - val_precision_m: 0.3693 - val_recall_m: 0.4624 - val_f1_m: 0.4030\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.51620\n",
      "Epoch 75/100\n",
      "32/32 - 1s - loss: 0.5301 - accuracy: 0.9707 - precision_m: 0.7378 - recall_m: 0.7752 - f1_m: 0.7390 - val_loss: 0.7908 - val_accuracy: 0.9341 - val_precision_m: 0.3082 - val_recall_m: 0.4587 - val_f1_m: 0.3645\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.51620\n",
      "Epoch 76/100\n",
      "32/32 - 1s - loss: 0.5244 - accuracy: 0.9690 - precision_m: 0.6994 - recall_m: 0.7822 - f1_m: 0.7275 - val_loss: 0.7906 - val_accuracy: 0.9246 - val_precision_m: 0.2825 - val_recall_m: 0.4997 - val_f1_m: 0.3582\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.51620\n",
      "Epoch 77/100\n",
      "32/32 - 1s - loss: 0.5189 - accuracy: 0.9702 - precision_m: 0.7164 - recall_m: 0.7662 - f1_m: 0.7203 - val_loss: 0.7984 - val_accuracy: 0.9258 - val_precision_m: 0.2637 - val_recall_m: 0.4726 - val_f1_m: 0.3374\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.51620\n",
      "Epoch 78/100\n",
      "32/32 - 1s - loss: 0.5240 - accuracy: 0.9687 - precision_m: 0.6984 - recall_m: 0.7981 - f1_m: 0.7170 - val_loss: 0.7944 - val_accuracy: 0.9116 - val_precision_m: 0.2628 - val_recall_m: 0.5321 - val_f1_m: 0.3479\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.51620\n",
      "Epoch 79/100\n",
      "32/32 - 1s - loss: 0.5160 - accuracy: 0.9710 - precision_m: 0.7331 - recall_m: 0.7580 - f1_m: 0.7326 - val_loss: 0.7938 - val_accuracy: 0.9631 - val_precision_m: 0.5874 - val_recall_m: 0.2886 - val_f1_m: 0.3779\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.51620\n",
      "Epoch 80/100\n",
      "32/32 - 1s - loss: 0.5090 - accuracy: 0.9686 - precision_m: 0.6987 - recall_m: 0.7769 - f1_m: 0.7156 - val_loss: 0.7692 - val_accuracy: 0.9623 - val_precision_m: 0.5905 - val_recall_m: 0.3193 - val_f1_m: 0.4097\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.51620\n",
      "Epoch 81/100\n",
      "32/32 - 1s - loss: 0.4859 - accuracy: 0.9731 - precision_m: 0.7441 - recall_m: 0.7946 - f1_m: 0.7561 - val_loss: 0.7586 - val_accuracy: 0.9602 - val_precision_m: 0.5321 - val_recall_m: 0.3785 - val_f1_m: 0.4302\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.51620\n",
      "Epoch 82/100\n",
      "32/32 - 1s - loss: 0.4745 - accuracy: 0.9743 - precision_m: 0.7748 - recall_m: 0.7953 - f1_m: 0.7717 - val_loss: 0.7596 - val_accuracy: 0.9587 - val_precision_m: 0.5311 - val_recall_m: 0.3440 - val_f1_m: 0.4150\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.51620\n",
      "Epoch 83/100\n",
      "32/32 - 1s - loss: 0.4846 - accuracy: 0.9725 - precision_m: 0.7337 - recall_m: 0.8047 - f1_m: 0.7530 - val_loss: 0.7354 - val_accuracy: 0.9622 - val_precision_m: 0.5327 - val_recall_m: 0.3923 - val_f1_m: 0.4422\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.51620\n",
      "Epoch 84/100\n",
      "32/32 - 1s - loss: 0.4720 - accuracy: 0.9738 - precision_m: 0.7645 - recall_m: 0.7856 - f1_m: 0.7602 - val_loss: 0.7718 - val_accuracy: 0.9552 - val_precision_m: 0.4681 - val_recall_m: 0.3352 - val_f1_m: 0.3878\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.51620\n",
      "Epoch 85/100\n",
      "32/32 - 1s - loss: 0.4579 - accuracy: 0.9739 - precision_m: 0.7588 - recall_m: 0.7887 - f1_m: 0.7617 - val_loss: 0.7452 - val_accuracy: 0.9514 - val_precision_m: 0.4275 - val_recall_m: 0.4251 - val_f1_m: 0.4221\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.51620\n",
      "Epoch 86/100\n",
      "32/32 - 1s - loss: 0.4501 - accuracy: 0.9762 - precision_m: 0.7698 - recall_m: 0.8152 - f1_m: 0.7768 - val_loss: 0.7716 - val_accuracy: 0.9625 - val_precision_m: 0.6103 - val_recall_m: 0.2707 - val_f1_m: 0.3707\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.51620\n",
      "Epoch 87/100\n",
      "32/32 - 1s - loss: 0.4191 - accuracy: 0.9793 - precision_m: 0.8185 - recall_m: 0.8179 - f1_m: 0.8089 - val_loss: 0.7869 - val_accuracy: 0.9608 - val_precision_m: 0.5450 - val_recall_m: 0.2529 - val_f1_m: 0.3436\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.51620\n",
      "Epoch 88/100\n",
      "32/32 - 1s - loss: 0.4673 - accuracy: 0.9734 - precision_m: 0.7579 - recall_m: 0.7674 - f1_m: 0.7443 - val_loss: 0.7496 - val_accuracy: 0.9540 - val_precision_m: 0.4513 - val_recall_m: 0.3891 - val_f1_m: 0.4068\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.51620\n",
      "Epoch 89/100\n",
      "32/32 - 1s - loss: 0.4496 - accuracy: 0.9761 - precision_m: 0.7642 - recall_m: 0.7671 - f1_m: 0.7514 - val_loss: 0.7809 - val_accuracy: 0.9551 - val_precision_m: 0.4451 - val_recall_m: 0.3011 - val_f1_m: 0.3512\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.51620\n",
      "Epoch 90/100\n",
      "32/32 - 1s - loss: 0.4485 - accuracy: 0.9757 - precision_m: 0.7857 - recall_m: 0.7575 - f1_m: 0.7551 - val_loss: 0.7494 - val_accuracy: 0.9622 - val_precision_m: 0.6007 - val_recall_m: 0.3071 - val_f1_m: 0.3987\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.51620\n",
      "Epoch 91/100\n",
      "32/32 - 1s - loss: 0.4300 - accuracy: 0.9781 - precision_m: 0.8079 - recall_m: 0.7849 - f1_m: 0.7814 - val_loss: 0.7399 - val_accuracy: 0.9564 - val_precision_m: 0.4798 - val_recall_m: 0.3812 - val_f1_m: 0.4155\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.51620\n",
      "Epoch 92/100\n",
      "32/32 - 1s - loss: 0.4003 - accuracy: 0.9792 - precision_m: 0.8277 - recall_m: 0.8035 - f1_m: 0.8060 - val_loss: 0.7043 - val_accuracy: 0.9593 - val_precision_m: 0.5166 - val_recall_m: 0.4173 - val_f1_m: 0.4575\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.51620\n",
      "Epoch 93/100\n",
      "32/32 - 1s - loss: 0.4213 - accuracy: 0.9772 - precision_m: 0.7880 - recall_m: 0.7942 - f1_m: 0.7789 - val_loss: 0.7315 - val_accuracy: 0.9550 - val_precision_m: 0.4482 - val_recall_m: 0.3848 - val_f1_m: 0.4089\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.51620\n",
      "Epoch 94/100\n",
      "32/32 - 1s - loss: 0.4258 - accuracy: 0.9757 - precision_m: 0.7692 - recall_m: 0.7780 - f1_m: 0.7542 - val_loss: 0.7255 - val_accuracy: 0.9578 - val_precision_m: 0.5098 - val_recall_m: 0.3724 - val_f1_m: 0.4215\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.51620\n",
      "Epoch 95/100\n",
      "32/32 - 1s - loss: 0.4323 - accuracy: 0.9747 - precision_m: 0.7699 - recall_m: 0.7739 - f1_m: 0.7576 - val_loss: 0.7030 - val_accuracy: 0.9586 - val_precision_m: 0.5186 - val_recall_m: 0.4166 - val_f1_m: 0.4538\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.51620\n",
      "Epoch 96/100\n",
      "32/32 - 1s - loss: 0.3994 - accuracy: 0.9783 - precision_m: 0.8245 - recall_m: 0.7672 - f1_m: 0.7872 - val_loss: 0.7700 - val_accuracy: 0.9618 - val_precision_m: 0.5981 - val_recall_m: 0.2343 - val_f1_m: 0.3283\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.51620\n",
      "Epoch 97/100\n",
      "32/32 - 1s - loss: 0.3961 - accuracy: 0.9784 - precision_m: 0.7921 - recall_m: 0.8208 - f1_m: 0.7910 - val_loss: 0.7176 - val_accuracy: 0.9640 - val_precision_m: 0.5975 - val_recall_m: 0.3289 - val_f1_m: 0.4166\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.51620\n",
      "Epoch 98/100\n",
      "32/32 - 1s - loss: 0.3939 - accuracy: 0.9784 - precision_m: 0.8039 - recall_m: 0.7898 - f1_m: 0.7829 - val_loss: 0.7014 - val_accuracy: 0.9593 - val_precision_m: 0.5335 - val_recall_m: 0.3889 - val_f1_m: 0.4402\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.51620\n",
      "Epoch 99/100\n",
      "32/32 - 1s - loss: 0.3791 - accuracy: 0.9791 - precision_m: 0.8226 - recall_m: 0.8036 - f1_m: 0.8008 - val_loss: 0.6989 - val_accuracy: 0.9601 - val_precision_m: 0.5299 - val_recall_m: 0.3931 - val_f1_m: 0.4453\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.51620\n",
      "Epoch 100/100\n",
      "32/32 - 1s - loss: 0.3989 - accuracy: 0.9763 - precision_m: 0.7897 - recall_m: 0.7761 - f1_m: 0.7664 - val_loss: 0.7334 - val_accuracy: 0.9616 - val_precision_m: 0.5834 - val_recall_m: 0.2778 - val_f1_m: 0.3738\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.51620\n",
      "5/5 - 0s - loss: 0.5640 - accuracy: 0.9560 - precision_m: 0.7364 - recall_m: 0.3752 - f1_m: 0.4916\n",
      "The metrics for the test set with loss tversky, learning rate 0.0001,  filters 8, and batch size 8 is [0.5640011429786682, 0.956034779548645, 0.7364265322685242, 0.37521377205848694, 0.4916296601295471]\n",
      "__________________________________________________________________________________________________________________________________________________________________________\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 128, 128, 5) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 128, 128, 16) 736         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 128, 128, 16) 64          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 128, 128, 16) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 16) 2320        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 16) 64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, 128, 16) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 64, 64, 16)   0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 32)   4640        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64, 64, 32)   128         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 64, 64, 32)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 64, 32)   9248        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 32)   128         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64, 64, 32)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 32)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 64)   18496       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 64)   256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 64)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 64)   36928       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 64)   256         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 64)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 64)   0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 16, 16, 128)  73856       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 16, 16, 128)  512         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 16, 16, 128)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 16, 16, 128)  147584      activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 16, 16, 128)  512         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16, 16, 128)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 128)    0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 8, 8, 256)    295168      max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 8, 8, 256)    1024        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 8, 8, 256)    0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 8, 8, 256)    590080      activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 8, 8, 256)    1024        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 8, 8, 256)    0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 16, 16, 128)  131200      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 16, 16, 256)  0           conv2d_transpose[0][0]           \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 128)  295040      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 128)  512         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 128)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 128)  147584      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 128)  512         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 128)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 32, 32, 64)   32832       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 128)  0           conv2d_transpose_1[0][0]         \n",
      "                                                                 activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 64)   73792       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 32, 32, 64)   256         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 32, 32, 64)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 32, 64)   36928       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 32, 32, 64)   256         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 32, 32, 64)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 64, 64, 32)   8224        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 64, 64, 64)   0           conv2d_transpose_2[0][0]         \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 32)   18464       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64, 64, 32)   128         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 64, 64, 32)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 32)   9248        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 64, 64, 32)   128         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 64, 64, 32)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 128, 128, 16) 2064        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 128, 128, 32) 0           conv2d_transpose_3[0][0]         \n",
      "                                                                 activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 128, 128, 16) 4624        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 128, 128, 16) 64          conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 128, 128, 16) 0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 128, 128, 16) 2320        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 128, 128, 16) 64          conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 128, 128, 16) 0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 128, 128, 1)  17          activation_17[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,947,281\n",
      "Trainable params: 1,944,337\n",
      "Non-trainable params: 2,944\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "Initialising in 3..........2..........1......................................\n",
      "Training on 284 images with learning rate 0.0001, batch size 8, and number of filters 16\n",
      "Epoch 1/100\n",
      "32/32 - 3s - loss: 0.9219 - accuracy: 0.6814 - precision_m: 0.0554 - recall_m: 0.3320 - f1_m: 0.0891 - val_loss: 0.9441 - val_accuracy: 0.6843 - val_precision_m: 0.0449 - val_recall_m: 0.3809 - val_f1_m: 0.0789\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.51620\n",
      "Epoch 2/100\n",
      "32/32 - 1s - loss: 0.9134 - accuracy: 0.3682 - precision_m: 0.0751 - recall_m: 0.8835 - f1_m: 0.1363 - val_loss: 0.9434 - val_accuracy: 0.3378 - val_precision_m: 0.0470 - val_recall_m: 0.8640 - val_f1_m: 0.0886\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.51620\n",
      "Epoch 3/100\n",
      "32/32 - 1s - loss: 0.9041 - accuracy: 0.4297 - precision_m: 0.0797 - recall_m: 0.8586 - f1_m: 0.1440 - val_loss: 0.9416 - val_accuracy: 0.2842 - val_precision_m: 0.0461 - val_recall_m: 0.9050 - val_f1_m: 0.0873\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.51620\n",
      "Epoch 4/100\n",
      "32/32 - 1s - loss: 0.8975 - accuracy: 0.4752 - precision_m: 0.0834 - recall_m: 0.8286 - f1_m: 0.1497 - val_loss: 0.9398 - val_accuracy: 0.2604 - val_precision_m: 0.0472 - val_recall_m: 0.9491 - val_f1_m: 0.0896\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.51620\n",
      "Epoch 5/100\n",
      "32/32 - 1s - loss: 0.8879 - accuracy: 0.5371 - precision_m: 0.0939 - recall_m: 0.8275 - f1_m: 0.1663 - val_loss: 0.9380 - val_accuracy: 0.2894 - val_precision_m: 0.0491 - val_recall_m: 0.9398 - val_f1_m: 0.0929\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.51620\n",
      "Epoch 6/100\n",
      "32/32 - 1s - loss: 0.8837 - accuracy: 0.5803 - precision_m: 0.0988 - recall_m: 0.7776 - f1_m: 0.1715 - val_loss: 0.9357 - val_accuracy: 0.3048 - val_precision_m: 0.0511 - val_recall_m: 0.9487 - val_f1_m: 0.0964\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.51620\n",
      "Epoch 7/100\n",
      "32/32 - 1s - loss: 0.8748 - accuracy: 0.6152 - precision_m: 0.1074 - recall_m: 0.7775 - f1_m: 0.1861 - val_loss: 0.9345 - val_accuracy: 0.3210 - val_precision_m: 0.0519 - val_recall_m: 0.9492 - val_f1_m: 0.0978\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.51620\n",
      "Epoch 8/100\n",
      "32/32 - 1s - loss: 0.8697 - accuracy: 0.6710 - precision_m: 0.1168 - recall_m: 0.7125 - f1_m: 0.1963 - val_loss: 0.9334 - val_accuracy: 0.3700 - val_precision_m: 0.0529 - val_recall_m: 0.9010 - val_f1_m: 0.0992\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.51620\n",
      "Epoch 9/100\n",
      "32/32 - 1s - loss: 0.8617 - accuracy: 0.6727 - precision_m: 0.1224 - recall_m: 0.7659 - f1_m: 0.2061 - val_loss: 0.9305 - val_accuracy: 0.4469 - val_precision_m: 0.0552 - val_recall_m: 0.8147 - val_f1_m: 0.1022\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.51620\n",
      "Epoch 10/100\n",
      "32/32 - 1s - loss: 0.8554 - accuracy: 0.7081 - precision_m: 0.1338 - recall_m: 0.7612 - f1_m: 0.2230 - val_loss: 0.9299 - val_accuracy: 0.5218 - val_precision_m: 0.0570 - val_recall_m: 0.6685 - val_f1_m: 0.1036\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.51620\n",
      "Epoch 11/100\n",
      "32/32 - 1s - loss: 0.8543 - accuracy: 0.7090 - precision_m: 0.1325 - recall_m: 0.7505 - f1_m: 0.2205 - val_loss: 0.9314 - val_accuracy: 0.5153 - val_precision_m: 0.0551 - val_recall_m: 0.6738 - val_f1_m: 0.1006\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.51620\n",
      "Epoch 12/100\n",
      "32/32 - 1s - loss: 0.8493 - accuracy: 0.7063 - precision_m: 0.1347 - recall_m: 0.7755 - f1_m: 0.2263 - val_loss: 0.9315 - val_accuracy: 0.4821 - val_precision_m: 0.0548 - val_recall_m: 0.7485 - val_f1_m: 0.1007\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.51620\n",
      "Epoch 13/100\n",
      "32/32 - 1s - loss: 0.8354 - accuracy: 0.7576 - precision_m: 0.1621 - recall_m: 0.7951 - f1_m: 0.2608 - val_loss: 0.9056 - val_accuracy: 0.8111 - val_precision_m: 0.0968 - val_recall_m: 0.4109 - val_f1_m: 0.1543\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.51620\n",
      "Epoch 14/100\n",
      "32/32 - 1s - loss: 0.8175 - accuracy: 0.7752 - precision_m: 0.1831 - recall_m: 0.8310 - f1_m: 0.2940 - val_loss: 0.9098 - val_accuracy: 0.8866 - val_precision_m: 0.1341 - val_recall_m: 0.3237 - val_f1_m: 0.1880\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.51620\n",
      "Epoch 15/100\n",
      "32/32 - 1s - loss: 0.8177 - accuracy: 0.7834 - precision_m: 0.1835 - recall_m: 0.8158 - f1_m: 0.2927 - val_loss: 0.9237 - val_accuracy: 0.4839 - val_precision_m: 0.0606 - val_recall_m: 0.8196 - val_f1_m: 0.1118\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.51620\n",
      "Epoch 16/100\n",
      "32/32 - 1s - loss: 0.8038 - accuracy: 0.8013 - precision_m: 0.2011 - recall_m: 0.8546 - f1_m: 0.3171 - val_loss: 0.9195 - val_accuracy: 0.4627 - val_precision_m: 0.0664 - val_recall_m: 0.9161 - val_f1_m: 0.1227\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.51620\n",
      "Epoch 17/100\n",
      "32/32 - 1s - loss: 0.8062 - accuracy: 0.8054 - precision_m: 0.1970 - recall_m: 0.8260 - f1_m: 0.3107 - val_loss: 0.9116 - val_accuracy: 0.5776 - val_precision_m: 0.0771 - val_recall_m: 0.8208 - val_f1_m: 0.1395\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.51620\n",
      "Epoch 18/100\n",
      "32/32 - 1s - loss: 0.8064 - accuracy: 0.8077 - precision_m: 0.2054 - recall_m: 0.8074 - f1_m: 0.3105 - val_loss: 0.9079 - val_accuracy: 0.5766 - val_precision_m: 0.0787 - val_recall_m: 0.8409 - val_f1_m: 0.1425\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.51620\n",
      "Epoch 19/100\n",
      "32/32 - 1s - loss: 0.7869 - accuracy: 0.8383 - precision_m: 0.2404 - recall_m: 0.8225 - f1_m: 0.3572 - val_loss: 0.9208 - val_accuracy: 0.4521 - val_precision_m: 0.0636 - val_recall_m: 0.9088 - val_f1_m: 0.1178\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.51620\n",
      "Epoch 20/100\n",
      "32/32 - 1s - loss: 0.8041 - accuracy: 0.8198 - precision_m: 0.2007 - recall_m: 0.7680 - f1_m: 0.3088 - val_loss: 0.9236 - val_accuracy: 0.3999 - val_precision_m: 0.0588 - val_recall_m: 0.9333 - val_f1_m: 0.1096\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.51620\n",
      "Epoch 21/100\n",
      "32/32 - 1s - loss: 0.7771 - accuracy: 0.8521 - precision_m: 0.2543 - recall_m: 0.8158 - f1_m: 0.3704 - val_loss: 0.9132 - val_accuracy: 0.5406 - val_precision_m: 0.0729 - val_recall_m: 0.8484 - val_f1_m: 0.1329\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.51620\n",
      "Epoch 22/100\n",
      "32/32 - 1s - loss: 0.7657 - accuracy: 0.8781 - precision_m: 0.2888 - recall_m: 0.7855 - f1_m: 0.4126 - val_loss: 0.8878 - val_accuracy: 0.8138 - val_precision_m: 0.1423 - val_recall_m: 0.6404 - val_f1_m: 0.2299\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.51620\n",
      "Epoch 23/100\n",
      "32/32 - 1s - loss: 0.7673 - accuracy: 0.8966 - precision_m: 0.3150 - recall_m: 0.6963 - f1_m: 0.4211 - val_loss: 0.9063 - val_accuracy: 0.5527 - val_precision_m: 0.0783 - val_recall_m: 0.8949 - val_f1_m: 0.1429\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.51620\n",
      "Epoch 24/100\n",
      "32/32 - 1s - loss: 0.7484 - accuracy: 0.8962 - precision_m: 0.3300 - recall_m: 0.7646 - f1_m: 0.4415 - val_loss: 0.8915 - val_accuracy: 0.7144 - val_precision_m: 0.1075 - val_recall_m: 0.7634 - val_f1_m: 0.1865\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.51620\n",
      "Epoch 25/100\n",
      "32/32 - 1s - loss: 0.7528 - accuracy: 0.9063 - precision_m: 0.3469 - recall_m: 0.6798 - f1_m: 0.4400 - val_loss: 0.8887 - val_accuracy: 0.6991 - val_precision_m: 0.1079 - val_recall_m: 0.7682 - val_f1_m: 0.1865\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.51620\n",
      "Epoch 26/100\n",
      "32/32 - 1s - loss: 0.7380 - accuracy: 0.9152 - precision_m: 0.3835 - recall_m: 0.7141 - f1_m: 0.4814 - val_loss: 0.8817 - val_accuracy: 0.9461 - val_precision_m: 0.4836 - val_recall_m: 0.2338 - val_f1_m: 0.2953\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.51620\n",
      "Epoch 27/100\n",
      "32/32 - 1s - loss: 0.7396 - accuracy: 0.9099 - precision_m: 0.3537 - recall_m: 0.7187 - f1_m: 0.4575 - val_loss: 0.9244 - val_accuracy: 0.9369 - val_precision_m: 0.3053 - val_recall_m: 0.0373 - val_f1_m: 0.0627\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.51620\n",
      "Epoch 28/100\n",
      "32/32 - 1s - loss: 0.7329 - accuracy: 0.9173 - precision_m: 0.3772 - recall_m: 0.7022 - f1_m: 0.4777 - val_loss: 0.9484 - val_accuracy: 0.9591 - val_precision_m: 0.1952 - val_recall_m: 0.0086 - val_f1_m: 0.0166\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.51620\n",
      "Epoch 29/100\n",
      "32/32 - 1s - loss: 0.7270 - accuracy: 0.9138 - precision_m: 0.3735 - recall_m: 0.7050 - f1_m: 0.4588 - val_loss: 0.9330 - val_accuracy: 0.9604 - val_precision_m: 0.1860 - val_recall_m: 0.0331 - val_f1_m: 0.0515\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.51620\n",
      "Epoch 30/100\n",
      "32/32 - 1s - loss: 0.7153 - accuracy: 0.9189 - precision_m: 0.3848 - recall_m: 0.7045 - f1_m: 0.4841 - val_loss: 0.9477 - val_accuracy: 0.9590 - val_precision_m: 0.2049 - val_recall_m: 0.0106 - val_f1_m: 0.0201\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.51620\n",
      "Epoch 31/100\n",
      "32/32 - 1s - loss: 0.7092 - accuracy: 0.9176 - precision_m: 0.3876 - recall_m: 0.7422 - f1_m: 0.4803 - val_loss: 0.9055 - val_accuracy: 0.9501 - val_precision_m: 0.3099 - val_recall_m: 0.0737 - val_f1_m: 0.1094\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.51620\n",
      "Epoch 32/100\n",
      "32/32 - 1s - loss: 0.7099 - accuracy: 0.9224 - precision_m: 0.3903 - recall_m: 0.7248 - f1_m: 0.4752 - val_loss: 0.8658 - val_accuracy: 0.9566 - val_precision_m: 0.5638 - val_recall_m: 0.1992 - val_f1_m: 0.2838\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.51620\n",
      "Epoch 33/100\n",
      "32/32 - 1s - loss: 0.6831 - accuracy: 0.9388 - precision_m: 0.4680 - recall_m: 0.7260 - f1_m: 0.5462 - val_loss: 0.9514 - val_accuracy: 0.9560 - val_precision_m: 0.0169 - val_recall_m: 6.4935e-04 - val_f1_m: 0.0013\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.51620\n",
      "Epoch 34/100\n",
      "32/32 - 1s - loss: 0.6760 - accuracy: 0.9344 - precision_m: 0.4528 - recall_m: 0.7452 - f1_m: 0.5329 - val_loss: 0.8299 - val_accuracy: 0.9020 - val_precision_m: 0.2627 - val_recall_m: 0.5165 - val_f1_m: 0.3337\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.51620\n",
      "Epoch 35/100\n",
      "32/32 - 1s - loss: 0.6538 - accuracy: 0.9468 - precision_m: 0.5178 - recall_m: 0.7621 - f1_m: 0.5910 - val_loss: 0.8225 - val_accuracy: 0.8849 - val_precision_m: 0.2284 - val_recall_m: 0.5950 - val_f1_m: 0.3238\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.51620\n",
      "Epoch 36/100\n",
      "32/32 - 1s - loss: 0.6450 - accuracy: 0.9454 - precision_m: 0.5185 - recall_m: 0.7410 - f1_m: 0.5865 - val_loss: 0.8932 - val_accuracy: 0.9428 - val_precision_m: 0.4633 - val_recall_m: 0.1008 - val_f1_m: 0.1413\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.51620\n",
      "Epoch 37/100\n",
      "32/32 - 1s - loss: 0.6327 - accuracy: 0.9475 - precision_m: 0.5194 - recall_m: 0.7699 - f1_m: 0.5922 - val_loss: 0.9046 - val_accuracy: 0.9534 - val_precision_m: 0.2158 - val_recall_m: 0.0767 - val_f1_m: 0.1047\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.51620\n",
      "Epoch 38/100\n",
      "32/32 - 1s - loss: 0.6316 - accuracy: 0.9513 - precision_m: 0.5518 - recall_m: 0.7438 - f1_m: 0.6006 - val_loss: 0.9566 - val_accuracy: 0.9612 - val_precision_m: 0.2500 - val_recall_m: 0.0022 - val_f1_m: 0.0043\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.51620\n",
      "Epoch 39/100\n",
      "32/32 - 1s - loss: 0.6238 - accuracy: 0.9463 - precision_m: 0.5166 - recall_m: 0.7719 - f1_m: 0.5833 - val_loss: 0.9371 - val_accuracy: 0.9556 - val_precision_m: 0.1405 - val_recall_m: 0.0249 - val_f1_m: 0.0423\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.51620\n",
      "Epoch 40/100\n",
      "32/32 - 1s - loss: 0.6357 - accuracy: 0.9470 - precision_m: 0.5316 - recall_m: 0.6829 - f1_m: 0.5654 - val_loss: 0.9568 - val_accuracy: 0.9606 - val_precision_m: 0.3027 - val_recall_m: 0.0034 - val_f1_m: 0.0064\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.51620\n",
      "Epoch 41/100\n",
      "32/32 - 1s - loss: 0.5858 - accuracy: 0.9572 - precision_m: 0.5953 - recall_m: 0.7611 - f1_m: 0.6490 - val_loss: 0.9354 - val_accuracy: 0.9605 - val_precision_m: 0.2543 - val_recall_m: 0.0221 - val_f1_m: 0.0406\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.51620\n",
      "Epoch 42/100\n",
      "32/32 - 1s - loss: 0.5774 - accuracy: 0.9587 - precision_m: 0.6022 - recall_m: 0.7628 - f1_m: 0.6424 - val_loss: 0.9394 - val_accuracy: 0.9589 - val_precision_m: 0.1131 - val_recall_m: 0.0237 - val_f1_m: 0.0392\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.51620\n",
      "Epoch 43/100\n",
      "32/32 - 1s - loss: 0.5623 - accuracy: 0.9624 - precision_m: 0.6328 - recall_m: 0.7470 - f1_m: 0.6637 - val_loss: 0.9428 - val_accuracy: 0.9619 - val_precision_m: 0.2165 - val_recall_m: 0.0165 - val_f1_m: 0.0307\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.51620\n",
      "Epoch 44/100\n",
      "32/32 - 1s - loss: 0.5840 - accuracy: 0.9528 - precision_m: 0.5894 - recall_m: 0.7031 - f1_m: 0.6145 - val_loss: 0.9612 - val_accuracy: 0.9611 - val_precision_m: 0.2500 - val_recall_m: 5.4682e-04 - val_f1_m: 0.0011\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.51620\n",
      "Epoch 45/100\n",
      "32/32 - 1s - loss: 0.5700 - accuracy: 0.9566 - precision_m: 0.5826 - recall_m: 0.7328 - f1_m: 0.6216 - val_loss: 0.9282 - val_accuracy: 0.9615 - val_precision_m: 0.1462 - val_recall_m: 0.0310 - val_f1_m: 0.0512\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.51620\n",
      "Epoch 46/100\n",
      "32/32 - 1s - loss: 0.5478 - accuracy: 0.9620 - precision_m: 0.6315 - recall_m: 0.7541 - f1_m: 0.6637 - val_loss: 0.9314 - val_accuracy: 0.9600 - val_precision_m: 0.1479 - val_recall_m: 0.0264 - val_f1_m: 0.0448\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.51620\n",
      "Epoch 47/100\n",
      "32/32 - 1s - loss: 0.5371 - accuracy: 0.9635 - precision_m: 0.6674 - recall_m: 0.7316 - f1_m: 0.6701 - val_loss: 0.9266 - val_accuracy: 0.9620 - val_precision_m: 0.1652 - val_recall_m: 0.0306 - val_f1_m: 0.0517\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.51620\n",
      "Epoch 48/100\n",
      "32/32 - 1s - loss: 0.5310 - accuracy: 0.9629 - precision_m: 0.6440 - recall_m: 0.7391 - f1_m: 0.6673 - val_loss: 0.9524 - val_accuracy: 0.9612 - val_precision_m: 0.2321 - val_recall_m: 0.0089 - val_f1_m: 0.0171\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.51620\n",
      "Epoch 49/100\n",
      "32/32 - 1s - loss: 0.5060 - accuracy: 0.9651 - precision_m: 0.6625 - recall_m: 0.7662 - f1_m: 0.6876 - val_loss: 0.9646 - val_accuracy: 0.9610 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.51620\n",
      "Epoch 50/100\n",
      "32/32 - 1s - loss: 0.4808 - accuracy: 0.9703 - precision_m: 0.7295 - recall_m: 0.7615 - f1_m: 0.7325 - val_loss: 0.9539 - val_accuracy: 0.9532 - val_precision_m: 0.1590 - val_recall_m: 0.0094 - val_f1_m: 0.0178\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.51620\n",
      "Epoch 51/100\n",
      "32/32 - 1s - loss: 0.4649 - accuracy: 0.9727 - precision_m: 0.7479 - recall_m: 0.7546 - f1_m: 0.7397 - val_loss: 0.9582 - val_accuracy: 0.9535 - val_precision_m: 0.2194 - val_recall_m: 0.0069 - val_f1_m: 0.0133\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.51620\n",
      "Epoch 52/100\n",
      "32/32 - 1s - loss: 0.4608 - accuracy: 0.9745 - precision_m: 0.7665 - recall_m: 0.7750 - f1_m: 0.7569 - val_loss: 0.9344 - val_accuracy: 0.9567 - val_precision_m: 0.3626 - val_recall_m: 0.0240 - val_f1_m: 0.0451\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.51620\n",
      "Epoch 53/100\n",
      "32/32 - 1s - loss: 0.4409 - accuracy: 0.9769 - precision_m: 0.7828 - recall_m: 0.8076 - f1_m: 0.7796 - val_loss: 0.8910 - val_accuracy: 0.9616 - val_precision_m: 0.4278 - val_recall_m: 0.0597 - val_f1_m: 0.1047\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.51620\n",
      "Epoch 54/100\n",
      "32/32 - 1s - loss: 0.4364 - accuracy: 0.9770 - precision_m: 0.7889 - recall_m: 0.8019 - f1_m: 0.7784 - val_loss: 0.8222 - val_accuracy: 0.9616 - val_precision_m: 0.6437 - val_recall_m: 0.1472 - val_f1_m: 0.2374\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.51620\n",
      "Epoch 55/100\n",
      "32/32 - 1s - loss: 0.4032 - accuracy: 0.9802 - precision_m: 0.8204 - recall_m: 0.8232 - f1_m: 0.8157 - val_loss: 0.8163 - val_accuracy: 0.9633 - val_precision_m: 0.6468 - val_recall_m: 0.1824 - val_f1_m: 0.2798\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.51620\n",
      "Epoch 56/100\n",
      "32/32 - 1s - loss: 0.4282 - accuracy: 0.9771 - precision_m: 0.7951 - recall_m: 0.8043 - f1_m: 0.7857 - val_loss: 0.7953 - val_accuracy: 0.9627 - val_precision_m: 0.6431 - val_recall_m: 0.2069 - val_f1_m: 0.3115\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.51620\n",
      "Epoch 57/100\n",
      "32/32 - 1s - loss: 0.4385 - accuracy: 0.9766 - precision_m: 0.7996 - recall_m: 0.7915 - f1_m: 0.7806 - val_loss: 0.7855 - val_accuracy: 0.9631 - val_precision_m: 0.6467 - val_recall_m: 0.2304 - val_f1_m: 0.3346\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.51620\n",
      "Epoch 58/100\n",
      "32/32 - 1s - loss: 0.4095 - accuracy: 0.9790 - precision_m: 0.8135 - recall_m: 0.8250 - f1_m: 0.8065 - val_loss: 0.7376 - val_accuracy: 0.9600 - val_precision_m: 0.5614 - val_recall_m: 0.3433 - val_f1_m: 0.4236\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.51620\n",
      "Epoch 59/100\n",
      "32/32 - 1s - loss: 0.4142 - accuracy: 0.9788 - precision_m: 0.8112 - recall_m: 0.8076 - f1_m: 0.7988 - val_loss: 0.7238 - val_accuracy: 0.9562 - val_precision_m: 0.5077 - val_recall_m: 0.4040 - val_f1_m: 0.4438\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.51620\n",
      "Epoch 60/100\n",
      "32/32 - 1s - loss: 0.4295 - accuracy: 0.9773 - precision_m: 0.8044 - recall_m: 0.7958 - f1_m: 0.7835 - val_loss: 0.7367 - val_accuracy: 0.9632 - val_precision_m: 0.6289 - val_recall_m: 0.3151 - val_f1_m: 0.4147\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.51620\n",
      "Epoch 61/100\n",
      "32/32 - 1s - loss: 0.3982 - accuracy: 0.9801 - precision_m: 0.8111 - recall_m: 0.8300 - f1_m: 0.8117 - val_loss: 0.7344 - val_accuracy: 0.9631 - val_precision_m: 0.6083 - val_recall_m: 0.3265 - val_f1_m: 0.4189\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.51620\n",
      "Epoch 62/100\n",
      "32/32 - 1s - loss: 0.3826 - accuracy: 0.9823 - precision_m: 0.8567 - recall_m: 0.8216 - f1_m: 0.8335 - val_loss: 0.7286 - val_accuracy: 0.9598 - val_precision_m: 0.5544 - val_recall_m: 0.3591 - val_f1_m: 0.4301\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.51620\n",
      "Epoch 63/100\n",
      "32/32 - 1s - loss: 0.4016 - accuracy: 0.9791 - precision_m: 0.8233 - recall_m: 0.8120 - f1_m: 0.8048 - val_loss: 0.7302 - val_accuracy: 0.9598 - val_precision_m: 0.5437 - val_recall_m: 0.3556 - val_f1_m: 0.4247\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.51620\n",
      "Epoch 64/100\n",
      "32/32 - 1s - loss: 0.3891 - accuracy: 0.9805 - precision_m: 0.8312 - recall_m: 0.8124 - f1_m: 0.8134 - val_loss: 0.7563 - val_accuracy: 0.9642 - val_precision_m: 0.6583 - val_recall_m: 0.2675 - val_f1_m: 0.3740\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.51620\n",
      "Epoch 65/100\n",
      "32/32 - 1s - loss: 0.4036 - accuracy: 0.9782 - precision_m: 0.8031 - recall_m: 0.8030 - f1_m: 0.7908 - val_loss: 0.7232 - val_accuracy: 0.9640 - val_precision_m: 0.6111 - val_recall_m: 0.3394 - val_f1_m: 0.4287\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.51620\n",
      "Epoch 66/100\n",
      "32/32 - 1s - loss: 0.3738 - accuracy: 0.9828 - precision_m: 0.8557 - recall_m: 0.8305 - f1_m: 0.8339 - val_loss: 0.7177 - val_accuracy: 0.9633 - val_precision_m: 0.5943 - val_recall_m: 0.3543 - val_f1_m: 0.4393\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.51620\n",
      "Epoch 67/100\n",
      "32/32 - 1s - loss: 0.3695 - accuracy: 0.9829 - precision_m: 0.8463 - recall_m: 0.8416 - f1_m: 0.8366 - val_loss: 0.7133 - val_accuracy: 0.9585 - val_precision_m: 0.5329 - val_recall_m: 0.3911 - val_f1_m: 0.4456\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.51620\n",
      "Epoch 68/100\n",
      "32/32 - 1s - loss: 0.3659 - accuracy: 0.9832 - precision_m: 0.8546 - recall_m: 0.8399 - f1_m: 0.8412 - val_loss: 0.7297 - val_accuracy: 0.9641 - val_precision_m: 0.6252 - val_recall_m: 0.3149 - val_f1_m: 0.4126\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.51620\n",
      "Epoch 69/100\n",
      "32/32 - 1s - loss: 0.3818 - accuracy: 0.9824 - precision_m: 0.8296 - recall_m: 0.8408 - f1_m: 0.8210 - val_loss: 0.7286 - val_accuracy: 0.9585 - val_precision_m: 0.5256 - val_recall_m: 0.3553 - val_f1_m: 0.4178\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.51620\n",
      "Epoch 70/100\n",
      "32/32 - 1s - loss: 0.3774 - accuracy: 0.9817 - precision_m: 0.8424 - recall_m: 0.8117 - f1_m: 0.8165 - val_loss: 0.7111 - val_accuracy: 0.9517 - val_precision_m: 0.4571 - val_recall_m: 0.4405 - val_f1_m: 0.4394\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.51620\n",
      "Epoch 71/100\n",
      "32/32 - 1s - loss: 0.3762 - accuracy: 0.9811 - precision_m: 0.8330 - recall_m: 0.8233 - f1_m: 0.8169 - val_loss: 0.7086 - val_accuracy: 0.9473 - val_precision_m: 0.4418 - val_recall_m: 0.4567 - val_f1_m: 0.4433\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.51620\n",
      "Epoch 72/100\n",
      "32/32 - 1s - loss: 0.3650 - accuracy: 0.9818 - precision_m: 0.8532 - recall_m: 0.8198 - f1_m: 0.8290 - val_loss: 0.7259 - val_accuracy: 0.9649 - val_precision_m: 0.6526 - val_recall_m: 0.2943 - val_f1_m: 0.4008\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.51620\n",
      "Epoch 73/100\n",
      "32/32 - 1s - loss: 0.3615 - accuracy: 0.9831 - precision_m: 0.8679 - recall_m: 0.8268 - f1_m: 0.8398 - val_loss: 0.7224 - val_accuracy: 0.9532 - val_precision_m: 0.5000 - val_recall_m: 0.3767 - val_f1_m: 0.4185\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.51620\n",
      "Epoch 74/100\n",
      "32/32 - 1s - loss: 0.3615 - accuracy: 0.9838 - precision_m: 0.8535 - recall_m: 0.8399 - f1_m: 0.8390 - val_loss: 0.7180 - val_accuracy: 0.9641 - val_precision_m: 0.6206 - val_recall_m: 0.3162 - val_f1_m: 0.4149\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.51620\n",
      "Epoch 75/100\n",
      "32/32 - 1s - loss: 0.3816 - accuracy: 0.9814 - precision_m: 0.8407 - recall_m: 0.8127 - f1_m: 0.8140 - val_loss: 0.7462 - val_accuracy: 0.9643 - val_precision_m: 0.6578 - val_recall_m: 0.2598 - val_f1_m: 0.3663\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.51620\n",
      "Epoch 76/100\n",
      "32/32 - 1s - loss: 0.3571 - accuracy: 0.9818 - precision_m: 0.8413 - recall_m: 0.8333 - f1_m: 0.8281 - val_loss: 0.7824 - val_accuracy: 0.9636 - val_precision_m: 0.7125 - val_recall_m: 0.1748 - val_f1_m: 0.2789\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.51620\n",
      "Epoch 77/100\n",
      "32/32 - 1s - loss: 0.3522 - accuracy: 0.9831 - precision_m: 0.8625 - recall_m: 0.8291 - f1_m: 0.8386 - val_loss: 0.7269 - val_accuracy: 0.9627 - val_precision_m: 0.6125 - val_recall_m: 0.2867 - val_f1_m: 0.3896\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.51620\n",
      "Epoch 78/100\n",
      "32/32 - 1s - loss: 0.3524 - accuracy: 0.9825 - precision_m: 0.8523 - recall_m: 0.8342 - f1_m: 0.8346 - val_loss: 0.8284 - val_accuracy: 0.9617 - val_precision_m: 0.6516 - val_recall_m: 0.1182 - val_f1_m: 0.1933\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.51620\n",
      "Epoch 79/100\n",
      "32/32 - 1s - loss: 0.3301 - accuracy: 0.9844 - precision_m: 0.8751 - recall_m: 0.8395 - f1_m: 0.8511 - val_loss: 0.7757 - val_accuracy: 0.9627 - val_precision_m: 0.6517 - val_recall_m: 0.1818 - val_f1_m: 0.2800\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.51620\n",
      "Epoch 80/100\n",
      "32/32 - 1s - loss: 0.3259 - accuracy: 0.9851 - precision_m: 0.8942 - recall_m: 0.8430 - f1_m: 0.8624 - val_loss: 0.7866 - val_accuracy: 0.9634 - val_precision_m: 0.7026 - val_recall_m: 0.1659 - val_f1_m: 0.2681\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.51620\n",
      "Epoch 81/100\n",
      "32/32 - 1s - loss: 0.3371 - accuracy: 0.9849 - precision_m: 0.8816 - recall_m: 0.8354 - f1_m: 0.8531 - val_loss: 0.7000 - val_accuracy: 0.9642 - val_precision_m: 0.6431 - val_recall_m: 0.3173 - val_f1_m: 0.4228\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.51620\n",
      "Epoch 82/100\n",
      "32/32 - 1s - loss: 0.3195 - accuracy: 0.9856 - precision_m: 0.8943 - recall_m: 0.8391 - f1_m: 0.8597 - val_loss: 0.7117 - val_accuracy: 0.9651 - val_precision_m: 0.6575 - val_recall_m: 0.3029 - val_f1_m: 0.4069\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.51620\n",
      "Epoch 83/100\n",
      "32/32 - 1s - loss: 0.3325 - accuracy: 0.9856 - precision_m: 0.8759 - recall_m: 0.8446 - f1_m: 0.8521 - val_loss: 0.6967 - val_accuracy: 0.9648 - val_precision_m: 0.6539 - val_recall_m: 0.3162 - val_f1_m: 0.4230\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.51620\n",
      "Epoch 84/100\n",
      "32/32 - 1s - loss: 0.3129 - accuracy: 0.9859 - precision_m: 0.8900 - recall_m: 0.8455 - f1_m: 0.8616 - val_loss: 0.6727 - val_accuracy: 0.9633 - val_precision_m: 0.6127 - val_recall_m: 0.3755 - val_f1_m: 0.4638\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.51620\n",
      "Epoch 85/100\n",
      "32/32 - 1s - loss: 0.3095 - accuracy: 0.9860 - precision_m: 0.8964 - recall_m: 0.8509 - f1_m: 0.8694 - val_loss: 0.7356 - val_accuracy: 0.9640 - val_precision_m: 0.6817 - val_recall_m: 0.2378 - val_f1_m: 0.3511\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.51620\n",
      "Epoch 86/100\n",
      "32/32 - 1s - loss: 0.3166 - accuracy: 0.9858 - precision_m: 0.8850 - recall_m: 0.8490 - f1_m: 0.8623 - val_loss: 0.7503 - val_accuracy: 0.9633 - val_precision_m: 0.6547 - val_recall_m: 0.2076 - val_f1_m: 0.3143\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.51620\n",
      "Epoch 87/100\n",
      "32/32 - 1s - loss: 0.3238 - accuracy: 0.9856 - precision_m: 0.8814 - recall_m: 0.8428 - f1_m: 0.8559 - val_loss: 0.8097 - val_accuracy: 0.9611 - val_precision_m: 0.6097 - val_recall_m: 0.1439 - val_f1_m: 0.2319\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.51620\n",
      "Epoch 88/100\n",
      "32/32 - 1s - loss: 0.3166 - accuracy: 0.9858 - precision_m: 0.8877 - recall_m: 0.8438 - f1_m: 0.8612 - val_loss: 0.6904 - val_accuracy: 0.9635 - val_precision_m: 0.6061 - val_recall_m: 0.3450 - val_f1_m: 0.4366\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.51620\n",
      "Epoch 89/100\n",
      "32/32 - 1s - loss: 0.2985 - accuracy: 0.9871 - precision_m: 0.9004 - recall_m: 0.8656 - f1_m: 0.8784 - val_loss: 0.6975 - val_accuracy: 0.9616 - val_precision_m: 0.5762 - val_recall_m: 0.3415 - val_f1_m: 0.4261\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.51620\n",
      "Epoch 90/100\n",
      "32/32 - 1s - loss: 0.3220 - accuracy: 0.9854 - precision_m: 0.8922 - recall_m: 0.8322 - f1_m: 0.8561 - val_loss: 0.6730 - val_accuracy: 0.9547 - val_precision_m: 0.5053 - val_recall_m: 0.4413 - val_f1_m: 0.4670\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.51620\n",
      "Epoch 91/100\n",
      "32/32 - 1s - loss: 0.3026 - accuracy: 0.9848 - precision_m: 0.8812 - recall_m: 0.8448 - f1_m: 0.8563 - val_loss: 0.6833 - val_accuracy: 0.9644 - val_precision_m: 0.6292 - val_recall_m: 0.3451 - val_f1_m: 0.4398\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.51620\n",
      "Epoch 92/100\n",
      "32/32 - 1s - loss: 0.3002 - accuracy: 0.9856 - precision_m: 0.9024 - recall_m: 0.8372 - f1_m: 0.8637 - val_loss: 0.6749 - val_accuracy: 0.9639 - val_precision_m: 0.6202 - val_recall_m: 0.3619 - val_f1_m: 0.4533\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.51620\n",
      "Epoch 93/100\n",
      "32/32 - 1s - loss: 0.2811 - accuracy: 0.9878 - precision_m: 0.9131 - recall_m: 0.8586 - f1_m: 0.8823 - val_loss: 0.6587 - val_accuracy: 0.9636 - val_precision_m: 0.6136 - val_recall_m: 0.3858 - val_f1_m: 0.4709\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.51620\n",
      "Epoch 94/100\n",
      "32/32 - 1s - loss: 0.2982 - accuracy: 0.9865 - precision_m: 0.9061 - recall_m: 0.8475 - f1_m: 0.8725 - val_loss: 0.6773 - val_accuracy: 0.9647 - val_precision_m: 0.6408 - val_recall_m: 0.3401 - val_f1_m: 0.4402\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.51620\n",
      "Epoch 95/100\n",
      "32/32 - 1s - loss: 0.3007 - accuracy: 0.9860 - precision_m: 0.8957 - recall_m: 0.8529 - f1_m: 0.8672 - val_loss: 0.6650 - val_accuracy: 0.9653 - val_precision_m: 0.6646 - val_recall_m: 0.3433 - val_f1_m: 0.4506\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.51620\n",
      "Epoch 96/100\n",
      "32/32 - 1s - loss: 0.3066 - accuracy: 0.9855 - precision_m: 0.8825 - recall_m: 0.8515 - f1_m: 0.8571 - val_loss: 0.7094 - val_accuracy: 0.9652 - val_precision_m: 0.6949 - val_recall_m: 0.2679 - val_f1_m: 0.3819\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.51620\n",
      "Epoch 97/100\n",
      "32/32 - 1s - loss: 0.2883 - accuracy: 0.9871 - precision_m: 0.9039 - recall_m: 0.8547 - f1_m: 0.8761 - val_loss: 0.7040 - val_accuracy: 0.9631 - val_precision_m: 0.6077 - val_recall_m: 0.2989 - val_f1_m: 0.3948\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.51620\n",
      "Epoch 98/100\n",
      "32/32 - 1s - loss: 0.2843 - accuracy: 0.9872 - precision_m: 0.9161 - recall_m: 0.8542 - f1_m: 0.8806 - val_loss: 0.7063 - val_accuracy: 0.9646 - val_precision_m: 0.6489 - val_recall_m: 0.2817 - val_f1_m: 0.3862\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.51620\n",
      "Epoch 99/100\n",
      "32/32 - 1s - loss: 0.2758 - accuracy: 0.9877 - precision_m: 0.9214 - recall_m: 0.8543 - f1_m: 0.8845 - val_loss: 0.6737 - val_accuracy: 0.9649 - val_precision_m: 0.6488 - val_recall_m: 0.3252 - val_f1_m: 0.4289\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.51620\n",
      "Epoch 100/100\n",
      "32/32 - 1s - loss: 0.2825 - accuracy: 0.9857 - precision_m: 0.8930 - recall_m: 0.8531 - f1_m: 0.8661 - val_loss: 0.6617 - val_accuracy: 0.9651 - val_precision_m: 0.6331 - val_recall_m: 0.3550 - val_f1_m: 0.4502\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.51620\n",
      "5/5 - 0s - loss: 0.4911 - accuracy: 0.9588 - precision_m: 0.7268 - recall_m: 0.4502 - f1_m: 0.5528\n",
      "The metrics for the test set with loss tversky, learning rate 0.0001,  filters 16, and batch size 8 is [0.49111926555633545, 0.9587917327880859, 0.7268065810203552, 0.45018553733825684, 0.5527887344360352]\n",
      "__________________________________________________________________________________________________________________________________________________________________________\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 128, 128, 5) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 128, 128, 32) 1472        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 128, 128, 32) 128         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 128, 128, 32) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 32) 9248        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 32) 128         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, 128, 32) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 64, 64, 32)   0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 64)   18496       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64, 64, 64)   256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 64, 64, 64)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 64, 64)   36928       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 64)   256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64, 64, 64)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 64)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 128)  73856       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 128)  512         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 128)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 128)  147584      activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 128)  512         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 128)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 128)  0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 16, 16, 256)  295168      max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 16, 16, 256)  1024        conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 16, 16, 256)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 16, 16, 256)  590080      activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 16, 16, 256)  1024        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16, 16, 256)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 256)    0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 8, 8, 512)    1180160     max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 8, 8, 512)    2048        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 8, 8, 512)    0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 8, 8, 512)    2359808     activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 8, 8, 512)    2048        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 8, 8, 512)    0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 16, 16, 128)  262272      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 16, 16, 384)  0           conv2d_transpose[0][0]           \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 256)  884992      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 256)  1024        conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 256)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 256)  590080      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 256)  1024        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 256)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 32, 32, 64)   65600       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 192)  0           conv2d_transpose_1[0][0]         \n",
      "                                                                 activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 128)  221312      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 32, 32, 128)  512         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 32, 32, 128)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 32, 128)  147584      activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 32, 32, 128)  512         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 32, 32, 128)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 64, 64, 32)   16416       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 64, 64, 96)   0           conv2d_transpose_2[0][0]         \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 64)   55360       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64, 64, 64)   256         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 64, 64, 64)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 64)   36928       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 64, 64, 64)   256         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 64, 64, 64)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 128, 128, 16) 4112        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 128, 128, 48) 0           conv2d_transpose_3[0][0]         \n",
      "                                                                 activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 128, 128, 32) 13856       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 128, 128, 32) 128         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 128, 128, 32) 0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 128, 128, 32) 9248        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 128, 128, 32) 128         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 128, 128, 32) 0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 128, 128, 1)  33          activation_17[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 7,032,369\n",
      "Trainable params: 7,026,481\n",
      "Non-trainable params: 5,888\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "Initialising in 3..........2..........1......................................\n",
      "Training on 284 images with learning rate 0.0001, batch size 8, and number of filters 32\n",
      "Epoch 1/100\n",
      "32/32 - 3s - loss: 0.9201 - accuracy: 0.5609 - precision_m: 0.0531 - recall_m: 0.5130 - f1_m: 0.0879 - val_loss: 0.9417 - val_accuracy: 0.5933 - val_precision_m: 0.0485 - val_recall_m: 0.5562 - val_f1_m: 0.0875\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.51620\n",
      "Epoch 2/100\n",
      "32/32 - 1s - loss: 0.9076 - accuracy: 0.4012 - precision_m: 0.0766 - recall_m: 0.8600 - f1_m: 0.1386 - val_loss: 0.9398 - val_accuracy: 0.4053 - val_precision_m: 0.0512 - val_recall_m: 0.8190 - val_f1_m: 0.0955\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.51620\n",
      "Epoch 3/100\n",
      "32/32 - 1s - loss: 0.8950 - accuracy: 0.4672 - precision_m: 0.0840 - recall_m: 0.8363 - f1_m: 0.1494 - val_loss: 0.9403 - val_accuracy: 0.2256 - val_precision_m: 0.0458 - val_recall_m: 0.9646 - val_f1_m: 0.0870\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.51620\n",
      "Epoch 4/100\n",
      "32/32 - 1s - loss: 0.8884 - accuracy: 0.5637 - precision_m: 0.0889 - recall_m: 0.7266 - f1_m: 0.1555 - val_loss: 0.9353 - val_accuracy: 0.3098 - val_precision_m: 0.0506 - val_recall_m: 0.9347 - val_f1_m: 0.0955\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.51620\n",
      "Epoch 5/100\n",
      "32/32 - 1s - loss: 0.8725 - accuracy: 0.6544 - precision_m: 0.1077 - recall_m: 0.6719 - f1_m: 0.1806 - val_loss: 0.9411 - val_accuracy: 0.1197 - val_precision_m: 0.0414 - val_recall_m: 0.9887 - val_f1_m: 0.0792\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.51620\n",
      "Epoch 6/100\n",
      "32/32 - 1s - loss: 0.8423 - accuracy: 0.7110 - precision_m: 0.1362 - recall_m: 0.7511 - f1_m: 0.2248 - val_loss: 0.9417 - val_accuracy: 0.1058 - val_precision_m: 0.0409 - val_recall_m: 0.9917 - val_f1_m: 0.0783\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.51620\n",
      "Epoch 7/100\n",
      "32/32 - 1s - loss: 0.8252 - accuracy: 0.7938 - precision_m: 0.1654 - recall_m: 0.6540 - f1_m: 0.2587 - val_loss: 0.9418 - val_accuracy: 0.1076 - val_precision_m: 0.0411 - val_recall_m: 0.9916 - val_f1_m: 0.0786\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.51620\n",
      "Epoch 8/100\n",
      "32/32 - 1s - loss: 0.8299 - accuracy: 0.7943 - precision_m: 0.1613 - recall_m: 0.6038 - f1_m: 0.2449 - val_loss: 0.9364 - val_accuracy: 0.2098 - val_precision_m: 0.0466 - val_recall_m: 0.9874 - val_f1_m: 0.0885\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.51620\n",
      "Epoch 9/100\n",
      "32/32 - 1s - loss: 0.8283 - accuracy: 0.7721 - precision_m: 0.1492 - recall_m: 0.6531 - f1_m: 0.2382 - val_loss: 0.9374 - val_accuracy: 0.1801 - val_precision_m: 0.0449 - val_recall_m: 0.9878 - val_f1_m: 0.0854\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.51620\n",
      "Epoch 10/100\n",
      "32/32 - 2s - loss: 0.8037 - accuracy: 0.8121 - precision_m: 0.1804 - recall_m: 0.6709 - f1_m: 0.2782 - val_loss: 0.9313 - val_accuracy: 0.2619 - val_precision_m: 0.0498 - val_recall_m: 0.9855 - val_f1_m: 0.0942\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.51620\n",
      "Epoch 11/100\n",
      "32/32 - 1s - loss: 0.7969 - accuracy: 0.8214 - precision_m: 0.1971 - recall_m: 0.6774 - f1_m: 0.2923 - val_loss: 0.9168 - val_accuracy: 0.4418 - val_precision_m: 0.0672 - val_recall_m: 0.9553 - val_f1_m: 0.1246\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.51620\n",
      "Epoch 12/100\n",
      "32/32 - 1s - loss: 0.7819 - accuracy: 0.8343 - precision_m: 0.2073 - recall_m: 0.6862 - f1_m: 0.3094 - val_loss: 0.9129 - val_accuracy: 0.4603 - val_precision_m: 0.0673 - val_recall_m: 0.9380 - val_f1_m: 0.1244\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.51620\n",
      "Epoch 13/100\n",
      "32/32 - 1s - loss: 0.7889 - accuracy: 0.8222 - precision_m: 0.1945 - recall_m: 0.6872 - f1_m: 0.2934 - val_loss: 0.9177 - val_accuracy: 0.4224 - val_precision_m: 0.0628 - val_recall_m: 0.9438 - val_f1_m: 0.1168\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.51620\n",
      "Epoch 14/100\n",
      "32/32 - 1s - loss: 0.7838 - accuracy: 0.8327 - precision_m: 0.1989 - recall_m: 0.6559 - f1_m: 0.2948 - val_loss: 0.8933 - val_accuracy: 0.6319 - val_precision_m: 0.0871 - val_recall_m: 0.7763 - val_f1_m: 0.1548\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.51620\n",
      "Epoch 15/100\n",
      "32/32 - 1s - loss: 0.7638 - accuracy: 0.8529 - precision_m: 0.2308 - recall_m: 0.6632 - f1_m: 0.3321 - val_loss: 0.8696 - val_accuracy: 0.7552 - val_precision_m: 0.1246 - val_recall_m: 0.6350 - val_f1_m: 0.2013\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.51620\n",
      "Epoch 16/100\n",
      "32/32 - 1s - loss: 0.7877 - accuracy: 0.8424 - precision_m: 0.2017 - recall_m: 0.5973 - f1_m: 0.2906 - val_loss: 0.8928 - val_accuracy: 0.6443 - val_precision_m: 0.0857 - val_recall_m: 0.7914 - val_f1_m: 0.1534\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.51620\n",
      "Epoch 17/100\n",
      "32/32 - 1s - loss: 0.7743 - accuracy: 0.8613 - precision_m: 0.2257 - recall_m: 0.5920 - f1_m: 0.3097 - val_loss: 0.8376 - val_accuracy: 0.8328 - val_precision_m: 0.1813 - val_recall_m: 0.5916 - val_f1_m: 0.2680\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.51620\n",
      "Epoch 18/100\n",
      "32/32 - 1s - loss: 0.7450 - accuracy: 0.8710 - precision_m: 0.2552 - recall_m: 0.6460 - f1_m: 0.3504 - val_loss: 0.9163 - val_accuracy: 0.9225 - val_precision_m: 0.0768 - val_recall_m: 0.0720 - val_f1_m: 0.0743\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.51620\n",
      "Epoch 19/100\n",
      "32/32 - 1s - loss: 0.7556 - accuracy: 0.8543 - precision_m: 0.2289 - recall_m: 0.6564 - f1_m: 0.3283 - val_loss: 0.8499 - val_accuracy: 0.7965 - val_precision_m: 0.1485 - val_recall_m: 0.5806 - val_f1_m: 0.2266\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.51620\n",
      "Epoch 20/100\n",
      "32/32 - 1s - loss: 0.7585 - accuracy: 0.8581 - precision_m: 0.2281 - recall_m: 0.6441 - f1_m: 0.3252 - val_loss: 0.9640 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.51620\n",
      "Epoch 21/100\n",
      "32/32 - 1s - loss: 0.7238 - accuracy: 0.8872 - precision_m: 0.2805 - recall_m: 0.6232 - f1_m: 0.3717 - val_loss: 0.9349 - val_accuracy: 0.9429 - val_precision_m: 0.1361 - val_recall_m: 0.0292 - val_f1_m: 0.0481\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.51620\n",
      "Epoch 22/100\n",
      "32/32 - 1s - loss: 0.7145 - accuracy: 0.9037 - precision_m: 0.3075 - recall_m: 0.5836 - f1_m: 0.3841 - val_loss: 0.8460 - val_accuracy: 0.9554 - val_precision_m: 0.4462 - val_recall_m: 0.1540 - val_f1_m: 0.2180\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.51620\n",
      "Epoch 23/100\n",
      "32/32 - 1s - loss: 0.6988 - accuracy: 0.9021 - precision_m: 0.3081 - recall_m: 0.6094 - f1_m: 0.3992 - val_loss: 0.9691 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.51620\n",
      "Epoch 24/100\n",
      "32/32 - 1s - loss: 0.7286 - accuracy: 0.9064 - precision_m: 0.2949 - recall_m: 0.4953 - f1_m: 0.3500 - val_loss: 0.9714 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.51620\n",
      "Epoch 25/100\n",
      "32/32 - 1s - loss: 0.6890 - accuracy: 0.9128 - precision_m: 0.3429 - recall_m: 0.5564 - f1_m: 0.4058 - val_loss: 0.9713 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.51620\n",
      "Epoch 26/100\n",
      "32/32 - 1s - loss: 0.7142 - accuracy: 0.9047 - precision_m: 0.2978 - recall_m: 0.5248 - f1_m: 0.3640 - val_loss: 0.9732 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.51620\n",
      "Epoch 27/100\n",
      "32/32 - 1s - loss: 0.6389 - accuracy: 0.9237 - precision_m: 0.3959 - recall_m: 0.6132 - f1_m: 0.4660 - val_loss: 0.9703 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.51620\n",
      "Epoch 28/100\n",
      "32/32 - 1s - loss: 0.6449 - accuracy: 0.9230 - precision_m: 0.3908 - recall_m: 0.6139 - f1_m: 0.4569 - val_loss: 0.9715 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.51620\n",
      "Epoch 29/100\n",
      "32/32 - 1s - loss: 0.5976 - accuracy: 0.9439 - precision_m: 0.5022 - recall_m: 0.5662 - f1_m: 0.5083 - val_loss: 0.9753 - val_accuracy: 0.9565 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.51620\n",
      "Epoch 30/100\n",
      "32/32 - 1s - loss: 0.5729 - accuracy: 0.9450 - precision_m: 0.4977 - recall_m: 0.5994 - f1_m: 0.5295 - val_loss: 0.9744 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.51620\n",
      "Epoch 31/100\n",
      "32/32 - 1s - loss: 0.5807 - accuracy: 0.9427 - precision_m: 0.4863 - recall_m: 0.5946 - f1_m: 0.5099 - val_loss: 0.9781 - val_accuracy: 0.9521 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.51620\n",
      "Epoch 32/100\n",
      "32/32 - 1s - loss: 0.5863 - accuracy: 0.9397 - precision_m: 0.4709 - recall_m: 0.5864 - f1_m: 0.4923 - val_loss: 0.9770 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.51620\n",
      "Epoch 33/100\n",
      "32/32 - 1s - loss: 0.5093 - accuracy: 0.9542 - precision_m: 0.5791 - recall_m: 0.6291 - f1_m: 0.5845 - val_loss: 0.9762 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.51620\n",
      "Epoch 34/100\n",
      "32/32 - 1s - loss: 0.5075 - accuracy: 0.9584 - precision_m: 0.6240 - recall_m: 0.6032 - f1_m: 0.5907 - val_loss: 0.9774 - val_accuracy: 0.9607 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.51620\n",
      "Epoch 35/100\n",
      "32/32 - 1s - loss: 0.4821 - accuracy: 0.9597 - precision_m: 0.6291 - recall_m: 0.6334 - f1_m: 0.6135 - val_loss: 0.9791 - val_accuracy: 0.9552 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.51620\n",
      "Epoch 36/100\n",
      "32/32 - 1s - loss: 0.4744 - accuracy: 0.9610 - precision_m: 0.6456 - recall_m: 0.6408 - f1_m: 0.6216 - val_loss: 0.9790 - val_accuracy: 0.9557 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.51620\n",
      "Epoch 37/100\n",
      "32/32 - 1s - loss: 0.4546 - accuracy: 0.9638 - precision_m: 0.6853 - recall_m: 0.6481 - f1_m: 0.6476 - val_loss: 0.9792 - val_accuracy: 0.9541 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.51620\n",
      "Epoch 38/100\n",
      "32/32 - 2s - loss: 0.4598 - accuracy: 0.9627 - precision_m: 0.6797 - recall_m: 0.6106 - f1_m: 0.6264 - val_loss: 0.6951 - val_accuracy: 0.9536 - val_precision_m: 0.4834 - val_recall_m: 0.3385 - val_f1_m: 0.3788\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.51620\n",
      "Epoch 39/100\n",
      "32/32 - 1s - loss: 0.4278 - accuracy: 0.9660 - precision_m: 0.7081 - recall_m: 0.6576 - f1_m: 0.6698 - val_loss: 0.8742 - val_accuracy: 0.9590 - val_precision_m: 0.4265 - val_recall_m: 0.0818 - val_f1_m: 0.1068\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.51620\n",
      "Epoch 40/100\n",
      "32/32 - 1s - loss: 0.4291 - accuracy: 0.9664 - precision_m: 0.7392 - recall_m: 0.6160 - f1_m: 0.6549 - val_loss: 0.9737 - val_accuracy: 0.9606 - val_precision_m: 0.0117 - val_recall_m: 2.3923e-04 - val_f1_m: 4.6892e-04\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.51620\n",
      "Epoch 41/100\n",
      "32/32 - 1s - loss: 0.4347 - accuracy: 0.9644 - precision_m: 0.6773 - recall_m: 0.6611 - f1_m: 0.6549 - val_loss: 0.9463 - val_accuracy: 0.9598 - val_precision_m: 0.0833 - val_recall_m: 0.0161 - val_f1_m: 0.0269\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.51620\n",
      "Epoch 42/100\n",
      "32/32 - 1s - loss: 0.4131 - accuracy: 0.9681 - precision_m: 0.7373 - recall_m: 0.6521 - f1_m: 0.6771 - val_loss: 0.8883 - val_accuracy: 0.9617 - val_precision_m: 0.4324 - val_recall_m: 0.0500 - val_f1_m: 0.0821\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.51620\n",
      "Epoch 43/100\n",
      "32/32 - 1s - loss: 0.3935 - accuracy: 0.9689 - precision_m: 0.7527 - recall_m: 0.6566 - f1_m: 0.6890 - val_loss: 0.6734 - val_accuracy: 0.9424 - val_precision_m: 0.3741 - val_recall_m: 0.5211 - val_f1_m: 0.4338\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.51620\n",
      "Epoch 44/100\n",
      "32/32 - 1s - loss: 0.3870 - accuracy: 0.9696 - precision_m: 0.7575 - recall_m: 0.6755 - f1_m: 0.7003 - val_loss: 0.6725 - val_accuracy: 0.9583 - val_precision_m: 0.5414 - val_recall_m: 0.3161 - val_f1_m: 0.3921\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.51620\n",
      "Epoch 45/100\n",
      "32/32 - 1s - loss: 0.3742 - accuracy: 0.9708 - precision_m: 0.7808 - recall_m: 0.6608 - f1_m: 0.7060 - val_loss: 0.7383 - val_accuracy: 0.9623 - val_precision_m: 0.6433 - val_recall_m: 0.1842 - val_f1_m: 0.2677\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.51620\n",
      "Epoch 46/100\n",
      "32/32 - 1s - loss: 0.3617 - accuracy: 0.9714 - precision_m: 0.7911 - recall_m: 0.6827 - f1_m: 0.7210 - val_loss: 0.7161 - val_accuracy: 0.9581 - val_precision_m: 0.5651 - val_recall_m: 0.2351 - val_f1_m: 0.2894\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.51620\n",
      "Epoch 47/100\n",
      "32/32 - 1s - loss: 0.3662 - accuracy: 0.9712 - precision_m: 0.7676 - recall_m: 0.7057 - f1_m: 0.7212 - val_loss: 0.6745 - val_accuracy: 0.9623 - val_precision_m: 0.5932 - val_recall_m: 0.2869 - val_f1_m: 0.3834\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.51620\n",
      "Epoch 48/100\n",
      "32/32 - 1s - loss: 0.3586 - accuracy: 0.9728 - precision_m: 0.7997 - recall_m: 0.6937 - f1_m: 0.7281 - val_loss: 0.6953 - val_accuracy: 0.9625 - val_precision_m: 0.5921 - val_recall_m: 0.2584 - val_f1_m: 0.3580\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.51620\n",
      "Epoch 49/100\n",
      "32/32 - 1s - loss: 0.3378 - accuracy: 0.9742 - precision_m: 0.8146 - recall_m: 0.7009 - f1_m: 0.7439 - val_loss: 0.6170 - val_accuracy: 0.9597 - val_precision_m: 0.5090 - val_recall_m: 0.4429 - val_f1_m: 0.4707\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.51620\n",
      "Epoch 50/100\n",
      "32/32 - 1s - loss: 0.3346 - accuracy: 0.9733 - precision_m: 0.8129 - recall_m: 0.6996 - f1_m: 0.7410 - val_loss: 0.8005 - val_accuracy: 0.9615 - val_precision_m: 0.6834 - val_recall_m: 0.1074 - val_f1_m: 0.1765\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.51620\n",
      "Epoch 51/100\n",
      "32/32 - 1s - loss: 0.3327 - accuracy: 0.9743 - precision_m: 0.8120 - recall_m: 0.6914 - f1_m: 0.7386 - val_loss: 0.8035 - val_accuracy: 0.9587 - val_precision_m: 0.5533 - val_recall_m: 0.1182 - val_f1_m: 0.1866\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.51620\n",
      "Epoch 52/100\n",
      "32/32 - 1s - loss: 0.3681 - accuracy: 0.9709 - precision_m: 0.7537 - recall_m: 0.6905 - f1_m: 0.7042 - val_loss: 0.6570 - val_accuracy: 0.9639 - val_precision_m: 0.6314 - val_recall_m: 0.2799 - val_f1_m: 0.3861\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.51620\n",
      "Epoch 53/100\n",
      "32/32 - 1s - loss: 0.3152 - accuracy: 0.9755 - precision_m: 0.8344 - recall_m: 0.7065 - f1_m: 0.7563 - val_loss: 0.7675 - val_accuracy: 0.9617 - val_precision_m: 0.6383 - val_recall_m: 0.1393 - val_f1_m: 0.2173\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.51620\n",
      "Epoch 54/100\n",
      "32/32 - 1s - loss: 0.3320 - accuracy: 0.9750 - precision_m: 0.8192 - recall_m: 0.6985 - f1_m: 0.7418 - val_loss: 0.8039 - val_accuracy: 0.9633 - val_precision_m: 0.7635 - val_recall_m: 0.0967 - val_f1_m: 0.1667\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.51620\n",
      "Epoch 55/100\n",
      "32/32 - 1s - loss: 0.3278 - accuracy: 0.9743 - precision_m: 0.8300 - recall_m: 0.6702 - f1_m: 0.7323 - val_loss: 0.6128 - val_accuracy: 0.9597 - val_precision_m: 0.5335 - val_recall_m: 0.4183 - val_f1_m: 0.4671\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.51620\n",
      "Epoch 56/100\n",
      "32/32 - 2s - loss: 0.3348 - accuracy: 0.9742 - precision_m: 0.8182 - recall_m: 0.6865 - f1_m: 0.7348 - val_loss: 0.7757 - val_accuracy: 0.9633 - val_precision_m: 0.7683 - val_recall_m: 0.1149 - val_f1_m: 0.1949\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.51620\n",
      "Epoch 57/100\n",
      "32/32 - 1s - loss: 0.2791 - accuracy: 0.9778 - precision_m: 0.8681 - recall_m: 0.7240 - f1_m: 0.7825 - val_loss: 0.9534 - val_accuracy: 0.9611 - val_precision_m: 0.4675 - val_recall_m: 0.0120 - val_f1_m: 0.0233\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.51620\n",
      "Epoch 58/100\n",
      "32/32 - 1s - loss: 0.2960 - accuracy: 0.9769 - precision_m: 0.8468 - recall_m: 0.7141 - f1_m: 0.7660 - val_loss: 0.8009 - val_accuracy: 0.9622 - val_precision_m: 0.7150 - val_recall_m: 0.1035 - val_f1_m: 0.1671\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.51620\n",
      "Epoch 59/100\n",
      "32/32 - 1s - loss: 0.3004 - accuracy: 0.9773 - precision_m: 0.8479 - recall_m: 0.7045 - f1_m: 0.7617 - val_loss: 0.9620 - val_accuracy: 0.9552 - val_precision_m: 0.3455 - val_recall_m: 0.0099 - val_f1_m: 0.0191\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.51620\n",
      "Epoch 60/100\n",
      "32/32 - 1s - loss: 0.2732 - accuracy: 0.9783 - precision_m: 0.8500 - recall_m: 0.7412 - f1_m: 0.7857 - val_loss: 0.6419 - val_accuracy: 0.9631 - val_precision_m: 0.6815 - val_recall_m: 0.2922 - val_f1_m: 0.3963\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.51620\n",
      "Epoch 61/100\n",
      "32/32 - 1s - loss: 0.3154 - accuracy: 0.9747 - precision_m: 0.8462 - recall_m: 0.6721 - f1_m: 0.7417 - val_loss: 0.6349 - val_accuracy: 0.9648 - val_precision_m: 0.6107 - val_recall_m: 0.3301 - val_f1_m: 0.4125\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.51620\n",
      "Epoch 62/100\n",
      "32/32 - 1s - loss: 0.2724 - accuracy: 0.9780 - precision_m: 0.8674 - recall_m: 0.7192 - f1_m: 0.7798 - val_loss: 0.8600 - val_accuracy: 0.9632 - val_precision_m: 0.7014 - val_recall_m: 0.0522 - val_f1_m: 0.0941\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.51620\n",
      "Epoch 63/100\n",
      "32/32 - 1s - loss: 0.2925 - accuracy: 0.9773 - precision_m: 0.8385 - recall_m: 0.7226 - f1_m: 0.7632 - val_loss: 0.8623 - val_accuracy: 0.9536 - val_precision_m: 0.3568 - val_recall_m: 0.0648 - val_f1_m: 0.1057\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.51620\n",
      "Epoch 64/100\n",
      "32/32 - 1s - loss: 0.3027 - accuracy: 0.9754 - precision_m: 0.8202 - recall_m: 0.7106 - f1_m: 0.7509 - val_loss: 0.5993 - val_accuracy: 0.9599 - val_precision_m: 0.5286 - val_recall_m: 0.4260 - val_f1_m: 0.4698\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.51620\n",
      "Epoch 65/100\n",
      "32/32 - 1s - loss: 0.2708 - accuracy: 0.9792 - precision_m: 0.8727 - recall_m: 0.7250 - f1_m: 0.7864 - val_loss: 0.5709 - val_accuracy: 0.9664 - val_precision_m: 0.6446 - val_recall_m: 0.3861 - val_f1_m: 0.4780\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.51620\n",
      "Epoch 66/100\n",
      "32/32 - 1s - loss: 0.2852 - accuracy: 0.9777 - precision_m: 0.8557 - recall_m: 0.7097 - f1_m: 0.7677 - val_loss: 0.9065 - val_accuracy: 0.9568 - val_precision_m: 0.3806 - val_recall_m: 0.0355 - val_f1_m: 0.0645\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.51620\n",
      "Epoch 67/100\n",
      "32/32 - 1s - loss: 0.2716 - accuracy: 0.9777 - precision_m: 0.8589 - recall_m: 0.7077 - f1_m: 0.7709 - val_loss: 0.7974 - val_accuracy: 0.9605 - val_precision_m: 0.6638 - val_recall_m: 0.1057 - val_f1_m: 0.1702\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.51620\n",
      "Epoch 68/100\n",
      "32/32 - 1s - loss: 0.2494 - accuracy: 0.9810 - precision_m: 0.8786 - recall_m: 0.7586 - f1_m: 0.8069 - val_loss: 0.6919 - val_accuracy: 0.9632 - val_precision_m: 0.7039 - val_recall_m: 0.1866 - val_f1_m: 0.2814\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.51620\n",
      "Epoch 69/100\n",
      "32/32 - 1s - loss: 0.2520 - accuracy: 0.9800 - precision_m: 0.8841 - recall_m: 0.7320 - f1_m: 0.7960 - val_loss: 0.7071 - val_accuracy: 0.9640 - val_precision_m: 0.7439 - val_recall_m: 0.1782 - val_f1_m: 0.2842\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.51620\n",
      "Epoch 70/100\n",
      "32/32 - 1s - loss: 0.2463 - accuracy: 0.9805 - precision_m: 0.8835 - recall_m: 0.7447 - f1_m: 0.8027 - val_loss: 0.5782 - val_accuracy: 0.9684 - val_precision_m: 0.7758 - val_recall_m: 0.3201 - val_f1_m: 0.4413\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.51620\n",
      "Epoch 71/100\n",
      "32/32 - 1s - loss: 0.2295 - accuracy: 0.9822 - precision_m: 0.9040 - recall_m: 0.7508 - f1_m: 0.8166 - val_loss: 0.8221 - val_accuracy: 0.9628 - val_precision_m: 0.8150 - val_recall_m: 0.0761 - val_f1_m: 0.1332\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.51620\n",
      "Epoch 72/100\n",
      "32/32 - 1s - loss: 0.2487 - accuracy: 0.9805 - precision_m: 0.8726 - recall_m: 0.7564 - f1_m: 0.8029 - val_loss: 0.7130 - val_accuracy: 0.9650 - val_precision_m: 0.7764 - val_recall_m: 0.1764 - val_f1_m: 0.2814\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.51620\n",
      "Epoch 73/100\n",
      "32/32 - 1s - loss: 0.2314 - accuracy: 0.9813 - precision_m: 0.9022 - recall_m: 0.7468 - f1_m: 0.8121 - val_loss: 0.5825 - val_accuracy: 0.9670 - val_precision_m: 0.6976 - val_recall_m: 0.3249 - val_f1_m: 0.4373\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.51620\n",
      "Epoch 74/100\n",
      "32/32 - 1s - loss: 0.2325 - accuracy: 0.9813 - precision_m: 0.8862 - recall_m: 0.7553 - f1_m: 0.8119 - val_loss: 0.6874 - val_accuracy: 0.9649 - val_precision_m: 0.7502 - val_recall_m: 0.1929 - val_f1_m: 0.3004\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.51620\n",
      "Epoch 75/100\n",
      "32/32 - 1s - loss: 0.2313 - accuracy: 0.9812 - precision_m: 0.9018 - recall_m: 0.7404 - f1_m: 0.8087 - val_loss: 0.5648 - val_accuracy: 0.9674 - val_precision_m: 0.7176 - val_recall_m: 0.3356 - val_f1_m: 0.4536\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.51620\n",
      "Epoch 76/100\n",
      "32/32 - 1s - loss: 0.2149 - accuracy: 0.9830 - precision_m: 0.9033 - recall_m: 0.7674 - f1_m: 0.8270 - val_loss: 0.9535 - val_accuracy: 0.9542 - val_precision_m: 0.2947 - val_recall_m: 0.0136 - val_f1_m: 0.0259\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.51620\n",
      "Epoch 77/100\n",
      "32/32 - 1s - loss: 0.2082 - accuracy: 0.9829 - precision_m: 0.9123 - recall_m: 0.7689 - f1_m: 0.8310 - val_loss: 0.9788 - val_accuracy: 0.9553 - val_precision_m: 0.1678 - val_recall_m: 0.0033 - val_f1_m: 0.0064\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.51620\n",
      "Epoch 78/100\n",
      "32/32 - 1s - loss: 0.2139 - accuracy: 0.9836 - precision_m: 0.9056 - recall_m: 0.7671 - f1_m: 0.8271 - val_loss: 0.8575 - val_accuracy: 0.9624 - val_precision_m: 0.5152 - val_recall_m: 0.0580 - val_f1_m: 0.1006\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.51620\n",
      "Epoch 79/100\n",
      "32/32 - 1s - loss: 0.1909 - accuracy: 0.9842 - precision_m: 0.9282 - recall_m: 0.7785 - f1_m: 0.8444 - val_loss: 0.6620 - val_accuracy: 0.9658 - val_precision_m: 0.7667 - val_recall_m: 0.1977 - val_f1_m: 0.3121\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.51620\n",
      "Epoch 80/100\n",
      "32/32 - 2s - loss: 0.1852 - accuracy: 0.9845 - precision_m: 0.9272 - recall_m: 0.7872 - f1_m: 0.8496 - val_loss: 0.5550 - val_accuracy: 0.9685 - val_precision_m: 0.7365 - val_recall_m: 0.3595 - val_f1_m: 0.4680\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.51620\n",
      "Epoch 81/100\n",
      "32/32 - 1s - loss: 0.2148 - accuracy: 0.9818 - precision_m: 0.9109 - recall_m: 0.7500 - f1_m: 0.8180 - val_loss: 0.5835 - val_accuracy: 0.9682 - val_precision_m: 0.8053 - val_recall_m: 0.2785 - val_f1_m: 0.4087\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.51620\n",
      "Epoch 82/100\n",
      "32/32 - 1s - loss: 0.2073 - accuracy: 0.9826 - precision_m: 0.9097 - recall_m: 0.7642 - f1_m: 0.8270 - val_loss: 0.8365 - val_accuracy: 0.9618 - val_precision_m: 0.6838 - val_recall_m: 0.0687 - val_f1_m: 0.1198\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.51620\n",
      "Epoch 83/100\n",
      "32/32 - 1s - loss: 0.1946 - accuracy: 0.9838 - precision_m: 0.9220 - recall_m: 0.7685 - f1_m: 0.8345 - val_loss: 0.5395 - val_accuracy: 0.9668 - val_precision_m: 0.6416 - val_recall_m: 0.4180 - val_f1_m: 0.4983\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.51620\n",
      "Epoch 84/100\n",
      "32/32 - 1s - loss: 0.1836 - accuracy: 0.9853 - precision_m: 0.9355 - recall_m: 0.7881 - f1_m: 0.8531 - val_loss: 0.5451 - val_accuracy: 0.9685 - val_precision_m: 0.7670 - val_recall_m: 0.3346 - val_f1_m: 0.4592\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.51620\n",
      "Epoch 85/100\n",
      "32/32 - 1s - loss: 0.1988 - accuracy: 0.9836 - precision_m: 0.9215 - recall_m: 0.7638 - f1_m: 0.8308 - val_loss: 0.5609 - val_accuracy: 0.9600 - val_precision_m: 0.5202 - val_recall_m: 0.4840 - val_f1_m: 0.4999\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.51620\n",
      "Epoch 86/100\n",
      "32/32 - 1s - loss: 0.2157 - accuracy: 0.9817 - precision_m: 0.8981 - recall_m: 0.7602 - f1_m: 0.8181 - val_loss: 0.5973 - val_accuracy: 0.9676 - val_precision_m: 0.7963 - val_recall_m: 0.2544 - val_f1_m: 0.3817\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.51620\n",
      "Epoch 87/100\n",
      "32/32 - 1s - loss: 0.1869 - accuracy: 0.9851 - precision_m: 0.9377 - recall_m: 0.7766 - f1_m: 0.8461 - val_loss: 0.6917 - val_accuracy: 0.9650 - val_precision_m: 0.8175 - val_recall_m: 0.1577 - val_f1_m: 0.2615\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.51620\n",
      "Epoch 88/100\n",
      "32/32 - 1s - loss: 0.1821 - accuracy: 0.9842 - precision_m: 0.9233 - recall_m: 0.7883 - f1_m: 0.8462 - val_loss: 0.8115 - val_accuracy: 0.9628 - val_precision_m: 0.7543 - val_recall_m: 0.0782 - val_f1_m: 0.1374\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.51620\n",
      "Epoch 89/100\n",
      "32/32 - 1s - loss: 0.1980 - accuracy: 0.9829 - precision_m: 0.9069 - recall_m: 0.7749 - f1_m: 0.8310 - val_loss: 0.9811 - val_accuracy: 0.9527 - val_precision_m: 0.2500 - val_recall_m: 0.0033 - val_f1_m: 0.0066\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.51620\n",
      "Epoch 90/100\n",
      "32/32 - 1s - loss: 0.1863 - accuracy: 0.9846 - precision_m: 0.9286 - recall_m: 0.7735 - f1_m: 0.8406 - val_loss: 0.9369 - val_accuracy: 0.9577 - val_precision_m: 0.3038 - val_recall_m: 0.0199 - val_f1_m: 0.0365\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.51620\n",
      "Epoch 91/100\n",
      "32/32 - 1s - loss: 0.1886 - accuracy: 0.9842 - precision_m: 0.9283 - recall_m: 0.7765 - f1_m: 0.8412 - val_loss: 0.7436 - val_accuracy: 0.9637 - val_precision_m: 0.7496 - val_recall_m: 0.1226 - val_f1_m: 0.2099\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.51620\n",
      "Epoch 92/100\n",
      "32/32 - 1s - loss: 0.1751 - accuracy: 0.9846 - precision_m: 0.9289 - recall_m: 0.7843 - f1_m: 0.8470 - val_loss: 0.8052 - val_accuracy: 0.9623 - val_precision_m: 0.7328 - val_recall_m: 0.0824 - val_f1_m: 0.1462\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.51620\n",
      "Epoch 93/100\n",
      "32/32 - 1s - loss: 0.2009 - accuracy: 0.9824 - precision_m: 0.9120 - recall_m: 0.7623 - f1_m: 0.8248 - val_loss: 0.9452 - val_accuracy: 0.9546 - val_precision_m: 0.1425 - val_recall_m: 0.0188 - val_f1_m: 0.0329\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.51620\n",
      "Epoch 94/100\n",
      "32/32 - 1s - loss: 0.1689 - accuracy: 0.9851 - precision_m: 0.9348 - recall_m: 0.7911 - f1_m: 0.8543 - val_loss: 0.9688 - val_accuracy: 0.9541 - val_precision_m: 0.3438 - val_recall_m: 0.0079 - val_f1_m: 0.0154\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.51620\n",
      "Epoch 95/100\n",
      "32/32 - 1s - loss: 0.1698 - accuracy: 0.9862 - precision_m: 0.9394 - recall_m: 0.7849 - f1_m: 0.8527 - val_loss: 0.7842 - val_accuracy: 0.9631 - val_precision_m: 0.7746 - val_recall_m: 0.0945 - val_f1_m: 0.1597\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.51620\n",
      "Epoch 96/100\n",
      "32/32 - 1s - loss: 0.1856 - accuracy: 0.9833 - precision_m: 0.9294 - recall_m: 0.7607 - f1_m: 0.8316 - val_loss: 0.9145 - val_accuracy: 0.9619 - val_precision_m: 0.5927 - val_recall_m: 0.0276 - val_f1_m: 0.0525\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.51620\n",
      "Epoch 97/100\n",
      "32/32 - 1s - loss: 0.1817 - accuracy: 0.9841 - precision_m: 0.9285 - recall_m: 0.7705 - f1_m: 0.8388 - val_loss: 0.9727 - val_accuracy: 0.9603 - val_precision_m: 0.4433 - val_recall_m: 0.0056 - val_f1_m: 0.0107\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.51620\n",
      "Epoch 98/100\n",
      "32/32 - 1s - loss: 0.1713 - accuracy: 0.9854 - precision_m: 0.9322 - recall_m: 0.7855 - f1_m: 0.8501 - val_loss: 0.8955 - val_accuracy: 0.9618 - val_precision_m: 0.4091 - val_recall_m: 0.0365 - val_f1_m: 0.0665\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.51620\n",
      "Epoch 99/100\n",
      "32/32 - 1s - loss: 0.1537 - accuracy: 0.9868 - precision_m: 0.9488 - recall_m: 0.8000 - f1_m: 0.8659 - val_loss: 0.7769 - val_accuracy: 0.9640 - val_precision_m: 0.8576 - val_recall_m: 0.0927 - val_f1_m: 0.1640\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.51620\n",
      "Epoch 100/100\n",
      "32/32 - 1s - loss: 0.1593 - accuracy: 0.9858 - precision_m: 0.9439 - recall_m: 0.7947 - f1_m: 0.8602 - val_loss: 0.7354 - val_accuracy: 0.9647 - val_precision_m: 0.8589 - val_recall_m: 0.1155 - val_f1_m: 0.2014\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.51620\n",
      "5/5 - 0s - loss: 0.5094 - accuracy: 0.9543 - precision_m: 0.8394 - recall_m: 0.2690 - f1_m: 0.4049\n",
      "The metrics for the test set with loss tversky, learning rate 0.0001,  filters 32, and batch size 8 is [0.5093915462493896, 0.9543336033821106, 0.8393877744674683, 0.2689957022666931, 0.404876172542572]\n",
      "__________________________________________________________________________________________________________________________________________________________________________\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 128, 128, 5) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 128, 128, 8)  368         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 128, 128, 8)  32          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 128, 128, 8)  0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 8)  584         activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 8)  32          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, 128, 8)  0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 64, 64, 8)    0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 16)   1168        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64, 64, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 64, 64, 16)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 64, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64, 64, 16)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 16)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 32)   4640        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 32)   128         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 32)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 32)   9248        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 32)   128         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 32)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 32)   0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 16, 16, 64)   18496       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 16, 16, 64)   256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 16, 16, 64)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 16, 16, 64)   36928       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 16, 16, 64)   256         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16, 16, 64)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 64)     0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 8, 8, 128)    73856       max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 8, 8, 128)    512         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 8, 8, 128)    0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 8, 8, 128)    147584      activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 8, 8, 128)    512         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 8, 8, 128)    0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 16, 16, 128)  65664       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 16, 16, 192)  0           conv2d_transpose[0][0]           \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 64)   110656      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 64)   256         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 64)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 64)   36928       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 64)   256         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 64)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 32, 32, 64)   16448       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 96)   0           conv2d_transpose_1[0][0]         \n",
      "                                                                 activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 32)   27680       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 32, 32, 32)   128         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 32, 32, 32)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 32, 32)   9248        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 32, 32, 32)   128         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 32, 32, 32)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 64, 64, 32)   4128        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 64, 64, 48)   0           conv2d_transpose_2[0][0]         \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 16)   6928        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64, 64, 16)   64          conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 64, 64, 16)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 16)   2320        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 64, 64, 16)   64          conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 64, 64, 16)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 128, 128, 16) 1040        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 128, 128, 24) 0           conv2d_transpose_3[0][0]         \n",
      "                                                                 activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 128, 128, 8)  1736        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 128, 128, 8)  32          conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 128, 128, 8)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 128, 128, 8)  584         activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 128, 128, 8)  32          conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 128, 128, 8)  0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 128, 128, 1)  9           activation_17[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 581,505\n",
      "Trainable params: 580,033\n",
      "Non-trainable params: 1,472\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "Initialising in 3..........2..........1......................................\n",
      "Training on 284 images with learning rate 0.0001, batch size 16, and number of filters 8\n",
      "Epoch 1/100\n",
      "16/16 - 3s - loss: 0.9209 - accuracy: 0.4876 - precision_m: 0.0623 - recall_m: 0.5764 - f1_m: 0.1116 - val_loss: 0.9453 - val_accuracy: 0.4835 - val_precision_m: 0.0428 - val_recall_m: 0.5968 - val_f1_m: 0.0796\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.51620\n",
      "Epoch 2/100\n",
      "16/16 - 0s - loss: 0.9185 - accuracy: 0.4303 - precision_m: 0.0671 - recall_m: 0.7039 - f1_m: 0.1217 - val_loss: 0.9450 - val_accuracy: 0.3985 - val_precision_m: 0.0441 - val_recall_m: 0.7293 - val_f1_m: 0.0830\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.51620\n",
      "Epoch 3/100\n",
      "16/16 - 0s - loss: 0.9156 - accuracy: 0.3741 - precision_m: 0.0720 - recall_m: 0.8467 - f1_m: 0.1320 - val_loss: 0.9447 - val_accuracy: 0.3547 - val_precision_m: 0.0451 - val_recall_m: 0.8015 - val_f1_m: 0.0852\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.51620\n",
      "Epoch 4/100\n",
      "16/16 - 0s - loss: 0.9117 - accuracy: 0.3951 - precision_m: 0.0763 - recall_m: 0.8721 - f1_m: 0.1393 - val_loss: 0.9442 - val_accuracy: 0.3422 - val_precision_m: 0.0458 - val_recall_m: 0.8304 - val_f1_m: 0.0867\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.51620\n",
      "Epoch 5/100\n",
      "16/16 - 0s - loss: 0.9080 - accuracy: 0.4320 - precision_m: 0.0812 - recall_m: 0.8727 - f1_m: 0.1472 - val_loss: 0.9438 - val_accuracy: 0.2924 - val_precision_m: 0.0454 - val_recall_m: 0.8863 - val_f1_m: 0.0862\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.51620\n",
      "Epoch 6/100\n",
      "16/16 - 0s - loss: 0.9035 - accuracy: 0.4627 - precision_m: 0.0860 - recall_m: 0.8884 - f1_m: 0.1562 - val_loss: 0.9439 - val_accuracy: 0.2403 - val_precision_m: 0.0440 - val_recall_m: 0.9241 - val_f1_m: 0.0839\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.51620\n",
      "Epoch 7/100\n",
      "16/16 - 0s - loss: 0.8984 - accuracy: 0.4939 - precision_m: 0.0935 - recall_m: 0.9096 - f1_m: 0.1683 - val_loss: 0.9434 - val_accuracy: 0.2466 - val_precision_m: 0.0438 - val_recall_m: 0.9142 - val_f1_m: 0.0835\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.51620\n",
      "Epoch 8/100\n",
      "16/16 - 0s - loss: 0.8938 - accuracy: 0.5188 - precision_m: 0.0978 - recall_m: 0.9098 - f1_m: 0.1754 - val_loss: 0.9429 - val_accuracy: 0.2162 - val_precision_m: 0.0439 - val_recall_m: 0.9494 - val_f1_m: 0.0839\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.51620\n",
      "Epoch 9/100\n",
      "16/16 - 0s - loss: 0.8928 - accuracy: 0.5282 - precision_m: 0.0955 - recall_m: 0.8749 - f1_m: 0.1705 - val_loss: 0.9423 - val_accuracy: 0.1894 - val_precision_m: 0.0439 - val_recall_m: 0.9770 - val_f1_m: 0.0839\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.51620\n",
      "Epoch 10/100\n",
      "16/16 - 0s - loss: 0.8853 - accuracy: 0.5578 - precision_m: 0.1050 - recall_m: 0.8844 - f1_m: 0.1866 - val_loss: 0.9424 - val_accuracy: 0.1963 - val_precision_m: 0.0431 - val_recall_m: 0.9546 - val_f1_m: 0.0824\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.51620\n",
      "Epoch 11/100\n",
      "16/16 - 0s - loss: 0.8793 - accuracy: 0.6069 - precision_m: 0.1134 - recall_m: 0.8732 - f1_m: 0.1992 - val_loss: 0.9419 - val_accuracy: 0.1799 - val_precision_m: 0.0437 - val_recall_m: 0.9830 - val_f1_m: 0.0835\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.51620\n",
      "Epoch 12/100\n",
      "16/16 - 0s - loss: 0.8746 - accuracy: 0.6394 - precision_m: 0.1204 - recall_m: 0.8483 - f1_m: 0.2091 - val_loss: 0.9421 - val_accuracy: 0.1383 - val_precision_m: 0.0419 - val_recall_m: 0.9894 - val_f1_m: 0.0804\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.51620\n",
      "Epoch 13/100\n",
      "16/16 - 0s - loss: 0.8680 - accuracy: 0.6597 - precision_m: 0.1291 - recall_m: 0.8641 - f1_m: 0.2215 - val_loss: 0.9413 - val_accuracy: 0.1707 - val_precision_m: 0.0431 - val_recall_m: 0.9804 - val_f1_m: 0.0824\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.51620\n",
      "Epoch 14/100\n",
      "16/16 - 0s - loss: 0.8657 - accuracy: 0.6745 - precision_m: 0.1302 - recall_m: 0.8455 - f1_m: 0.2234 - val_loss: 0.9409 - val_accuracy: 0.1893 - val_precision_m: 0.0435 - val_recall_m: 0.9708 - val_f1_m: 0.0832\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.51620\n",
      "Epoch 15/100\n",
      "16/16 - 0s - loss: 0.8648 - accuracy: 0.6916 - precision_m: 0.1328 - recall_m: 0.8158 - f1_m: 0.2259 - val_loss: 0.9384 - val_accuracy: 0.2484 - val_precision_m: 0.0463 - val_recall_m: 0.9537 - val_f1_m: 0.0882\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.51620\n",
      "Epoch 16/100\n",
      "16/16 - 0s - loss: 0.8626 - accuracy: 0.7074 - precision_m: 0.1377 - recall_m: 0.7976 - f1_m: 0.2321 - val_loss: 0.9368 - val_accuracy: 0.3603 - val_precision_m: 0.0504 - val_recall_m: 0.8886 - val_f1_m: 0.0954\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.51620\n",
      "Epoch 17/100\n",
      "16/16 - 0s - loss: 0.8572 - accuracy: 0.6924 - precision_m: 0.1385 - recall_m: 0.8408 - f1_m: 0.2342 - val_loss: 0.9377 - val_accuracy: 0.3225 - val_precision_m: 0.0478 - val_recall_m: 0.8896 - val_f1_m: 0.0906\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.51620\n",
      "Epoch 18/100\n",
      "16/16 - 0s - loss: 0.8530 - accuracy: 0.7201 - precision_m: 0.1487 - recall_m: 0.8317 - f1_m: 0.2506 - val_loss: 0.9364 - val_accuracy: 0.3825 - val_precision_m: 0.0493 - val_recall_m: 0.8389 - val_f1_m: 0.0931\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.51620\n",
      "Epoch 19/100\n",
      "16/16 - 0s - loss: 0.8466 - accuracy: 0.7399 - precision_m: 0.1576 - recall_m: 0.8382 - f1_m: 0.2623 - val_loss: 0.9368 - val_accuracy: 0.3676 - val_precision_m: 0.0488 - val_recall_m: 0.8510 - val_f1_m: 0.0923\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.51620\n",
      "Epoch 20/100\n",
      "16/16 - 0s - loss: 0.8476 - accuracy: 0.7473 - precision_m: 0.1578 - recall_m: 0.7994 - f1_m: 0.2594 - val_loss: 0.9347 - val_accuracy: 0.3808 - val_precision_m: 0.0511 - val_recall_m: 0.8707 - val_f1_m: 0.0964\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.51620\n",
      "Epoch 21/100\n",
      "16/16 - 0s - loss: 0.8437 - accuracy: 0.7557 - precision_m: 0.1631 - recall_m: 0.8247 - f1_m: 0.2670 - val_loss: 0.9327 - val_accuracy: 0.4041 - val_precision_m: 0.0534 - val_recall_m: 0.8766 - val_f1_m: 0.1005\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.51620\n",
      "Epoch 22/100\n",
      "16/16 - 0s - loss: 0.8349 - accuracy: 0.7742 - precision_m: 0.1777 - recall_m: 0.8303 - f1_m: 0.2894 - val_loss: 0.9360 - val_accuracy: 0.3608 - val_precision_m: 0.0489 - val_recall_m: 0.8613 - val_f1_m: 0.0925\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.51620\n",
      "Epoch 23/100\n",
      "16/16 - 0s - loss: 0.8324 - accuracy: 0.7954 - precision_m: 0.1905 - recall_m: 0.8218 - f1_m: 0.3039 - val_loss: 0.9356 - val_accuracy: 0.3151 - val_precision_m: 0.0491 - val_recall_m: 0.9265 - val_f1_m: 0.0932\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.51620\n",
      "Epoch 24/100\n",
      "16/16 - 0s - loss: 0.8315 - accuracy: 0.8028 - precision_m: 0.1939 - recall_m: 0.8134 - f1_m: 0.3060 - val_loss: 0.9333 - val_accuracy: 0.4223 - val_precision_m: 0.0517 - val_recall_m: 0.8204 - val_f1_m: 0.0972\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.51620\n",
      "Epoch 25/100\n",
      "16/16 - 0s - loss: 0.8271 - accuracy: 0.8094 - precision_m: 0.2044 - recall_m: 0.8156 - f1_m: 0.3206 - val_loss: 0.9333 - val_accuracy: 0.3749 - val_precision_m: 0.0513 - val_recall_m: 0.8860 - val_f1_m: 0.0969\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.51620\n",
      "Epoch 26/100\n",
      "16/16 - 0s - loss: 0.8220 - accuracy: 0.8163 - precision_m: 0.2110 - recall_m: 0.8326 - f1_m: 0.3318 - val_loss: 0.9330 - val_accuracy: 0.4217 - val_precision_m: 0.0518 - val_recall_m: 0.8265 - val_f1_m: 0.0973\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.51620\n",
      "Epoch 27/100\n",
      "16/16 - 1s - loss: 0.8208 - accuracy: 0.8073 - precision_m: 0.2084 - recall_m: 0.8548 - f1_m: 0.3322 - val_loss: 0.9340 - val_accuracy: 0.3978 - val_precision_m: 0.0504 - val_recall_m: 0.8339 - val_f1_m: 0.0949\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.51620\n",
      "Epoch 28/100\n",
      "16/16 - 0s - loss: 0.8190 - accuracy: 0.8101 - precision_m: 0.2120 - recall_m: 0.8412 - f1_m: 0.3329 - val_loss: 0.9318 - val_accuracy: 0.3439 - val_precision_m: 0.0525 - val_recall_m: 0.9493 - val_f1_m: 0.0993\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.51620\n",
      "Epoch 29/100\n",
      "16/16 - 0s - loss: 0.8130 - accuracy: 0.8241 - precision_m: 0.2232 - recall_m: 0.8590 - f1_m: 0.3493 - val_loss: 0.9303 - val_accuracy: 0.4293 - val_precision_m: 0.0543 - val_recall_m: 0.8499 - val_f1_m: 0.1020\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.51620\n",
      "Epoch 30/100\n",
      "16/16 - 0s - loss: 0.8136 - accuracy: 0.8330 - precision_m: 0.2261 - recall_m: 0.8199 - f1_m: 0.3529 - val_loss: 0.9223 - val_accuracy: 0.6019 - val_precision_m: 0.0670 - val_recall_m: 0.7288 - val_f1_m: 0.1227\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.51620\n",
      "Epoch 31/100\n",
      "16/16 - 0s - loss: 0.8184 - accuracy: 0.8302 - precision_m: 0.2228 - recall_m: 0.7988 - f1_m: 0.3418 - val_loss: 0.9272 - val_accuracy: 0.5211 - val_precision_m: 0.0589 - val_recall_m: 0.7761 - val_f1_m: 0.1094\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.51620\n",
      "Epoch 32/100\n",
      "16/16 - 0s - loss: 0.8064 - accuracy: 0.8391 - precision_m: 0.2397 - recall_m: 0.8561 - f1_m: 0.3704 - val_loss: 0.9243 - val_accuracy: 0.4838 - val_precision_m: 0.0603 - val_recall_m: 0.8579 - val_f1_m: 0.1127\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.51620\n",
      "Epoch 33/100\n",
      "16/16 - 0s - loss: 0.7994 - accuracy: 0.8381 - precision_m: 0.2466 - recall_m: 0.8835 - f1_m: 0.3811 - val_loss: 0.9215 - val_accuracy: 0.5105 - val_precision_m: 0.0640 - val_recall_m: 0.8646 - val_f1_m: 0.1192\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.51620\n",
      "Epoch 34/100\n",
      "16/16 - 0s - loss: 0.8189 - accuracy: 0.8256 - precision_m: 0.2227 - recall_m: 0.7993 - f1_m: 0.3348 - val_loss: 0.9205 - val_accuracy: 0.6087 - val_precision_m: 0.0681 - val_recall_m: 0.7286 - val_f1_m: 0.1245\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.51620\n",
      "Epoch 35/100\n",
      "16/16 - 0s - loss: 0.8019 - accuracy: 0.8436 - precision_m: 0.2446 - recall_m: 0.8437 - f1_m: 0.3699 - val_loss: 0.9252 - val_accuracy: 0.5474 - val_precision_m: 0.0602 - val_recall_m: 0.7484 - val_f1_m: 0.1114\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.51620\n",
      "Epoch 36/100\n",
      "16/16 - 0s - loss: 0.7968 - accuracy: 0.8575 - precision_m: 0.2661 - recall_m: 0.8495 - f1_m: 0.3959 - val_loss: 0.9191 - val_accuracy: 0.5560 - val_precision_m: 0.0665 - val_recall_m: 0.8108 - val_f1_m: 0.1228\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.51620\n",
      "Epoch 37/100\n",
      "16/16 - 0s - loss: 0.7921 - accuracy: 0.8644 - precision_m: 0.2727 - recall_m: 0.8372 - f1_m: 0.4067 - val_loss: 0.9115 - val_accuracy: 0.6431 - val_precision_m: 0.0779 - val_recall_m: 0.7495 - val_f1_m: 0.1411\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.51620\n",
      "Epoch 38/100\n",
      "16/16 - 0s - loss: 0.7958 - accuracy: 0.8624 - precision_m: 0.2667 - recall_m: 0.8480 - f1_m: 0.3953 - val_loss: 0.9178 - val_accuracy: 0.6053 - val_precision_m: 0.0699 - val_recall_m: 0.7486 - val_f1_m: 0.1278\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.51620\n",
      "Epoch 39/100\n",
      "16/16 - 0s - loss: 0.7881 - accuracy: 0.8603 - precision_m: 0.2698 - recall_m: 0.8602 - f1_m: 0.4043 - val_loss: 0.9132 - val_accuracy: 0.6316 - val_precision_m: 0.0744 - val_recall_m: 0.7412 - val_f1_m: 0.1353\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.51620\n",
      "Epoch 40/100\n",
      "16/16 - 0s - loss: 0.7863 - accuracy: 0.8521 - precision_m: 0.2647 - recall_m: 0.8858 - f1_m: 0.3985 - val_loss: 0.9134 - val_accuracy: 0.6626 - val_precision_m: 0.0756 - val_recall_m: 0.6861 - val_f1_m: 0.1362\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.51620\n",
      "Epoch 41/100\n",
      "16/16 - 0s - loss: 0.7797 - accuracy: 0.8635 - precision_m: 0.2762 - recall_m: 0.8767 - f1_m: 0.4167 - val_loss: 0.9170 - val_accuracy: 0.6235 - val_precision_m: 0.0711 - val_recall_m: 0.7247 - val_f1_m: 0.1295\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.51620\n",
      "Epoch 42/100\n",
      "16/16 - 0s - loss: 0.7880 - accuracy: 0.8629 - precision_m: 0.2769 - recall_m: 0.8324 - f1_m: 0.4062 - val_loss: 0.9153 - val_accuracy: 0.6557 - val_precision_m: 0.0735 - val_recall_m: 0.6797 - val_f1_m: 0.1326\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.51620\n",
      "Epoch 43/100\n",
      "16/16 - 0s - loss: 0.7745 - accuracy: 0.8791 - precision_m: 0.2995 - recall_m: 0.8615 - f1_m: 0.4388 - val_loss: 0.9122 - val_accuracy: 0.6120 - val_precision_m: 0.0745 - val_recall_m: 0.7878 - val_f1_m: 0.1361\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.51620\n",
      "Epoch 44/100\n",
      "16/16 - 0s - loss: 0.7716 - accuracy: 0.8837 - precision_m: 0.3083 - recall_m: 0.8615 - f1_m: 0.4486 - val_loss: 0.9025 - val_accuracy: 0.7625 - val_precision_m: 0.0993 - val_recall_m: 0.6107 - val_f1_m: 0.1708\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.51620\n",
      "Epoch 45/100\n",
      "16/16 - 0s - loss: 0.7850 - accuracy: 0.8684 - precision_m: 0.2795 - recall_m: 0.8117 - f1_m: 0.4057 - val_loss: 0.9131 - val_accuracy: 0.6199 - val_precision_m: 0.0729 - val_recall_m: 0.7505 - val_f1_m: 0.1330\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.51620\n",
      "Epoch 46/100\n",
      "16/16 - 0s - loss: 0.7882 - accuracy: 0.8571 - precision_m: 0.2586 - recall_m: 0.8306 - f1_m: 0.3821 - val_loss: 0.8974 - val_accuracy: 0.8188 - val_precision_m: 0.1240 - val_recall_m: 0.5107 - val_f1_m: 0.1987\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.51620\n",
      "Epoch 47/100\n",
      "16/16 - 0s - loss: 0.7729 - accuracy: 0.8672 - precision_m: 0.2775 - recall_m: 0.8791 - f1_m: 0.4144 - val_loss: 0.9050 - val_accuracy: 0.7649 - val_precision_m: 0.0959 - val_recall_m: 0.5843 - val_f1_m: 0.1648\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.51620\n",
      "Epoch 48/100\n",
      "16/16 - 0s - loss: 0.7551 - accuracy: 0.8931 - precision_m: 0.3359 - recall_m: 0.8790 - f1_m: 0.4794 - val_loss: 0.9046 - val_accuracy: 0.9159 - val_precision_m: 0.2340 - val_recall_m: 0.3032 - val_f1_m: 0.2545\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.51620\n",
      "Epoch 49/100\n",
      "16/16 - 0s - loss: 0.7587 - accuracy: 0.8918 - precision_m: 0.3303 - recall_m: 0.8652 - f1_m: 0.4717 - val_loss: 0.8859 - val_accuracy: 0.8548 - val_precision_m: 0.1717 - val_recall_m: 0.5610 - val_f1_m: 0.2591\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.51620\n",
      "Epoch 50/100\n",
      "16/16 - 0s - loss: 0.7506 - accuracy: 0.9006 - precision_m: 0.3509 - recall_m: 0.8754 - f1_m: 0.4896 - val_loss: 0.9016 - val_accuracy: 0.9059 - val_precision_m: 0.2342 - val_recall_m: 0.3235 - val_f1_m: 0.2585\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.51620\n",
      "Epoch 51/100\n",
      "16/16 - 0s - loss: 0.7502 - accuracy: 0.9080 - precision_m: 0.3688 - recall_m: 0.8400 - f1_m: 0.5066 - val_loss: 0.8845 - val_accuracy: 0.9073 - val_precision_m: 0.2195 - val_recall_m: 0.4359 - val_f1_m: 0.2905\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.51620\n",
      "Epoch 52/100\n",
      "16/16 - 0s - loss: 0.7442 - accuracy: 0.9107 - precision_m: 0.3748 - recall_m: 0.8637 - f1_m: 0.5128 - val_loss: 0.8732 - val_accuracy: 0.9125 - val_precision_m: 0.2467 - val_recall_m: 0.4906 - val_f1_m: 0.3272\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.51620\n",
      "Epoch 53/100\n",
      "16/16 - 0s - loss: 0.7451 - accuracy: 0.9123 - precision_m: 0.3734 - recall_m: 0.8587 - f1_m: 0.5079 - val_loss: 0.8939 - val_accuracy: 0.9325 - val_precision_m: 0.2752 - val_recall_m: 0.3035 - val_f1_m: 0.2812\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.51620\n",
      "Epoch 54/100\n",
      "16/16 - 0s - loss: 0.7480 - accuracy: 0.8996 - precision_m: 0.3479 - recall_m: 0.8670 - f1_m: 0.4825 - val_loss: 0.8784 - val_accuracy: 0.9369 - val_precision_m: 0.3472 - val_recall_m: 0.3648 - val_f1_m: 0.3460\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.51620\n",
      "Epoch 55/100\n",
      "16/16 - 0s - loss: 0.7395 - accuracy: 0.9164 - precision_m: 0.3913 - recall_m: 0.8376 - f1_m: 0.5218 - val_loss: 0.8759 - val_accuracy: 0.9236 - val_precision_m: 0.2782 - val_recall_m: 0.4360 - val_f1_m: 0.3366\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.51620\n",
      "Epoch 56/100\n",
      "16/16 - 0s - loss: 0.7370 - accuracy: 0.9072 - precision_m: 0.3642 - recall_m: 0.8713 - f1_m: 0.5033 - val_loss: 0.8912 - val_accuracy: 0.9466 - val_precision_m: 0.3392 - val_recall_m: 0.2598 - val_f1_m: 0.2894\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.51620\n",
      "Epoch 57/100\n",
      "16/16 - 0s - loss: 0.7427 - accuracy: 0.9128 - precision_m: 0.3791 - recall_m: 0.8388 - f1_m: 0.5045 - val_loss: 0.9118 - val_accuracy: 0.9524 - val_precision_m: 0.4711 - val_recall_m: 0.1652 - val_f1_m: 0.2354\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.51620\n",
      "Epoch 58/100\n",
      "16/16 - 0s - loss: 0.7230 - accuracy: 0.9177 - precision_m: 0.3978 - recall_m: 0.8824 - f1_m: 0.5380 - val_loss: 0.9156 - val_accuracy: 0.9316 - val_precision_m: 0.3447 - val_recall_m: 0.1793 - val_f1_m: 0.2064\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.51620\n",
      "Epoch 59/100\n",
      "16/16 - 0s - loss: 0.7221 - accuracy: 0.9245 - precision_m: 0.4146 - recall_m: 0.8791 - f1_m: 0.5476 - val_loss: 0.9360 - val_accuracy: 0.9342 - val_precision_m: 0.3334 - val_recall_m: 0.0879 - val_f1_m: 0.1307\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.51620\n",
      "Epoch 60/100\n",
      "16/16 - 0s - loss: 0.7232 - accuracy: 0.9209 - precision_m: 0.4049 - recall_m: 0.8566 - f1_m: 0.5397 - val_loss: 0.8970 - val_accuracy: 0.9486 - val_precision_m: 0.4042 - val_recall_m: 0.2440 - val_f1_m: 0.2959\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.51620\n",
      "Epoch 61/100\n",
      "16/16 - 0s - loss: 0.7343 - accuracy: 0.9144 - precision_m: 0.3818 - recall_m: 0.8154 - f1_m: 0.5158 - val_loss: 0.8976 - val_accuracy: 0.9600 - val_precision_m: 0.4894 - val_recall_m: 0.2149 - val_f1_m: 0.2976\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.51620\n",
      "Epoch 62/100\n",
      "16/16 - 0s - loss: 0.7399 - accuracy: 0.9100 - precision_m: 0.3729 - recall_m: 0.7976 - f1_m: 0.4824 - val_loss: 0.8865 - val_accuracy: 0.9575 - val_precision_m: 0.4880 - val_recall_m: 0.2571 - val_f1_m: 0.3348\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.51620\n",
      "Epoch 63/100\n",
      "16/16 - 0s - loss: 0.7143 - accuracy: 0.9190 - precision_m: 0.4039 - recall_m: 0.8756 - f1_m: 0.5415 - val_loss: 0.9161 - val_accuracy: 0.9566 - val_precision_m: 0.4449 - val_recall_m: 0.1411 - val_f1_m: 0.2136\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.51620\n",
      "Epoch 64/100\n",
      "16/16 - 0s - loss: 0.7140 - accuracy: 0.9363 - precision_m: 0.4609 - recall_m: 0.8229 - f1_m: 0.5741 - val_loss: 0.9404 - val_accuracy: 0.9565 - val_precision_m: 0.3807 - val_recall_m: 0.0451 - val_f1_m: 0.0798\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.51620\n",
      "Epoch 65/100\n",
      "16/16 - 0s - loss: 0.7158 - accuracy: 0.9199 - precision_m: 0.4024 - recall_m: 0.8531 - f1_m: 0.5330 - val_loss: 0.8774 - val_accuracy: 0.9443 - val_precision_m: 0.3357 - val_recall_m: 0.3121 - val_f1_m: 0.3145\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.51620\n",
      "Epoch 66/100\n",
      "16/16 - 0s - loss: 0.6996 - accuracy: 0.9379 - precision_m: 0.4678 - recall_m: 0.8675 - f1_m: 0.5963 - val_loss: 0.9091 - val_accuracy: 0.9551 - val_precision_m: 0.4332 - val_recall_m: 0.1522 - val_f1_m: 0.2214\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.51620\n",
      "Epoch 67/100\n",
      "16/16 - 0s - loss: 0.6866 - accuracy: 0.9394 - precision_m: 0.4758 - recall_m: 0.8844 - f1_m: 0.6140 - val_loss: 0.9325 - val_accuracy: 0.9484 - val_precision_m: 0.4037 - val_recall_m: 0.0766 - val_f1_m: 0.1196\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.51620\n",
      "Epoch 68/100\n",
      "16/16 - 0s - loss: 0.6873 - accuracy: 0.9427 - precision_m: 0.4946 - recall_m: 0.8740 - f1_m: 0.6198 - val_loss: 0.9056 - val_accuracy: 0.9512 - val_precision_m: 0.3843 - val_recall_m: 0.1819 - val_f1_m: 0.2441\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.51620\n",
      "Epoch 69/100\n",
      "16/16 - 1s - loss: 0.6783 - accuracy: 0.9446 - precision_m: 0.5041 - recall_m: 0.9040 - f1_m: 0.6352 - val_loss: 0.9060 - val_accuracy: 0.9514 - val_precision_m: 0.4337 - val_recall_m: 0.1672 - val_f1_m: 0.2319\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.51620\n",
      "Epoch 70/100\n",
      "16/16 - 0s - loss: 0.6806 - accuracy: 0.9484 - precision_m: 0.5273 - recall_m: 0.8717 - f1_m: 0.6473 - val_loss: 0.8756 - val_accuracy: 0.9553 - val_precision_m: 0.4564 - val_recall_m: 0.3060 - val_f1_m: 0.3644\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.51620\n",
      "Epoch 71/100\n",
      "16/16 - 0s - loss: 0.6751 - accuracy: 0.9490 - precision_m: 0.5302 - recall_m: 0.8909 - f1_m: 0.6539 - val_loss: 0.8756 - val_accuracy: 0.9578 - val_precision_m: 0.4948 - val_recall_m: 0.2992 - val_f1_m: 0.3712\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.51620\n",
      "Epoch 72/100\n",
      "16/16 - 0s - loss: 0.6644 - accuracy: 0.9554 - precision_m: 0.5660 - recall_m: 0.8946 - f1_m: 0.6860 - val_loss: 0.8693 - val_accuracy: 0.9531 - val_precision_m: 0.4133 - val_recall_m: 0.3358 - val_f1_m: 0.3704\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.51620\n",
      "Epoch 73/100\n",
      "16/16 - 0s - loss: 0.6803 - accuracy: 0.9457 - precision_m: 0.5076 - recall_m: 0.8850 - f1_m: 0.6272 - val_loss: 0.8565 - val_accuracy: 0.9395 - val_precision_m: 0.3236 - val_recall_m: 0.4249 - val_f1_m: 0.3671\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.51620\n",
      "Epoch 74/100\n",
      "16/16 - 0s - loss: 0.6660 - accuracy: 0.9563 - precision_m: 0.5645 - recall_m: 0.8969 - f1_m: 0.6801 - val_loss: 0.8568 - val_accuracy: 0.9445 - val_precision_m: 0.3473 - val_recall_m: 0.4050 - val_f1_m: 0.3738\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.51620\n",
      "Epoch 75/100\n",
      "16/16 - 0s - loss: 0.6681 - accuracy: 0.9575 - precision_m: 0.5715 - recall_m: 0.8795 - f1_m: 0.6844 - val_loss: 0.8644 - val_accuracy: 0.9347 - val_precision_m: 0.2906 - val_recall_m: 0.3985 - val_f1_m: 0.3360\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.51620\n",
      "Epoch 76/100\n",
      "16/16 - 0s - loss: 0.6631 - accuracy: 0.9530 - precision_m: 0.5470 - recall_m: 0.9011 - f1_m: 0.6702 - val_loss: 0.8480 - val_accuracy: 0.9293 - val_precision_m: 0.2898 - val_recall_m: 0.4787 - val_f1_m: 0.3603\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.51620\n",
      "Epoch 77/100\n",
      "16/16 - 0s - loss: 0.6765 - accuracy: 0.9481 - precision_m: 0.5151 - recall_m: 0.8903 - f1_m: 0.6292 - val_loss: 0.8498 - val_accuracy: 0.9226 - val_precision_m: 0.2683 - val_recall_m: 0.4909 - val_f1_m: 0.3462\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.51620\n",
      "Epoch 78/100\n",
      "16/16 - 0s - loss: 0.6580 - accuracy: 0.9581 - precision_m: 0.5796 - recall_m: 0.8976 - f1_m: 0.6960 - val_loss: 0.8527 - val_accuracy: 0.9336 - val_precision_m: 0.3068 - val_recall_m: 0.4429 - val_f1_m: 0.3614\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.51620\n",
      "Epoch 79/100\n",
      "16/16 - 0s - loss: 0.6685 - accuracy: 0.9560 - precision_m: 0.5612 - recall_m: 0.8794 - f1_m: 0.6685 - val_loss: 0.8571 - val_accuracy: 0.9145 - val_precision_m: 0.2380 - val_recall_m: 0.4764 - val_f1_m: 0.3170\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.51620\n",
      "Epoch 80/100\n",
      "16/16 - 0s - loss: 0.6632 - accuracy: 0.9508 - precision_m: 0.5304 - recall_m: 0.9016 - f1_m: 0.6566 - val_loss: 0.8490 - val_accuracy: 0.9386 - val_precision_m: 0.3177 - val_recall_m: 0.4307 - val_f1_m: 0.3654\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.51620\n",
      "Epoch 81/100\n",
      "16/16 - 0s - loss: 0.6565 - accuracy: 0.9573 - precision_m: 0.5706 - recall_m: 0.8997 - f1_m: 0.6870 - val_loss: 0.8526 - val_accuracy: 0.9312 - val_precision_m: 0.2868 - val_recall_m: 0.4468 - val_f1_m: 0.3491\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.51620\n",
      "Epoch 82/100\n",
      "16/16 - 0s - loss: 0.6513 - accuracy: 0.9602 - precision_m: 0.5971 - recall_m: 0.8884 - f1_m: 0.7062 - val_loss: 0.8538 - val_accuracy: 0.9299 - val_precision_m: 0.2866 - val_recall_m: 0.4447 - val_f1_m: 0.3480\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.51620\n",
      "Epoch 83/100\n",
      "16/16 - 0s - loss: 0.6528 - accuracy: 0.9564 - precision_m: 0.5738 - recall_m: 0.8962 - f1_m: 0.6900 - val_loss: 0.8517 - val_accuracy: 0.9236 - val_precision_m: 0.2740 - val_recall_m: 0.4671 - val_f1_m: 0.3440\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.51620\n",
      "Epoch 84/100\n",
      "16/16 - 0s - loss: 0.6741 - accuracy: 0.9545 - precision_m: 0.5618 - recall_m: 0.8578 - f1_m: 0.6613 - val_loss: 0.8637 - val_accuracy: 0.8861 - val_precision_m: 0.1848 - val_recall_m: 0.5210 - val_f1_m: 0.2727\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.51620\n",
      "Epoch 85/100\n",
      "16/16 - 0s - loss: 0.6537 - accuracy: 0.9526 - precision_m: 0.5493 - recall_m: 0.9014 - f1_m: 0.6751 - val_loss: 0.8478 - val_accuracy: 0.9193 - val_precision_m: 0.2613 - val_recall_m: 0.4908 - val_f1_m: 0.3401\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.51620\n",
      "Epoch 86/100\n",
      "16/16 - 0s - loss: 0.6501 - accuracy: 0.9621 - precision_m: 0.5978 - recall_m: 0.9016 - f1_m: 0.7046 - val_loss: 0.8487 - val_accuracy: 0.9278 - val_precision_m: 0.2807 - val_recall_m: 0.4655 - val_f1_m: 0.3498\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.51620\n",
      "Epoch 87/100\n",
      "16/16 - 0s - loss: 0.6468 - accuracy: 0.9617 - precision_m: 0.6008 - recall_m: 0.9005 - f1_m: 0.7129 - val_loss: 0.8518 - val_accuracy: 0.9376 - val_precision_m: 0.3162 - val_recall_m: 0.4227 - val_f1_m: 0.3614\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.51620\n",
      "Epoch 88/100\n",
      "16/16 - 0s - loss: 0.6465 - accuracy: 0.9616 - precision_m: 0.6078 - recall_m: 0.8972 - f1_m: 0.7151 - val_loss: 0.8534 - val_accuracy: 0.9117 - val_precision_m: 0.2360 - val_recall_m: 0.4917 - val_f1_m: 0.3184\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.51620\n",
      "Epoch 89/100\n",
      "16/16 - 0s - loss: 0.6518 - accuracy: 0.9589 - precision_m: 0.5880 - recall_m: 0.8915 - f1_m: 0.6940 - val_loss: 0.8434 - val_accuracy: 0.9427 - val_precision_m: 0.3459 - val_recall_m: 0.4306 - val_f1_m: 0.3833\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.51620\n",
      "Epoch 90/100\n",
      "16/16 - 0s - loss: 0.6413 - accuracy: 0.9610 - precision_m: 0.5947 - recall_m: 0.9081 - f1_m: 0.7106 - val_loss: 0.8494 - val_accuracy: 0.9439 - val_precision_m: 0.3521 - val_recall_m: 0.4084 - val_f1_m: 0.3779\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.51620\n",
      "Epoch 91/100\n",
      "16/16 - 0s - loss: 0.6453 - accuracy: 0.9625 - precision_m: 0.6070 - recall_m: 0.8859 - f1_m: 0.7111 - val_loss: 0.8442 - val_accuracy: 0.9240 - val_precision_m: 0.2712 - val_recall_m: 0.4873 - val_f1_m: 0.3480\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.51620\n",
      "Epoch 92/100\n",
      "16/16 - 0s - loss: 0.6472 - accuracy: 0.9578 - precision_m: 0.5799 - recall_m: 0.8838 - f1_m: 0.6938 - val_loss: 0.8424 - val_accuracy: 0.9298 - val_precision_m: 0.2870 - val_recall_m: 0.4778 - val_f1_m: 0.3584\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.51620\n",
      "Epoch 93/100\n",
      "16/16 - 0s - loss: 0.6336 - accuracy: 0.9660 - precision_m: 0.6443 - recall_m: 0.8925 - f1_m: 0.7448 - val_loss: 0.8465 - val_accuracy: 0.9253 - val_precision_m: 0.2756 - val_recall_m: 0.4820 - val_f1_m: 0.3504\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.51620\n",
      "Epoch 94/100\n",
      "16/16 - 0s - loss: 0.6342 - accuracy: 0.9639 - precision_m: 0.6275 - recall_m: 0.8948 - f1_m: 0.7327 - val_loss: 0.8484 - val_accuracy: 0.9108 - val_precision_m: 0.2399 - val_recall_m: 0.5163 - val_f1_m: 0.3268\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.51620\n",
      "Epoch 95/100\n",
      "16/16 - 0s - loss: 0.6388 - accuracy: 0.9630 - precision_m: 0.6191 - recall_m: 0.8900 - f1_m: 0.7195 - val_loss: 0.8480 - val_accuracy: 0.9213 - val_precision_m: 0.2578 - val_recall_m: 0.4894 - val_f1_m: 0.3377\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.51620\n",
      "Epoch 96/100\n",
      "16/16 - 0s - loss: 0.6326 - accuracy: 0.9659 - precision_m: 0.6467 - recall_m: 0.8831 - f1_m: 0.7418 - val_loss: 0.8467 - val_accuracy: 0.9120 - val_precision_m: 0.2418 - val_recall_m: 0.5124 - val_f1_m: 0.3281\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.51620\n",
      "Epoch 97/100\n",
      "16/16 - 0s - loss: 0.6299 - accuracy: 0.9621 - precision_m: 0.6082 - recall_m: 0.9124 - f1_m: 0.7255 - val_loss: 0.8421 - val_accuracy: 0.9221 - val_precision_m: 0.2653 - val_recall_m: 0.4914 - val_f1_m: 0.3443\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.51620\n",
      "Epoch 98/100\n",
      "16/16 - 0s - loss: 0.6320 - accuracy: 0.9670 - precision_m: 0.6440 - recall_m: 0.8870 - f1_m: 0.7416 - val_loss: 0.8502 - val_accuracy: 0.9140 - val_precision_m: 0.2381 - val_recall_m: 0.4868 - val_f1_m: 0.3196\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.51620\n",
      "Epoch 99/100\n",
      "16/16 - 0s - loss: 0.6333 - accuracy: 0.9656 - precision_m: 0.6289 - recall_m: 0.9005 - f1_m: 0.7283 - val_loss: 0.8416 - val_accuracy: 0.9210 - val_precision_m: 0.2596 - val_recall_m: 0.5048 - val_f1_m: 0.3428\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.51620\n",
      "Epoch 100/100\n",
      "16/16 - 0s - loss: 0.6440 - accuracy: 0.9580 - precision_m: 0.5824 - recall_m: 0.8731 - f1_m: 0.6889 - val_loss: 0.8397 - val_accuracy: 0.9280 - val_precision_m: 0.2804 - val_recall_m: 0.4786 - val_f1_m: 0.3535\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.51620\n",
      "5/5 - 0s - loss: 0.7260 - accuracy: 0.9332 - precision_m: 0.4500 - recall_m: 0.6317 - f1_m: 0.5201\n",
      "The metrics for the test set with loss tversky, learning rate 0.0001,  filters 8, and batch size 16 is [0.7259541153907776, 0.933163046836853, 0.44997015595436096, 0.6316591501235962, 0.5200805068016052]\n",
      "__________________________________________________________________________________________________________________________________________________________________________\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 128, 128, 5) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 128, 128, 16) 736         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 128, 128, 16) 64          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 128, 128, 16) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 16) 2320        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 16) 64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, 128, 16) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 64, 64, 16)   0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 32)   4640        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64, 64, 32)   128         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 64, 64, 32)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 64, 32)   9248        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 32)   128         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64, 64, 32)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 32)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 64)   18496       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 64)   256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 64)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 64)   36928       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 64)   256         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 64)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 64)   0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 16, 16, 128)  73856       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 16, 16, 128)  512         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 16, 16, 128)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 16, 16, 128)  147584      activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 16, 16, 128)  512         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16, 16, 128)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 128)    0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 8, 8, 256)    295168      max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 8, 8, 256)    1024        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 8, 8, 256)    0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 8, 8, 256)    590080      activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 8, 8, 256)    1024        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 8, 8, 256)    0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 16, 16, 128)  131200      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 16, 16, 256)  0           conv2d_transpose[0][0]           \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 128)  295040      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 128)  512         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 128)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 128)  147584      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 128)  512         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 128)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 32, 32, 64)   32832       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 128)  0           conv2d_transpose_1[0][0]         \n",
      "                                                                 activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 64)   73792       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 32, 32, 64)   256         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 32, 32, 64)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 32, 64)   36928       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 32, 32, 64)   256         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 32, 32, 64)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 64, 64, 32)   8224        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 64, 64, 64)   0           conv2d_transpose_2[0][0]         \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 32)   18464       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64, 64, 32)   128         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 64, 64, 32)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 32)   9248        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 64, 64, 32)   128         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 64, 64, 32)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 128, 128, 16) 2064        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 128, 128, 32) 0           conv2d_transpose_3[0][0]         \n",
      "                                                                 activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 128, 128, 16) 4624        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 128, 128, 16) 64          conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 128, 128, 16) 0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 128, 128, 16) 2320        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 128, 128, 16) 64          conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 128, 128, 16) 0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 128, 128, 1)  17          activation_17[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,947,281\n",
      "Trainable params: 1,944,337\n",
      "Non-trainable params: 2,944\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "Initialising in 3..........2..........1......................................\n",
      "Training on 284 images with learning rate 0.0001, batch size 16, and number of filters 16\n",
      "Epoch 1/100\n",
      "16/16 - 3s - loss: 0.9212 - accuracy: 0.5905 - precision_m: 0.0610 - recall_m: 0.4598 - f1_m: 0.1048 - val_loss: 0.9443 - val_accuracy: 0.5192 - val_precision_m: 0.0462 - val_recall_m: 0.6070 - val_f1_m: 0.0857\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.51620\n",
      "Epoch 2/100\n",
      "16/16 - 1s - loss: 0.9174 - accuracy: 0.3178 - precision_m: 0.0693 - recall_m: 0.8945 - f1_m: 0.1280 - val_loss: 0.9438 - val_accuracy: 0.4238 - val_precision_m: 0.0457 - val_recall_m: 0.7258 - val_f1_m: 0.0860\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.51620\n",
      "Epoch 3/100\n",
      "16/16 - 1s - loss: 0.9125 - accuracy: 0.3631 - precision_m: 0.0742 - recall_m: 0.8935 - f1_m: 0.1361 - val_loss: 0.9431 - val_accuracy: 0.3755 - val_precision_m: 0.0473 - val_recall_m: 0.8137 - val_f1_m: 0.0893\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.51620\n",
      "Epoch 4/100\n",
      "16/16 - 1s - loss: 0.9057 - accuracy: 0.4176 - precision_m: 0.0806 - recall_m: 0.8873 - f1_m: 0.1469 - val_loss: 0.9427 - val_accuracy: 0.3104 - val_precision_m: 0.0470 - val_recall_m: 0.8961 - val_f1_m: 0.0893\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.51620\n",
      "Epoch 5/100\n",
      "16/16 - 1s - loss: 0.8995 - accuracy: 0.4510 - precision_m: 0.0862 - recall_m: 0.9090 - f1_m: 0.1552 - val_loss: 0.9428 - val_accuracy: 0.2568 - val_precision_m: 0.0453 - val_recall_m: 0.9284 - val_f1_m: 0.0863\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.51620\n",
      "Epoch 6/100\n",
      "16/16 - 1s - loss: 0.8905 - accuracy: 0.4899 - precision_m: 0.0933 - recall_m: 0.9247 - f1_m: 0.1689 - val_loss: 0.9423 - val_accuracy: 0.2216 - val_precision_m: 0.0450 - val_recall_m: 0.9630 - val_f1_m: 0.0858\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.51620\n",
      "Epoch 7/100\n",
      "16/16 - 1s - loss: 0.8832 - accuracy: 0.5255 - precision_m: 0.1007 - recall_m: 0.9136 - f1_m: 0.1797 - val_loss: 0.9424 - val_accuracy: 0.1775 - val_precision_m: 0.0430 - val_recall_m: 0.9705 - val_f1_m: 0.0822\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.51620\n",
      "Epoch 8/100\n",
      "16/16 - 1s - loss: 0.8823 - accuracy: 0.5462 - precision_m: 0.0989 - recall_m: 0.8719 - f1_m: 0.1760 - val_loss: 0.9388 - val_accuracy: 0.3009 - val_precision_m: 0.0470 - val_recall_m: 0.9114 - val_f1_m: 0.0894\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.51620\n",
      "Epoch 9/100\n",
      "16/16 - 1s - loss: 0.8783 - accuracy: 0.5480 - precision_m: 0.1025 - recall_m: 0.8841 - f1_m: 0.1807 - val_loss: 0.9420 - val_accuracy: 0.1522 - val_precision_m: 0.0425 - val_recall_m: 0.9862 - val_f1_m: 0.0813\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.51620\n",
      "Epoch 10/100\n",
      "16/16 - 1s - loss: 0.8674 - accuracy: 0.6293 - precision_m: 0.1173 - recall_m: 0.8383 - f1_m: 0.2039 - val_loss: 0.9395 - val_accuracy: 0.2307 - val_precision_m: 0.0458 - val_recall_m: 0.9716 - val_f1_m: 0.0874\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.51620\n",
      "Epoch 11/100\n",
      "16/16 - 1s - loss: 0.8702 - accuracy: 0.6176 - precision_m: 0.1105 - recall_m: 0.8006 - f1_m: 0.1925 - val_loss: 0.9391 - val_accuracy: 0.2628 - val_precision_m: 0.0461 - val_recall_m: 0.9401 - val_f1_m: 0.0879\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.51620\n",
      "Epoch 12/100\n",
      "16/16 - 1s - loss: 0.8626 - accuracy: 0.6634 - precision_m: 0.1219 - recall_m: 0.7813 - f1_m: 0.2095 - val_loss: 0.9367 - val_accuracy: 0.3230 - val_precision_m: 0.0500 - val_recall_m: 0.9319 - val_f1_m: 0.0948\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.51620\n",
      "Epoch 13/100\n",
      "16/16 - 1s - loss: 0.8581 - accuracy: 0.6741 - precision_m: 0.1268 - recall_m: 0.7996 - f1_m: 0.2155 - val_loss: 0.9365 - val_accuracy: 0.3386 - val_precision_m: 0.0494 - val_recall_m: 0.9056 - val_f1_m: 0.0936\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.51620\n",
      "Epoch 14/100\n",
      "16/16 - 1s - loss: 0.8635 - accuracy: 0.6494 - precision_m: 0.1170 - recall_m: 0.8103 - f1_m: 0.2015 - val_loss: 0.9352 - val_accuracy: 0.2755 - val_precision_m: 0.0486 - val_recall_m: 0.9711 - val_f1_m: 0.0926\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.51620\n",
      "Epoch 15/100\n",
      "16/16 - 1s - loss: 0.8464 - accuracy: 0.7132 - precision_m: 0.1414 - recall_m: 0.8126 - f1_m: 0.2376 - val_loss: 0.9363 - val_accuracy: 0.3130 - val_precision_m: 0.0495 - val_recall_m: 0.9390 - val_f1_m: 0.0939\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.51620\n",
      "Epoch 16/100\n",
      "16/16 - 1s - loss: 0.8528 - accuracy: 0.6949 - precision_m: 0.1317 - recall_m: 0.7877 - f1_m: 0.2234 - val_loss: 0.9360 - val_accuracy: 0.3191 - val_precision_m: 0.0483 - val_recall_m: 0.9110 - val_f1_m: 0.0917\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.51620\n",
      "Epoch 17/100\n",
      "16/16 - 1s - loss: 0.8424 - accuracy: 0.7159 - precision_m: 0.1436 - recall_m: 0.8006 - f1_m: 0.2405 - val_loss: 0.9353 - val_accuracy: 0.2717 - val_precision_m: 0.0483 - val_recall_m: 0.9705 - val_f1_m: 0.0920\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.51620\n",
      "Epoch 18/100\n",
      "16/16 - 1s - loss: 0.8250 - accuracy: 0.7576 - precision_m: 0.1697 - recall_m: 0.8417 - f1_m: 0.2791 - val_loss: 0.9396 - val_accuracy: 0.1663 - val_precision_m: 0.0434 - val_recall_m: 0.9919 - val_f1_m: 0.0830\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.51620\n",
      "Epoch 19/100\n",
      "16/16 - 1s - loss: 0.8184 - accuracy: 0.7664 - precision_m: 0.1765 - recall_m: 0.8636 - f1_m: 0.2886 - val_loss: 0.9377 - val_accuracy: 0.2129 - val_precision_m: 0.0453 - val_recall_m: 0.9805 - val_f1_m: 0.0865\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.51620\n",
      "Epoch 20/100\n",
      "16/16 - 1s - loss: 0.8153 - accuracy: 0.7874 - precision_m: 0.1867 - recall_m: 0.8369 - f1_m: 0.3021 - val_loss: 0.9361 - val_accuracy: 0.3192 - val_precision_m: 0.0476 - val_recall_m: 0.8985 - val_f1_m: 0.0903\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.51620\n",
      "Epoch 21/100\n",
      "16/16 - 1s - loss: 0.8012 - accuracy: 0.7985 - precision_m: 0.2062 - recall_m: 0.8688 - f1_m: 0.3288 - val_loss: 0.9377 - val_accuracy: 0.2223 - val_precision_m: 0.0454 - val_recall_m: 0.9711 - val_f1_m: 0.0865\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.51620\n",
      "Epoch 22/100\n",
      "16/16 - 1s - loss: 0.7940 - accuracy: 0.8328 - precision_m: 0.2308 - recall_m: 0.8337 - f1_m: 0.3528 - val_loss: 0.9358 - val_accuracy: 0.2641 - val_precision_m: 0.0475 - val_recall_m: 0.9645 - val_f1_m: 0.0904\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.51620\n",
      "Epoch 23/100\n",
      "16/16 - 1s - loss: 0.7928 - accuracy: 0.8357 - precision_m: 0.2299 - recall_m: 0.8339 - f1_m: 0.3529 - val_loss: 0.9355 - val_accuracy: 0.2676 - val_precision_m: 0.0473 - val_recall_m: 0.9561 - val_f1_m: 0.0901\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.51620\n",
      "Epoch 24/100\n",
      "16/16 - 1s - loss: 0.7884 - accuracy: 0.8296 - precision_m: 0.2316 - recall_m: 0.8523 - f1_m: 0.3591 - val_loss: 0.9333 - val_accuracy: 0.2958 - val_precision_m: 0.0492 - val_recall_m: 0.9573 - val_f1_m: 0.0935\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.51620\n",
      "Epoch 25/100\n",
      "16/16 - 1s - loss: 0.7800 - accuracy: 0.8381 - precision_m: 0.2408 - recall_m: 0.8651 - f1_m: 0.3729 - val_loss: 0.9319 - val_accuracy: 0.3144 - val_precision_m: 0.0505 - val_recall_m: 0.9570 - val_f1_m: 0.0958\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.51620\n",
      "Epoch 26/100\n",
      "16/16 - 1s - loss: 0.7794 - accuracy: 0.8525 - precision_m: 0.2550 - recall_m: 0.8239 - f1_m: 0.3791 - val_loss: 0.9321 - val_accuracy: 0.3251 - val_precision_m: 0.0506 - val_recall_m: 0.9439 - val_f1_m: 0.0959\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.51620\n",
      "Epoch 27/100\n",
      "16/16 - 1s - loss: 0.7762 - accuracy: 0.8474 - precision_m: 0.2514 - recall_m: 0.8401 - f1_m: 0.3795 - val_loss: 0.9291 - val_accuracy: 0.4475 - val_precision_m: 0.0533 - val_recall_m: 0.8143 - val_f1_m: 0.0999\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.51620\n",
      "Epoch 28/100\n",
      "16/16 - 1s - loss: 0.7628 - accuracy: 0.8594 - precision_m: 0.2678 - recall_m: 0.8687 - f1_m: 0.4051 - val_loss: 0.9275 - val_accuracy: 0.5638 - val_precision_m: 0.0580 - val_recall_m: 0.7058 - val_f1_m: 0.1070\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.51620\n",
      "Epoch 29/100\n",
      "16/16 - 1s - loss: 0.7752 - accuracy: 0.8745 - precision_m: 0.2791 - recall_m: 0.7633 - f1_m: 0.3987 - val_loss: 0.9194 - val_accuracy: 0.4733 - val_precision_m: 0.0637 - val_recall_m: 0.9305 - val_f1_m: 0.1191\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.51620\n",
      "Epoch 30/100\n",
      "16/16 - 1s - loss: 0.7665 - accuracy: 0.8774 - precision_m: 0.2895 - recall_m: 0.7897 - f1_m: 0.4096 - val_loss: 0.9217 - val_accuracy: 0.4303 - val_precision_m: 0.0607 - val_recall_m: 0.9611 - val_f1_m: 0.1141\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.51620\n",
      "Epoch 31/100\n",
      "16/16 - 1s - loss: 0.7531 - accuracy: 0.8811 - precision_m: 0.2983 - recall_m: 0.8265 - f1_m: 0.4304 - val_loss: 0.9202 - val_accuracy: 0.4714 - val_precision_m: 0.0615 - val_recall_m: 0.9009 - val_f1_m: 0.1150\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.51620\n",
      "Epoch 32/100\n",
      "16/16 - 1s - loss: 0.7532 - accuracy: 0.8772 - precision_m: 0.2946 - recall_m: 0.8402 - f1_m: 0.4259 - val_loss: 0.9137 - val_accuracy: 0.7498 - val_precision_m: 0.0765 - val_recall_m: 0.5140 - val_f1_m: 0.1327\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.51620\n",
      "Epoch 33/100\n",
      "16/16 - 1s - loss: 0.7475 - accuracy: 0.8851 - precision_m: 0.3063 - recall_m: 0.8138 - f1_m: 0.4369 - val_loss: 0.9101 - val_accuracy: 0.7106 - val_precision_m: 0.0782 - val_recall_m: 0.6353 - val_f1_m: 0.1388\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.51620\n",
      "Epoch 34/100\n",
      "16/16 - 1s - loss: 0.7419 - accuracy: 0.8875 - precision_m: 0.3136 - recall_m: 0.8242 - f1_m: 0.4472 - val_loss: 0.9151 - val_accuracy: 0.5766 - val_precision_m: 0.0681 - val_recall_m: 0.8035 - val_f1_m: 0.1253\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.51620\n",
      "Epoch 35/100\n",
      "16/16 - 1s - loss: 0.7350 - accuracy: 0.8987 - precision_m: 0.3395 - recall_m: 0.8229 - f1_m: 0.4702 - val_loss: 0.9077 - val_accuracy: 0.8709 - val_precision_m: 0.1109 - val_recall_m: 0.3613 - val_f1_m: 0.1679\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.51620\n",
      "Epoch 36/100\n",
      "16/16 - 1s - loss: 0.7435 - accuracy: 0.8982 - precision_m: 0.3301 - recall_m: 0.7959 - f1_m: 0.4477 - val_loss: 0.9081 - val_accuracy: 0.9159 - val_precision_m: 0.1312 - val_recall_m: 0.2149 - val_f1_m: 0.1596\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.51620\n",
      "Epoch 37/100\n",
      "16/16 - 1s - loss: 0.7173 - accuracy: 0.9171 - precision_m: 0.3865 - recall_m: 0.8175 - f1_m: 0.5180 - val_loss: 0.9162 - val_accuracy: 0.9175 - val_precision_m: 0.1215 - val_recall_m: 0.1799 - val_f1_m: 0.1423\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.51620\n",
      "Epoch 38/100\n",
      "16/16 - 1s - loss: 0.7195 - accuracy: 0.9100 - precision_m: 0.3673 - recall_m: 0.8134 - f1_m: 0.5001 - val_loss: 0.8931 - val_accuracy: 0.8428 - val_precision_m: 0.1146 - val_recall_m: 0.4580 - val_f1_m: 0.1832\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.51620\n",
      "Epoch 39/100\n",
      "16/16 - 1s - loss: 0.7092 - accuracy: 0.9232 - precision_m: 0.4073 - recall_m: 0.8113 - f1_m: 0.5347 - val_loss: 0.8943 - val_accuracy: 0.8144 - val_precision_m: 0.1075 - val_recall_m: 0.5198 - val_f1_m: 0.1781\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.51620\n",
      "Epoch 40/100\n",
      "16/16 - 1s - loss: 0.6898 - accuracy: 0.9305 - precision_m: 0.4413 - recall_m: 0.8372 - f1_m: 0.5732 - val_loss: 0.9004 - val_accuracy: 0.9143 - val_precision_m: 0.1465 - val_recall_m: 0.2623 - val_f1_m: 0.1863\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.51620\n",
      "Epoch 41/100\n",
      "16/16 - 1s - loss: 0.7242 - accuracy: 0.9147 - precision_m: 0.3718 - recall_m: 0.7572 - f1_m: 0.4852 - val_loss: 0.8534 - val_accuracy: 0.9425 - val_precision_m: 0.3362 - val_recall_m: 0.3566 - val_f1_m: 0.3450\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.51620\n",
      "Epoch 42/100\n",
      "16/16 - 1s - loss: 0.6917 - accuracy: 0.9336 - precision_m: 0.4520 - recall_m: 0.8034 - f1_m: 0.5643 - val_loss: 0.8833 - val_accuracy: 0.8360 - val_precision_m: 0.1229 - val_recall_m: 0.5374 - val_f1_m: 0.1998\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.51620\n",
      "Epoch 43/100\n",
      "16/16 - 1s - loss: 0.6959 - accuracy: 0.9249 - precision_m: 0.4128 - recall_m: 0.7922 - f1_m: 0.5330 - val_loss: 0.8406 - val_accuracy: 0.8912 - val_precision_m: 0.2416 - val_recall_m: 0.5564 - val_f1_m: 0.3270\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.51620\n",
      "Epoch 44/100\n",
      "16/16 - 1s - loss: 0.6786 - accuracy: 0.9314 - precision_m: 0.4389 - recall_m: 0.8440 - f1_m: 0.5658 - val_loss: 0.8345 - val_accuracy: 0.9076 - val_precision_m: 0.2587 - val_recall_m: 0.5406 - val_f1_m: 0.3447\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.51620\n",
      "Epoch 45/100\n",
      "16/16 - 1s - loss: 0.6836 - accuracy: 0.9342 - precision_m: 0.4508 - recall_m: 0.7961 - f1_m: 0.5624 - val_loss: 0.8461 - val_accuracy: 0.9216 - val_precision_m: 0.2804 - val_recall_m: 0.4322 - val_f1_m: 0.3338\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.51620\n",
      "Epoch 46/100\n",
      "16/16 - 1s - loss: 0.7046 - accuracy: 0.9314 - precision_m: 0.4399 - recall_m: 0.7078 - f1_m: 0.5258 - val_loss: 0.8384 - val_accuracy: 0.8960 - val_precision_m: 0.2378 - val_recall_m: 0.5966 - val_f1_m: 0.3363\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.51620\n",
      "Epoch 47/100\n",
      "16/16 - 1s - loss: 0.6520 - accuracy: 0.9437 - precision_m: 0.5014 - recall_m: 0.8511 - f1_m: 0.6216 - val_loss: 0.8641 - val_accuracy: 0.8463 - val_precision_m: 0.1520 - val_recall_m: 0.5912 - val_f1_m: 0.2415\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.51620\n",
      "Epoch 48/100\n",
      "16/16 - 1s - loss: 0.6575 - accuracy: 0.9423 - precision_m: 0.5037 - recall_m: 0.8051 - f1_m: 0.6101 - val_loss: 0.8553 - val_accuracy: 0.9110 - val_precision_m: 0.2575 - val_recall_m: 0.4193 - val_f1_m: 0.3116\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.51620\n",
      "Epoch 49/100\n",
      "16/16 - 1s - loss: 0.6700 - accuracy: 0.9362 - precision_m: 0.4636 - recall_m: 0.7731 - f1_m: 0.5703 - val_loss: 0.8442 - val_accuracy: 0.8685 - val_precision_m: 0.1773 - val_recall_m: 0.6176 - val_f1_m: 0.2752\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.51620\n",
      "Epoch 50/100\n",
      "16/16 - 1s - loss: 0.6427 - accuracy: 0.9484 - precision_m: 0.5290 - recall_m: 0.8174 - f1_m: 0.6365 - val_loss: 0.8938 - val_accuracy: 0.8542 - val_precision_m: 0.1111 - val_recall_m: 0.4111 - val_f1_m: 0.1742\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.51620\n",
      "Epoch 51/100\n",
      "16/16 - 1s - loss: 0.6490 - accuracy: 0.9447 - precision_m: 0.5111 - recall_m: 0.7959 - f1_m: 0.6088 - val_loss: 0.9200 - val_accuracy: 0.9569 - val_precision_m: 0.4205 - val_recall_m: 0.0683 - val_f1_m: 0.1169\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.51620\n",
      "Epoch 52/100\n",
      "16/16 - 1s - loss: 0.6257 - accuracy: 0.9557 - precision_m: 0.5750 - recall_m: 0.8111 - f1_m: 0.6620 - val_loss: 0.8298 - val_accuracy: 0.9391 - val_precision_m: 0.3402 - val_recall_m: 0.4085 - val_f1_m: 0.3697\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.51620\n",
      "Epoch 53/100\n",
      "16/16 - 1s - loss: 0.6321 - accuracy: 0.9480 - precision_m: 0.5192 - recall_m: 0.8220 - f1_m: 0.6289 - val_loss: 0.8466 - val_accuracy: 0.9516 - val_precision_m: 0.4708 - val_recall_m: 0.2544 - val_f1_m: 0.3048\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.51620\n",
      "Epoch 54/100\n",
      "16/16 - 1s - loss: 0.6362 - accuracy: 0.9453 - precision_m: 0.5040 - recall_m: 0.8037 - f1_m: 0.6062 - val_loss: 0.8243 - val_accuracy: 0.9497 - val_precision_m: 0.4564 - val_recall_m: 0.3393 - val_f1_m: 0.3827\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.51620\n",
      "Epoch 55/100\n",
      "16/16 - 1s - loss: 0.6130 - accuracy: 0.9547 - precision_m: 0.5635 - recall_m: 0.8258 - f1_m: 0.6638 - val_loss: 0.8245 - val_accuracy: 0.9501 - val_precision_m: 0.4272 - val_recall_m: 0.3477 - val_f1_m: 0.3794\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.51620\n",
      "Epoch 56/100\n",
      "16/16 - 1s - loss: 0.6091 - accuracy: 0.9571 - precision_m: 0.5861 - recall_m: 0.8187 - f1_m: 0.6678 - val_loss: 0.8503 - val_accuracy: 0.9536 - val_precision_m: 0.4642 - val_recall_m: 0.2480 - val_f1_m: 0.3203\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.51620\n",
      "Epoch 57/100\n",
      "16/16 - 1s - loss: 0.6294 - accuracy: 0.9487 - precision_m: 0.5306 - recall_m: 0.7857 - f1_m: 0.6171 - val_loss: 0.8055 - val_accuracy: 0.9350 - val_precision_m: 0.3184 - val_recall_m: 0.4594 - val_f1_m: 0.3744\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.51620\n",
      "Epoch 58/100\n",
      "16/16 - 1s - loss: 0.6038 - accuracy: 0.9571 - precision_m: 0.5812 - recall_m: 0.8037 - f1_m: 0.6639 - val_loss: 0.8480 - val_accuracy: 0.8952 - val_precision_m: 0.1951 - val_recall_m: 0.4474 - val_f1_m: 0.2689\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.51620\n",
      "Epoch 59/100\n",
      "16/16 - 1s - loss: 0.6403 - accuracy: 0.9436 - precision_m: 0.4969 - recall_m: 0.7462 - f1_m: 0.5802 - val_loss: 0.8192 - val_accuracy: 0.9493 - val_precision_m: 0.4325 - val_recall_m: 0.3245 - val_f1_m: 0.3655\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.51620\n",
      "Epoch 60/100\n",
      "16/16 - 1s - loss: 0.6009 - accuracy: 0.9513 - precision_m: 0.5469 - recall_m: 0.8200 - f1_m: 0.6429 - val_loss: 0.8279 - val_accuracy: 0.9398 - val_precision_m: 0.3304 - val_recall_m: 0.3610 - val_f1_m: 0.3421\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.51620\n",
      "Epoch 61/100\n",
      "16/16 - 1s - loss: 0.5716 - accuracy: 0.9643 - precision_m: 0.6372 - recall_m: 0.8373 - f1_m: 0.7123 - val_loss: 0.8017 - val_accuracy: 0.9253 - val_precision_m: 0.3187 - val_recall_m: 0.5054 - val_f1_m: 0.3839\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.51620\n",
      "Epoch 62/100\n",
      "16/16 - 1s - loss: 0.5887 - accuracy: 0.9586 - precision_m: 0.6044 - recall_m: 0.8019 - f1_m: 0.6765 - val_loss: 0.8411 - val_accuracy: 0.8961 - val_precision_m: 0.1849 - val_recall_m: 0.4703 - val_f1_m: 0.2654\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.51620\n",
      "Epoch 63/100\n",
      "16/16 - 1s - loss: 0.5886 - accuracy: 0.9600 - precision_m: 0.6217 - recall_m: 0.7912 - f1_m: 0.6780 - val_loss: 0.8492 - val_accuracy: 0.9097 - val_precision_m: 0.1989 - val_recall_m: 0.3950 - val_f1_m: 0.2643\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.51620\n",
      "Epoch 64/100\n",
      "16/16 - 1s - loss: 0.5557 - accuracy: 0.9665 - precision_m: 0.6630 - recall_m: 0.8486 - f1_m: 0.7289 - val_loss: 0.8679 - val_accuracy: 0.8956 - val_precision_m: 0.1573 - val_recall_m: 0.3687 - val_f1_m: 0.2205\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.51620\n",
      "Epoch 65/100\n",
      "16/16 - 1s - loss: 0.5422 - accuracy: 0.9684 - precision_m: 0.6782 - recall_m: 0.8332 - f1_m: 0.7427 - val_loss: 0.8101 - val_accuracy: 0.9308 - val_precision_m: 0.2998 - val_recall_m: 0.4263 - val_f1_m: 0.3487\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.51620\n",
      "Epoch 66/100\n",
      "16/16 - 1s - loss: 0.5469 - accuracy: 0.9643 - precision_m: 0.6437 - recall_m: 0.8286 - f1_m: 0.7174 - val_loss: 0.8086 - val_accuracy: 0.9451 - val_precision_m: 0.3955 - val_recall_m: 0.3627 - val_f1_m: 0.3721\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.51620\n",
      "Epoch 67/100\n",
      "16/16 - 1s - loss: 0.5475 - accuracy: 0.9677 - precision_m: 0.6918 - recall_m: 0.7975 - f1_m: 0.7285 - val_loss: 0.8113 - val_accuracy: 0.9540 - val_precision_m: 0.4303 - val_recall_m: 0.3022 - val_f1_m: 0.3547\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.51620\n",
      "Epoch 68/100\n",
      "16/16 - 1s - loss: 0.5427 - accuracy: 0.9666 - precision_m: 0.6808 - recall_m: 0.7932 - f1_m: 0.7268 - val_loss: 0.8045 - val_accuracy: 0.9415 - val_precision_m: 0.3364 - val_recall_m: 0.3905 - val_f1_m: 0.3593\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.51620\n",
      "Epoch 69/100\n",
      "16/16 - 1s - loss: 0.5363 - accuracy: 0.9697 - precision_m: 0.7071 - recall_m: 0.7904 - f1_m: 0.7374 - val_loss: 0.7881 - val_accuracy: 0.9498 - val_precision_m: 0.4078 - val_recall_m: 0.3793 - val_f1_m: 0.3917\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.51620\n",
      "Epoch 70/100\n",
      "16/16 - 1s - loss: 0.4980 - accuracy: 0.9742 - precision_m: 0.7305 - recall_m: 0.8523 - f1_m: 0.7820 - val_loss: 0.7667 - val_accuracy: 0.9482 - val_precision_m: 0.4221 - val_recall_m: 0.4413 - val_f1_m: 0.4251\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.51620\n",
      "Epoch 71/100\n",
      "16/16 - 1s - loss: 0.5066 - accuracy: 0.9717 - precision_m: 0.7106 - recall_m: 0.8381 - f1_m: 0.7626 - val_loss: 0.7943 - val_accuracy: 0.9564 - val_precision_m: 0.4824 - val_recall_m: 0.3138 - val_f1_m: 0.3797\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.51620\n",
      "Epoch 72/100\n",
      "16/16 - 1s - loss: 0.5094 - accuracy: 0.9703 - precision_m: 0.7039 - recall_m: 0.8220 - f1_m: 0.7526 - val_loss: 0.8034 - val_accuracy: 0.9259 - val_precision_m: 0.2786 - val_recall_m: 0.4267 - val_f1_m: 0.3348\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.51620\n",
      "Epoch 73/100\n",
      "16/16 - 1s - loss: 0.5125 - accuracy: 0.9697 - precision_m: 0.6887 - recall_m: 0.8286 - f1_m: 0.7406 - val_loss: 0.7995 - val_accuracy: 0.9612 - val_precision_m: 0.6043 - val_recall_m: 0.2513 - val_f1_m: 0.3531\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.51620\n",
      "Epoch 74/100\n",
      "16/16 - 1s - loss: 0.4934 - accuracy: 0.9725 - precision_m: 0.7131 - recall_m: 0.8374 - f1_m: 0.7630 - val_loss: 0.7783 - val_accuracy: 0.9586 - val_precision_m: 0.5413 - val_recall_m: 0.3156 - val_f1_m: 0.3947\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.51620\n",
      "Epoch 75/100\n",
      "16/16 - 1s - loss: 0.4926 - accuracy: 0.9739 - precision_m: 0.7408 - recall_m: 0.8241 - f1_m: 0.7661 - val_loss: 0.8274 - val_accuracy: 0.9597 - val_precision_m: 0.6045 - val_recall_m: 0.1995 - val_f1_m: 0.2956\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.51620\n",
      "Epoch 76/100\n",
      "16/16 - 1s - loss: 0.4674 - accuracy: 0.9764 - precision_m: 0.7589 - recall_m: 0.8482 - f1_m: 0.7940 - val_loss: 0.8320 - val_accuracy: 0.9607 - val_precision_m: 0.5918 - val_recall_m: 0.1792 - val_f1_m: 0.2729\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.51620\n",
      "Epoch 77/100\n",
      "16/16 - 1s - loss: 0.4547 - accuracy: 0.9792 - precision_m: 0.7902 - recall_m: 0.8509 - f1_m: 0.8130 - val_loss: 0.7790 - val_accuracy: 0.9561 - val_precision_m: 0.5062 - val_recall_m: 0.3104 - val_f1_m: 0.3805\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.51620\n",
      "Epoch 78/100\n",
      "16/16 - 1s - loss: 0.4830 - accuracy: 0.9732 - precision_m: 0.7352 - recall_m: 0.8184 - f1_m: 0.7598 - val_loss: 0.7599 - val_accuracy: 0.9584 - val_precision_m: 0.5632 - val_recall_m: 0.3309 - val_f1_m: 0.4111\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.51620\n",
      "Epoch 79/100\n",
      "16/16 - 1s - loss: 0.4733 - accuracy: 0.9734 - precision_m: 0.7271 - recall_m: 0.8321 - f1_m: 0.7632 - val_loss: 0.7992 - val_accuracy: 0.9586 - val_precision_m: 0.5228 - val_recall_m: 0.2350 - val_f1_m: 0.3215\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.51620\n",
      "Epoch 80/100\n",
      "16/16 - 1s - loss: 0.4538 - accuracy: 0.9776 - precision_m: 0.7732 - recall_m: 0.8395 - f1_m: 0.7964 - val_loss: 0.8623 - val_accuracy: 0.9614 - val_precision_m: 0.5905 - val_recall_m: 0.1201 - val_f1_m: 0.1993\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.51620\n",
      "Epoch 81/100\n",
      "16/16 - 1s - loss: 0.4679 - accuracy: 0.9735 - precision_m: 0.7353 - recall_m: 0.8094 - f1_m: 0.7631 - val_loss: 0.8160 - val_accuracy: 0.9624 - val_precision_m: 0.6033 - val_recall_m: 0.1899 - val_f1_m: 0.2889\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.51620\n",
      "Epoch 82/100\n",
      "16/16 - 1s - loss: 0.4278 - accuracy: 0.9797 - precision_m: 0.8001 - recall_m: 0.8450 - f1_m: 0.8182 - val_loss: 0.8139 - val_accuracy: 0.9605 - val_precision_m: 0.5880 - val_recall_m: 0.1963 - val_f1_m: 0.2929\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.51620\n",
      "Epoch 83/100\n",
      "16/16 - 1s - loss: 0.4666 - accuracy: 0.9728 - precision_m: 0.7358 - recall_m: 0.7880 - f1_m: 0.7518 - val_loss: 0.8655 - val_accuracy: 0.9620 - val_precision_m: 0.6529 - val_recall_m: 0.1155 - val_f1_m: 0.1962\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.51620\n",
      "Epoch 84/100\n",
      "16/16 - 1s - loss: 0.4351 - accuracy: 0.9760 - precision_m: 0.7683 - recall_m: 0.8269 - f1_m: 0.7900 - val_loss: 0.7546 - val_accuracy: 0.9475 - val_precision_m: 0.3903 - val_recall_m: 0.3971 - val_f1_m: 0.3921\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.51620\n",
      "Epoch 85/100\n",
      "16/16 - 1s - loss: 0.4368 - accuracy: 0.9763 - precision_m: 0.7639 - recall_m: 0.8324 - f1_m: 0.7873 - val_loss: 0.7672 - val_accuracy: 0.9461 - val_precision_m: 0.3854 - val_recall_m: 0.3547 - val_f1_m: 0.3639\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.51620\n",
      "Epoch 86/100\n",
      "16/16 - 1s - loss: 0.4095 - accuracy: 0.9802 - precision_m: 0.8125 - recall_m: 0.8457 - f1_m: 0.8237 - val_loss: 0.7766 - val_accuracy: 0.9605 - val_precision_m: 0.5864 - val_recall_m: 0.2469 - val_f1_m: 0.3454\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.51620\n",
      "Epoch 87/100\n",
      "16/16 - 1s - loss: 0.4168 - accuracy: 0.9776 - precision_m: 0.7863 - recall_m: 0.8316 - f1_m: 0.8024 - val_loss: 0.7371 - val_accuracy: 0.9506 - val_precision_m: 0.4259 - val_recall_m: 0.3981 - val_f1_m: 0.4088\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.51620\n",
      "Epoch 88/100\n",
      "16/16 - 1s - loss: 0.4276 - accuracy: 0.9766 - precision_m: 0.7713 - recall_m: 0.8178 - f1_m: 0.7844 - val_loss: 0.7833 - val_accuracy: 0.9616 - val_precision_m: 0.5987 - val_recall_m: 0.2302 - val_f1_m: 0.3319\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.51620\n",
      "Epoch 89/100\n",
      "16/16 - 1s - loss: 0.4273 - accuracy: 0.9770 - precision_m: 0.7919 - recall_m: 0.8186 - f1_m: 0.7863 - val_loss: 0.7594 - val_accuracy: 0.9618 - val_precision_m: 0.6022 - val_recall_m: 0.2725 - val_f1_m: 0.3748\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.51620\n",
      "Epoch 90/100\n",
      "16/16 - 1s - loss: 0.4041 - accuracy: 0.9799 - precision_m: 0.8148 - recall_m: 0.8331 - f1_m: 0.8154 - val_loss: 0.8596 - val_accuracy: 0.9537 - val_precision_m: 0.4463 - val_recall_m: 0.1361 - val_f1_m: 0.2070\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.51620\n",
      "Epoch 91/100\n",
      "16/16 - 1s - loss: 0.4306 - accuracy: 0.9748 - precision_m: 0.7496 - recall_m: 0.8017 - f1_m: 0.7629 - val_loss: 0.8031 - val_accuracy: 0.9579 - val_precision_m: 0.5119 - val_recall_m: 0.1994 - val_f1_m: 0.2862\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.51620\n",
      "Epoch 92/100\n",
      "16/16 - 1s - loss: 0.3943 - accuracy: 0.9787 - precision_m: 0.7970 - recall_m: 0.8335 - f1_m: 0.8108 - val_loss: 0.7843 - val_accuracy: 0.9623 - val_precision_m: 0.6368 - val_recall_m: 0.2078 - val_f1_m: 0.3131\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.51620\n",
      "Epoch 93/100\n",
      "16/16 - 1s - loss: 0.3771 - accuracy: 0.9819 - precision_m: 0.8417 - recall_m: 0.8434 - f1_m: 0.8343 - val_loss: 0.7421 - val_accuracy: 0.9625 - val_precision_m: 0.5863 - val_recall_m: 0.2831 - val_f1_m: 0.3816\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.51620\n",
      "Epoch 94/100\n",
      "16/16 - 1s - loss: 0.3587 - accuracy: 0.9837 - precision_m: 0.8456 - recall_m: 0.8631 - f1_m: 0.8496 - val_loss: 0.7444 - val_accuracy: 0.9615 - val_precision_m: 0.6067 - val_recall_m: 0.2736 - val_f1_m: 0.3764\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.51620\n",
      "Epoch 95/100\n",
      "16/16 - 1s - loss: 0.3757 - accuracy: 0.9820 - precision_m: 0.8281 - recall_m: 0.8422 - f1_m: 0.8282 - val_loss: 0.7314 - val_accuracy: 0.9636 - val_precision_m: 0.6360 - val_recall_m: 0.2802 - val_f1_m: 0.3890\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.51620\n",
      "Epoch 96/100\n",
      "16/16 - 1s - loss: 0.3497 - accuracy: 0.9846 - precision_m: 0.8632 - recall_m: 0.8574 - f1_m: 0.8570 - val_loss: 0.7113 - val_accuracy: 0.9575 - val_precision_m: 0.5318 - val_recall_m: 0.3603 - val_f1_m: 0.4271\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.51620\n",
      "Epoch 97/100\n",
      "16/16 - 1s - loss: 0.3343 - accuracy: 0.9852 - precision_m: 0.8847 - recall_m: 0.8581 - f1_m: 0.8681 - val_loss: 0.7112 - val_accuracy: 0.9610 - val_precision_m: 0.5436 - val_recall_m: 0.3399 - val_f1_m: 0.4183\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.51620\n",
      "Epoch 98/100\n",
      "16/16 - 1s - loss: 0.3530 - accuracy: 0.9840 - precision_m: 0.8461 - recall_m: 0.8661 - f1_m: 0.8509 - val_loss: 0.8084 - val_accuracy: 0.9628 - val_precision_m: 0.6598 - val_recall_m: 0.1606 - val_f1_m: 0.2583\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.51620\n",
      "Epoch 99/100\n",
      "16/16 - 1s - loss: 0.3677 - accuracy: 0.9802 - precision_m: 0.8295 - recall_m: 0.8294 - f1_m: 0.8199 - val_loss: 0.7801 - val_accuracy: 0.9631 - val_precision_m: 0.6610 - val_recall_m: 0.1946 - val_f1_m: 0.3006\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.51620\n",
      "Epoch 100/100\n",
      "16/16 - 1s - loss: 0.3753 - accuracy: 0.9779 - precision_m: 0.7917 - recall_m: 0.8303 - f1_m: 0.8015 - val_loss: 0.7249 - val_accuracy: 0.9615 - val_precision_m: 0.5579 - val_recall_m: 0.2926 - val_f1_m: 0.3838\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.51620\n",
      "5/5 - 0s - loss: 0.5393 - accuracy: 0.9575 - precision_m: 0.7000 - recall_m: 0.4541 - f1_m: 0.5473\n",
      "The metrics for the test set with loss tversky, learning rate 0.0001,  filters 16, and batch size 16 is [0.5393276214599609, 0.9574892520904541, 0.6999720335006714, 0.45413970947265625, 0.5473474860191345]\n",
      "__________________________________________________________________________________________________________________________________________________________________________\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 128, 128, 5) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 128, 128, 32) 1472        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 128, 128, 32) 128         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 128, 128, 32) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 32) 9248        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 32) 128         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, 128, 32) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 64, 64, 32)   0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 64)   18496       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64, 64, 64)   256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 64, 64, 64)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 64, 64)   36928       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 64)   256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64, 64, 64)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 64)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 128)  73856       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 128)  512         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 128)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 128)  147584      activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 128)  512         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 128)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 128)  0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 16, 16, 256)  295168      max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 16, 16, 256)  1024        conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 16, 16, 256)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 16, 16, 256)  590080      activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 16, 16, 256)  1024        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16, 16, 256)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 256)    0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 8, 8, 512)    1180160     max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 8, 8, 512)    2048        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 8, 8, 512)    0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 8, 8, 512)    2359808     activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 8, 8, 512)    2048        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 8, 8, 512)    0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 16, 16, 128)  262272      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 16, 16, 384)  0           conv2d_transpose[0][0]           \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 256)  884992      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 256)  1024        conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 256)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 256)  590080      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 256)  1024        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 256)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 32, 32, 64)   65600       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 192)  0           conv2d_transpose_1[0][0]         \n",
      "                                                                 activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 128)  221312      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 32, 32, 128)  512         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 32, 32, 128)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 32, 128)  147584      activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 32, 32, 128)  512         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 32, 32, 128)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 64, 64, 32)   16416       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 64, 64, 96)   0           conv2d_transpose_2[0][0]         \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 64)   55360       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64, 64, 64)   256         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 64, 64, 64)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 64)   36928       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 64, 64, 64)   256         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 64, 64, 64)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 128, 128, 16) 4112        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 128, 128, 48) 0           conv2d_transpose_3[0][0]         \n",
      "                                                                 activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 128, 128, 32) 13856       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 128, 128, 32) 128         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 128, 128, 32) 0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 128, 128, 32) 9248        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 128, 128, 32) 128         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 128, 128, 32) 0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 128, 128, 1)  33          activation_17[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 7,032,369\n",
      "Trainable params: 7,026,481\n",
      "Non-trainable params: 5,888\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "Initialising in 3..........2..........1......................................\n",
      "Training on 284 images with learning rate 0.0001, batch size 16, and number of filters 32\n",
      "Epoch 1/100\n",
      "16/16 - 3s - loss: 0.9230 - accuracy: 0.2433 - precision_m: 0.0619 - recall_m: 0.8756 - f1_m: 0.1149 - val_loss: 0.9460 - val_accuracy: 0.3888 - val_precision_m: 0.0444 - val_recall_m: 0.7470 - val_f1_m: 0.0836\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.51620\n",
      "Epoch 2/100\n",
      "16/16 - 1s - loss: 0.9163 - accuracy: 0.2826 - precision_m: 0.0692 - recall_m: 0.9352 - f1_m: 0.1278 - val_loss: 0.9435 - val_accuracy: 0.4098 - val_precision_m: 0.0460 - val_recall_m: 0.7487 - val_f1_m: 0.0865\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.51620\n",
      "Epoch 3/100\n",
      "16/16 - 1s - loss: 0.9074 - accuracy: 0.3709 - precision_m: 0.0754 - recall_m: 0.9090 - f1_m: 0.1389 - val_loss: 0.9419 - val_accuracy: 0.3608 - val_precision_m: 0.0464 - val_recall_m: 0.8165 - val_f1_m: 0.0878\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.51620\n",
      "Epoch 4/100\n",
      "16/16 - 1s - loss: 0.8968 - accuracy: 0.4413 - precision_m: 0.0833 - recall_m: 0.8782 - f1_m: 0.1509 - val_loss: 0.9421 - val_accuracy: 0.2467 - val_precision_m: 0.0437 - val_recall_m: 0.9032 - val_f1_m: 0.0832\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.51620\n",
      "Epoch 5/100\n",
      "16/16 - 1s - loss: 0.8863 - accuracy: 0.5115 - precision_m: 0.0915 - recall_m: 0.8493 - f1_m: 0.1636 - val_loss: 0.9393 - val_accuracy: 0.2648 - val_precision_m: 0.0471 - val_recall_m: 0.9519 - val_f1_m: 0.0896\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.51620\n",
      "Epoch 6/100\n",
      "16/16 - 1s - loss: 0.8771 - accuracy: 0.5447 - precision_m: 0.0982 - recall_m: 0.8587 - f1_m: 0.1743 - val_loss: 0.9418 - val_accuracy: 0.1118 - val_precision_m: 0.0407 - val_recall_m: 0.9879 - val_f1_m: 0.0781\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.51620\n",
      "Epoch 7/100\n",
      "16/16 - 1s - loss: 0.8686 - accuracy: 0.6020 - precision_m: 0.1076 - recall_m: 0.8269 - f1_m: 0.1878 - val_loss: 0.9388 - val_accuracy: 0.2402 - val_precision_m: 0.0437 - val_recall_m: 0.9194 - val_f1_m: 0.0834\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.51620\n",
      "Epoch 8/100\n",
      "16/16 - 1s - loss: 0.8521 - accuracy: 0.6607 - precision_m: 0.1264 - recall_m: 0.8386 - f1_m: 0.2179 - val_loss: 0.9415 - val_accuracy: 0.1121 - val_precision_m: 0.0409 - val_recall_m: 0.9923 - val_f1_m: 0.0785\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.51620\n",
      "Epoch 9/100\n",
      "16/16 - 1s - loss: 0.8329 - accuracy: 0.7103 - precision_m: 0.1497 - recall_m: 0.8782 - f1_m: 0.2526 - val_loss: 0.9423 - val_accuracy: 0.0933 - val_precision_m: 0.0402 - val_recall_m: 0.9945 - val_f1_m: 0.0772\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.51620\n",
      "Epoch 10/100\n",
      "16/16 - 1s - loss: 0.8181 - accuracy: 0.7582 - precision_m: 0.1729 - recall_m: 0.8697 - f1_m: 0.2853 - val_loss: 0.9429 - val_accuracy: 0.0829 - val_precision_m: 0.0399 - val_recall_m: 0.9985 - val_f1_m: 0.0767\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.51620\n",
      "Epoch 11/100\n",
      "16/16 - 1s - loss: 0.8113 - accuracy: 0.7774 - precision_m: 0.1837 - recall_m: 0.8637 - f1_m: 0.3010 - val_loss: 0.9403 - val_accuracy: 0.1539 - val_precision_m: 0.0427 - val_recall_m: 0.9877 - val_f1_m: 0.0817\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.51620\n",
      "Epoch 12/100\n",
      "16/16 - 1s - loss: 0.8150 - accuracy: 0.7645 - precision_m: 0.1783 - recall_m: 0.8441 - f1_m: 0.2893 - val_loss: 0.9352 - val_accuracy: 0.2909 - val_precision_m: 0.0485 - val_recall_m: 0.9510 - val_f1_m: 0.0922\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.51620\n",
      "Epoch 13/100\n",
      "16/16 - 1s - loss: 0.8242 - accuracy: 0.7426 - precision_m: 0.1573 - recall_m: 0.8151 - f1_m: 0.2623 - val_loss: 0.9395 - val_accuracy: 0.2199 - val_precision_m: 0.0435 - val_recall_m: 0.9364 - val_f1_m: 0.0830\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.51620\n",
      "Epoch 14/100\n",
      "16/16 - 1s - loss: 0.8068 - accuracy: 0.7691 - precision_m: 0.1795 - recall_m: 0.8673 - f1_m: 0.2946 - val_loss: 0.9378 - val_accuracy: 0.2306 - val_precision_m: 0.0452 - val_recall_m: 0.9566 - val_f1_m: 0.0861\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.51620\n",
      "Epoch 15/100\n",
      "16/16 - 1s - loss: 0.8112 - accuracy: 0.7898 - precision_m: 0.1849 - recall_m: 0.7950 - f1_m: 0.2968 - val_loss: 0.9365 - val_accuracy: 0.2323 - val_precision_m: 0.0460 - val_recall_m: 0.9726 - val_f1_m: 0.0877\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.51620\n",
      "Epoch 16/100\n",
      "16/16 - 1s - loss: 0.7873 - accuracy: 0.8064 - precision_m: 0.2110 - recall_m: 0.8890 - f1_m: 0.3393 - val_loss: 0.9394 - val_accuracy: 0.1624 - val_precision_m: 0.0435 - val_recall_m: 0.9976 - val_f1_m: 0.0833\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.51620\n",
      "Epoch 17/100\n",
      "16/16 - 1s - loss: 0.7810 - accuracy: 0.8279 - precision_m: 0.2284 - recall_m: 0.8799 - f1_m: 0.3573 - val_loss: 0.9265 - val_accuracy: 0.4999 - val_precision_m: 0.0579 - val_recall_m: 0.8066 - val_f1_m: 0.1077\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.51620\n",
      "Epoch 18/100\n",
      "16/16 - 1s - loss: 0.7890 - accuracy: 0.8272 - precision_m: 0.2192 - recall_m: 0.8099 - f1_m: 0.3409 - val_loss: 0.9263 - val_accuracy: 0.9310 - val_precision_m: 0.1200 - val_recall_m: 0.1167 - val_f1_m: 0.1174\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.51620\n",
      "Epoch 19/100\n",
      "16/16 - 1s - loss: 0.7878 - accuracy: 0.8064 - precision_m: 0.2059 - recall_m: 0.8477 - f1_m: 0.3282 - val_loss: 0.9149 - val_accuracy: 0.5400 - val_precision_m: 0.0678 - val_recall_m: 0.8644 - val_f1_m: 0.1257\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.51620\n",
      "Epoch 20/100\n",
      "16/16 - 1s - loss: 0.7782 - accuracy: 0.8170 - precision_m: 0.2209 - recall_m: 0.8891 - f1_m: 0.3442 - val_loss: 0.9181 - val_accuracy: 0.4927 - val_precision_m: 0.0639 - val_recall_m: 0.9027 - val_f1_m: 0.1194\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.51620\n",
      "Epoch 21/100\n",
      "16/16 - 1s - loss: 0.7663 - accuracy: 0.8448 - precision_m: 0.2491 - recall_m: 0.8651 - f1_m: 0.3830 - val_loss: 0.9065 - val_accuracy: 0.5802 - val_precision_m: 0.0773 - val_recall_m: 0.9051 - val_f1_m: 0.1425\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.51620\n",
      "Epoch 22/100\n",
      "16/16 - 1s - loss: 0.7558 - accuracy: 0.8483 - precision_m: 0.2607 - recall_m: 0.8930 - f1_m: 0.3980 - val_loss: 0.9004 - val_accuracy: 0.6679 - val_precision_m: 0.0894 - val_recall_m: 0.8090 - val_f1_m: 0.1610\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.51620\n",
      "Epoch 23/100\n",
      "16/16 - 1s - loss: 0.7632 - accuracy: 0.8377 - precision_m: 0.2453 - recall_m: 0.8806 - f1_m: 0.3803 - val_loss: 0.8565 - val_accuracy: 0.8486 - val_precision_m: 0.2087 - val_recall_m: 0.6611 - val_f1_m: 0.3069\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.51620\n",
      "Epoch 24/100\n",
      "16/16 - 1s - loss: 0.7606 - accuracy: 0.8447 - precision_m: 0.2498 - recall_m: 0.8578 - f1_m: 0.3820 - val_loss: 0.8865 - val_accuracy: 0.7630 - val_precision_m: 0.1150 - val_recall_m: 0.7062 - val_f1_m: 0.1975\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.51620\n",
      "Epoch 25/100\n",
      "16/16 - 1s - loss: 0.7559 - accuracy: 0.8590 - precision_m: 0.2627 - recall_m: 0.8477 - f1_m: 0.3942 - val_loss: 0.8794 - val_accuracy: 0.7950 - val_precision_m: 0.1446 - val_recall_m: 0.6697 - val_f1_m: 0.2357\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.51620\n",
      "Epoch 26/100\n",
      "16/16 - 1s - loss: 0.7556 - accuracy: 0.8486 - precision_m: 0.2505 - recall_m: 0.8735 - f1_m: 0.3845 - val_loss: 0.9374 - val_accuracy: 0.9338 - val_precision_m: 0.5103 - val_recall_m: 0.0589 - val_f1_m: 0.0519\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.51620\n",
      "Epoch 27/100\n",
      "16/16 - 1s - loss: 0.7390 - accuracy: 0.8744 - precision_m: 0.2947 - recall_m: 0.8476 - f1_m: 0.4314 - val_loss: 0.9550 - val_accuracy: 0.9389 - val_precision_m: 0.3333 - val_recall_m: 1.5083e-04 - val_f1_m: 3.0152e-04\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.51620\n",
      "Epoch 28/100\n",
      "16/16 - 1s - loss: 0.7352 - accuracy: 0.8871 - precision_m: 0.3127 - recall_m: 0.8398 - f1_m: 0.4478 - val_loss: 0.9463 - val_accuracy: 0.9617 - val_precision_m: 0.3895 - val_recall_m: 0.0178 - val_f1_m: 0.0341\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.51620\n",
      "Epoch 29/100\n",
      "16/16 - 1s - loss: 0.7201 - accuracy: 0.8980 - precision_m: 0.3384 - recall_m: 0.8226 - f1_m: 0.4729 - val_loss: 0.9532 - val_accuracy: 0.9363 - val_precision_m: 0.0126 - val_recall_m: 0.0132 - val_f1_m: 0.0129\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.51620\n",
      "Epoch 30/100\n",
      "16/16 - 1s - loss: 0.7186 - accuracy: 0.8975 - precision_m: 0.3336 - recall_m: 0.8371 - f1_m: 0.4703 - val_loss: 0.9376 - val_accuracy: 0.9613 - val_precision_m: 0.2718 - val_recall_m: 0.0337 - val_f1_m: 0.0599\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.51620\n",
      "Epoch 31/100\n",
      "16/16 - 1s - loss: 0.7087 - accuracy: 0.9125 - precision_m: 0.3740 - recall_m: 0.8123 - f1_m: 0.5012 - val_loss: 0.9513 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.51620\n",
      "Epoch 32/100\n",
      "16/16 - 1s - loss: 0.7175 - accuracy: 0.9056 - precision_m: 0.3433 - recall_m: 0.7926 - f1_m: 0.4700 - val_loss: 0.9436 - val_accuracy: 0.9608 - val_precision_m: 0.2149 - val_recall_m: 0.0156 - val_f1_m: 0.0291\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.51620\n",
      "Epoch 33/100\n",
      "16/16 - 1s - loss: 0.6998 - accuracy: 0.9128 - precision_m: 0.3705 - recall_m: 0.8174 - f1_m: 0.5020 - val_loss: 0.9528 - val_accuracy: 0.9610 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.51620\n",
      "Epoch 34/100\n",
      "16/16 - 1s - loss: 0.6900 - accuracy: 0.9182 - precision_m: 0.3895 - recall_m: 0.8233 - f1_m: 0.5200 - val_loss: 0.9500 - val_accuracy: 0.9614 - val_precision_m: 0.4439 - val_recall_m: 0.0073 - val_f1_m: 0.0144\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.51620\n",
      "Epoch 35/100\n",
      "16/16 - 1s - loss: 0.6796 - accuracy: 0.9252 - precision_m: 0.4105 - recall_m: 0.8439 - f1_m: 0.5406 - val_loss: 0.9510 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.51620\n",
      "Epoch 36/100\n",
      "16/16 - 1s - loss: 0.6682 - accuracy: 0.9346 - precision_m: 0.4544 - recall_m: 0.8083 - f1_m: 0.5752 - val_loss: 0.9457 - val_accuracy: 0.9614 - val_precision_m: 0.3606 - val_recall_m: 0.0095 - val_f1_m: 0.0185\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.51620\n",
      "Epoch 37/100\n",
      "16/16 - 1s - loss: 0.6636 - accuracy: 0.9345 - precision_m: 0.4505 - recall_m: 0.8191 - f1_m: 0.5702 - val_loss: 0.9484 - val_accuracy: 0.9613 - val_precision_m: 0.5000 - val_recall_m: 0.0047 - val_f1_m: 0.0094\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.51620\n",
      "Epoch 38/100\n",
      "16/16 - 1s - loss: 0.6450 - accuracy: 0.9434 - precision_m: 0.4923 - recall_m: 0.8293 - f1_m: 0.6067 - val_loss: 0.9449 - val_accuracy: 0.9615 - val_precision_m: 0.3743 - val_recall_m: 0.0114 - val_f1_m: 0.0222\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.51620\n",
      "Epoch 39/100\n",
      "16/16 - 1s - loss: 0.6273 - accuracy: 0.9495 - precision_m: 0.5304 - recall_m: 0.8275 - f1_m: 0.6376 - val_loss: 0.9485 - val_accuracy: 0.9613 - val_precision_m: 0.4716 - val_recall_m: 0.0056 - val_f1_m: 0.0111\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.51620\n",
      "Epoch 40/100\n",
      "16/16 - 1s - loss: 0.6127 - accuracy: 0.9560 - precision_m: 0.5697 - recall_m: 0.8549 - f1_m: 0.6739 - val_loss: 0.9510 - val_accuracy: 0.9611 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.51620\n",
      "Epoch 41/100\n",
      "16/16 - 1s - loss: 0.6083 - accuracy: 0.9553 - precision_m: 0.5689 - recall_m: 0.8520 - f1_m: 0.6712 - val_loss: 0.9509 - val_accuracy: 0.9611 - val_precision_m: 0.5000 - val_recall_m: 3.7927e-04 - val_f1_m: 7.5796e-04\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.51620\n",
      "Epoch 42/100\n",
      "16/16 - 1s - loss: 0.6153 - accuracy: 0.9547 - precision_m: 0.5684 - recall_m: 0.8111 - f1_m: 0.6615 - val_loss: 0.9499 - val_accuracy: 0.9613 - val_precision_m: 0.4919 - val_recall_m: 0.0051 - val_f1_m: 0.0101\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.51620\n",
      "Epoch 43/100\n",
      "16/16 - 1s - loss: 0.6182 - accuracy: 0.9509 - precision_m: 0.5477 - recall_m: 0.7902 - f1_m: 0.6375 - val_loss: 0.9535 - val_accuracy: 0.9611 - val_precision_m: 0.1667 - val_recall_m: 8.4281e-05 - val_f1_m: 1.6848e-04\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.51620\n",
      "Epoch 44/100\n",
      "16/16 - 1s - loss: 0.5978 - accuracy: 0.9588 - precision_m: 0.5945 - recall_m: 0.8312 - f1_m: 0.6874 - val_loss: 0.9523 - val_accuracy: 0.9584 - val_precision_m: 0.0375 - val_recall_m: 0.0047 - val_f1_m: 0.0083\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.51620\n",
      "Epoch 45/100\n",
      "16/16 - 1s - loss: 0.5823 - accuracy: 0.9644 - precision_m: 0.6370 - recall_m: 0.8444 - f1_m: 0.7200 - val_loss: 0.9613 - val_accuracy: 0.9363 - val_precision_m: 0.0038 - val_recall_m: 0.0038 - val_f1_m: 0.0038\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.51620\n",
      "Epoch 46/100\n",
      "16/16 - 1s - loss: 0.5849 - accuracy: 0.9610 - precision_m: 0.6158 - recall_m: 0.8410 - f1_m: 0.7032 - val_loss: 0.9560 - val_accuracy: 0.9466 - val_precision_m: 0.0077 - val_recall_m: 0.0046 - val_f1_m: 0.0058\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.51620\n",
      "Epoch 47/100\n",
      "16/16 - 1s - loss: 0.5783 - accuracy: 0.9638 - precision_m: 0.6253 - recall_m: 0.8650 - f1_m: 0.7136 - val_loss: 0.9522 - val_accuracy: 0.9482 - val_precision_m: 0.0188 - val_recall_m: 0.0105 - val_f1_m: 0.0135\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.51620\n",
      "Epoch 48/100\n",
      "16/16 - 1s - loss: 0.5641 - accuracy: 0.9682 - precision_m: 0.6734 - recall_m: 0.8537 - f1_m: 0.7448 - val_loss: 0.9550 - val_accuracy: 0.9485 - val_precision_m: 9.9834e-04 - val_recall_m: 5.0569e-04 - val_f1_m: 6.7131e-04\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.51620\n",
      "Epoch 49/100\n",
      "16/16 - 1s - loss: 0.5637 - accuracy: 0.9692 - precision_m: 0.6810 - recall_m: 0.8431 - f1_m: 0.7434 - val_loss: 0.9247 - val_accuracy: 0.9615 - val_precision_m: 0.6529 - val_recall_m: 0.0389 - val_f1_m: 0.0734\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.51620\n",
      "Epoch 50/100\n",
      "16/16 - 1s - loss: 0.5591 - accuracy: 0.9691 - precision_m: 0.6780 - recall_m: 0.8561 - f1_m: 0.7464 - val_loss: 0.9411 - val_accuracy: 0.9538 - val_precision_m: 0.4843 - val_recall_m: 0.0176 - val_f1_m: 0.0330\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.51620\n",
      "Epoch 51/100\n",
      "16/16 - 1s - loss: 0.5537 - accuracy: 0.9684 - precision_m: 0.6693 - recall_m: 0.8619 - f1_m: 0.7484 - val_loss: 0.9505 - val_accuracy: 0.9520 - val_precision_m: 0.0150 - val_recall_m: 0.0058 - val_f1_m: 0.0084\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.51620\n",
      "Epoch 52/100\n",
      "16/16 - 1s - loss: 0.5464 - accuracy: 0.9727 - precision_m: 0.7087 - recall_m: 0.8647 - f1_m: 0.7705 - val_loss: 0.9492 - val_accuracy: 0.9488 - val_precision_m: 0.5141 - val_recall_m: 0.0076 - val_f1_m: 0.0101\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.51620\n",
      "Epoch 53/100\n",
      "16/16 - 1s - loss: 0.5534 - accuracy: 0.9699 - precision_m: 0.6948 - recall_m: 0.8232 - f1_m: 0.7499 - val_loss: 0.9416 - val_accuracy: 0.9460 - val_precision_m: 0.4856 - val_recall_m: 0.0242 - val_f1_m: 0.0403\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.51620\n",
      "Epoch 54/100\n",
      "16/16 - 1s - loss: 0.5385 - accuracy: 0.9720 - precision_m: 0.7004 - recall_m: 0.8674 - f1_m: 0.7675 - val_loss: 0.9122 - val_accuracy: 0.9565 - val_precision_m: 0.4966 - val_recall_m: 0.0590 - val_f1_m: 0.1010\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.51620\n",
      "Epoch 55/100\n",
      "16/16 - 1s - loss: 0.5378 - accuracy: 0.9733 - precision_m: 0.7102 - recall_m: 0.8780 - f1_m: 0.7770 - val_loss: 0.9270 - val_accuracy: 0.9520 - val_precision_m: 0.4758 - val_recall_m: 0.0423 - val_f1_m: 0.0710\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.51620\n",
      "Epoch 56/100\n",
      "16/16 - 1s - loss: 0.5349 - accuracy: 0.9749 - precision_m: 0.7427 - recall_m: 0.8703 - f1_m: 0.7949 - val_loss: 0.9208 - val_accuracy: 0.9567 - val_precision_m: 0.4996 - val_recall_m: 0.0484 - val_f1_m: 0.0843\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.51620\n",
      "Epoch 57/100\n",
      "16/16 - 1s - loss: 0.5197 - accuracy: 0.9778 - precision_m: 0.7571 - recall_m: 0.8848 - f1_m: 0.8096 - val_loss: 0.8768 - val_accuracy: 0.9627 - val_precision_m: 0.6529 - val_recall_m: 0.1094 - val_f1_m: 0.1870\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.51620\n",
      "Epoch 58/100\n",
      "16/16 - 1s - loss: 0.5224 - accuracy: 0.9768 - precision_m: 0.7516 - recall_m: 0.8714 - f1_m: 0.8029 - val_loss: 0.8846 - val_accuracy: 0.9612 - val_precision_m: 0.6104 - val_recall_m: 0.0971 - val_f1_m: 0.1666\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.51620\n",
      "Epoch 59/100\n",
      "16/16 - 1s - loss: 0.5104 - accuracy: 0.9788 - precision_m: 0.7704 - recall_m: 0.8904 - f1_m: 0.8210 - val_loss: 0.8320 - val_accuracy: 0.9634 - val_precision_m: 0.6493 - val_recall_m: 0.2024 - val_f1_m: 0.3086\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.51620\n",
      "Epoch 60/100\n",
      "16/16 - 1s - loss: 0.5182 - accuracy: 0.9781 - precision_m: 0.7503 - recall_m: 0.8928 - f1_m: 0.8058 - val_loss: 0.8014 - val_accuracy: 0.9628 - val_precision_m: 0.5721 - val_recall_m: 0.2965 - val_f1_m: 0.3902\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.51620\n",
      "Epoch 61/100\n",
      "16/16 - 1s - loss: 0.5146 - accuracy: 0.9802 - precision_m: 0.7834 - recall_m: 0.8859 - f1_m: 0.8253 - val_loss: 0.8014 - val_accuracy: 0.9630 - val_precision_m: 0.5823 - val_recall_m: 0.2886 - val_f1_m: 0.3858\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.51620\n",
      "Epoch 62/100\n",
      "16/16 - 1s - loss: 0.5180 - accuracy: 0.9785 - precision_m: 0.7579 - recall_m: 0.8878 - f1_m: 0.8102 - val_loss: 0.8371 - val_accuracy: 0.9628 - val_precision_m: 0.6290 - val_recall_m: 0.1922 - val_f1_m: 0.2943\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.51620\n",
      "Epoch 63/100\n",
      "16/16 - 1s - loss: 0.5329 - accuracy: 0.9764 - precision_m: 0.7444 - recall_m: 0.8643 - f1_m: 0.7877 - val_loss: 0.8112 - val_accuracy: 0.9630 - val_precision_m: 0.6099 - val_recall_m: 0.2558 - val_f1_m: 0.3602\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.51620\n",
      "Epoch 64/100\n",
      "16/16 - 1s - loss: 0.5155 - accuracy: 0.9778 - precision_m: 0.7506 - recall_m: 0.8896 - f1_m: 0.8044 - val_loss: 0.8058 - val_accuracy: 0.9632 - val_precision_m: 0.6129 - val_recall_m: 0.2692 - val_f1_m: 0.3740\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.51620\n",
      "Epoch 65/100\n",
      "16/16 - 1s - loss: 0.5195 - accuracy: 0.9791 - precision_m: 0.7663 - recall_m: 0.8825 - f1_m: 0.8123 - val_loss: 0.8079 - val_accuracy: 0.9631 - val_precision_m: 0.6010 - val_recall_m: 0.2709 - val_f1_m: 0.3734\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.51620\n",
      "Epoch 66/100\n",
      "16/16 - 1s - loss: 0.5053 - accuracy: 0.9811 - precision_m: 0.7892 - recall_m: 0.8926 - f1_m: 0.8336 - val_loss: 0.7905 - val_accuracy: 0.9630 - val_precision_m: 0.5773 - val_recall_m: 0.3272 - val_f1_m: 0.4170\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.51620\n",
      "Epoch 67/100\n",
      "16/16 - 1s - loss: 0.5182 - accuracy: 0.9776 - precision_m: 0.7568 - recall_m: 0.8751 - f1_m: 0.8041 - val_loss: 0.7997 - val_accuracy: 0.9628 - val_precision_m: 0.5643 - val_recall_m: 0.3000 - val_f1_m: 0.3903\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.51620\n",
      "Epoch 68/100\n",
      "16/16 - 1s - loss: 0.5200 - accuracy: 0.9779 - precision_m: 0.7672 - recall_m: 0.8625 - f1_m: 0.8051 - val_loss: 0.8006 - val_accuracy: 0.9638 - val_precision_m: 0.6265 - val_recall_m: 0.2858 - val_f1_m: 0.3925\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.51620\n",
      "Epoch 69/100\n",
      "16/16 - 1s - loss: 0.5142 - accuracy: 0.9758 - precision_m: 0.7378 - recall_m: 0.8852 - f1_m: 0.8012 - val_loss: 0.7983 - val_accuracy: 0.9632 - val_precision_m: 0.5827 - val_recall_m: 0.3041 - val_f1_m: 0.3986\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.51620\n",
      "Epoch 70/100\n",
      "16/16 - 1s - loss: 0.4987 - accuracy: 0.9818 - precision_m: 0.7925 - recall_m: 0.8926 - f1_m: 0.8352 - val_loss: 0.7946 - val_accuracy: 0.9631 - val_precision_m: 0.5949 - val_recall_m: 0.3051 - val_f1_m: 0.4033\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.51620\n",
      "Epoch 71/100\n",
      "16/16 - 1s - loss: 0.4965 - accuracy: 0.9822 - precision_m: 0.8024 - recall_m: 0.8947 - f1_m: 0.8413 - val_loss: 0.8067 - val_accuracy: 0.9632 - val_precision_m: 0.5945 - val_recall_m: 0.2717 - val_f1_m: 0.3725\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.51620\n",
      "Epoch 72/100\n",
      "16/16 - 1s - loss: 0.5124 - accuracy: 0.9787 - precision_m: 0.7580 - recall_m: 0.8918 - f1_m: 0.8079 - val_loss: 0.7859 - val_accuracy: 0.9617 - val_precision_m: 0.5365 - val_recall_m: 0.3520 - val_f1_m: 0.4241\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.51620\n",
      "Epoch 73/100\n",
      "16/16 - 1s - loss: 0.4985 - accuracy: 0.9804 - precision_m: 0.7770 - recall_m: 0.8995 - f1_m: 0.8276 - val_loss: 0.7913 - val_accuracy: 0.9629 - val_precision_m: 0.5832 - val_recall_m: 0.3166 - val_f1_m: 0.4102\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.51620\n",
      "Epoch 74/100\n",
      "16/16 - 1s - loss: 0.4989 - accuracy: 0.9821 - precision_m: 0.7921 - recall_m: 0.8966 - f1_m: 0.8345 - val_loss: 0.7960 - val_accuracy: 0.9632 - val_precision_m: 0.5999 - val_recall_m: 0.3032 - val_f1_m: 0.4027\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.51620\n",
      "Epoch 75/100\n",
      "16/16 - 1s - loss: 0.4904 - accuracy: 0.9833 - precision_m: 0.8173 - recall_m: 0.8948 - f1_m: 0.8509 - val_loss: 0.7843 - val_accuracy: 0.9625 - val_precision_m: 0.5550 - val_recall_m: 0.3417 - val_f1_m: 0.4217\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.51620\n",
      "Epoch 76/100\n",
      "16/16 - 1s - loss: 0.4916 - accuracy: 0.9826 - precision_m: 0.8064 - recall_m: 0.8979 - f1_m: 0.8446 - val_loss: 0.7712 - val_accuracy: 0.9605 - val_precision_m: 0.5257 - val_recall_m: 0.3924 - val_f1_m: 0.4493\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.51620\n",
      "Epoch 77/100\n",
      "16/16 - 1s - loss: 0.4896 - accuracy: 0.9823 - precision_m: 0.7935 - recall_m: 0.9035 - f1_m: 0.8385 - val_loss: 0.7690 - val_accuracy: 0.9604 - val_precision_m: 0.5174 - val_recall_m: 0.3958 - val_f1_m: 0.4484\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.51620\n",
      "Epoch 78/100\n",
      "16/16 - 1s - loss: 0.4791 - accuracy: 0.9839 - precision_m: 0.8192 - recall_m: 0.9102 - f1_m: 0.8600 - val_loss: 0.7776 - val_accuracy: 0.9624 - val_precision_m: 0.5615 - val_recall_m: 0.3544 - val_f1_m: 0.4343\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.51620\n",
      "Epoch 79/100\n",
      "16/16 - 1s - loss: 0.4867 - accuracy: 0.9836 - precision_m: 0.8182 - recall_m: 0.8996 - f1_m: 0.8529 - val_loss: 0.7904 - val_accuracy: 0.9620 - val_precision_m: 0.5454 - val_recall_m: 0.3249 - val_f1_m: 0.4063\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.51620\n",
      "Epoch 80/100\n",
      "16/16 - 1s - loss: 0.4901 - accuracy: 0.9830 - precision_m: 0.8214 - recall_m: 0.8903 - f1_m: 0.8514 - val_loss: 0.8050 - val_accuracy: 0.9633 - val_precision_m: 0.6149 - val_recall_m: 0.2647 - val_f1_m: 0.3701\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.51620\n",
      "Epoch 81/100\n",
      "16/16 - 1s - loss: 0.4901 - accuracy: 0.9820 - precision_m: 0.7893 - recall_m: 0.9033 - f1_m: 0.8377 - val_loss: 0.7847 - val_accuracy: 0.9631 - val_precision_m: 0.5791 - val_recall_m: 0.3308 - val_f1_m: 0.4205\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.51620\n",
      "Epoch 82/100\n",
      "16/16 - 1s - loss: 0.5021 - accuracy: 0.9804 - precision_m: 0.7891 - recall_m: 0.8830 - f1_m: 0.8223 - val_loss: 0.7942 - val_accuracy: 0.9625 - val_precision_m: 0.5553 - val_recall_m: 0.3079 - val_f1_m: 0.3947\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.51620\n",
      "Epoch 83/100\n",
      "16/16 - 1s - loss: 0.4903 - accuracy: 0.9824 - precision_m: 0.8055 - recall_m: 0.8989 - f1_m: 0.8430 - val_loss: 0.7888 - val_accuracy: 0.9630 - val_precision_m: 0.5822 - val_recall_m: 0.3184 - val_f1_m: 0.4113\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.51620\n",
      "Epoch 84/100\n",
      "16/16 - 1s - loss: 0.4839 - accuracy: 0.9833 - precision_m: 0.8118 - recall_m: 0.9000 - f1_m: 0.8499 - val_loss: 0.7787 - val_accuracy: 0.9627 - val_precision_m: 0.5623 - val_recall_m: 0.3495 - val_f1_m: 0.4303\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.51620\n",
      "Epoch 85/100\n",
      "16/16 - 1s - loss: 0.4851 - accuracy: 0.9844 - precision_m: 0.8271 - recall_m: 0.9031 - f1_m: 0.8590 - val_loss: 0.7797 - val_accuracy: 0.9618 - val_precision_m: 0.5493 - val_recall_m: 0.3533 - val_f1_m: 0.4298\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.51620\n",
      "Epoch 86/100\n",
      "16/16 - 1s - loss: 0.4804 - accuracy: 0.9838 - precision_m: 0.8134 - recall_m: 0.9110 - f1_m: 0.8550 - val_loss: 0.7927 - val_accuracy: 0.9630 - val_precision_m: 0.5788 - val_recall_m: 0.3122 - val_f1_m: 0.4045\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.51620\n",
      "Epoch 87/100\n",
      "16/16 - 1s - loss: 0.4789 - accuracy: 0.9848 - precision_m: 0.8276 - recall_m: 0.9063 - f1_m: 0.8606 - val_loss: 0.7877 - val_accuracy: 0.9627 - val_precision_m: 0.5602 - val_recall_m: 0.3327 - val_f1_m: 0.4160\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.51620\n",
      "Epoch 88/100\n",
      "16/16 - 1s - loss: 0.4926 - accuracy: 0.9818 - precision_m: 0.7902 - recall_m: 0.9006 - f1_m: 0.8307 - val_loss: 0.7770 - val_accuracy: 0.9627 - val_precision_m: 0.5694 - val_recall_m: 0.3606 - val_f1_m: 0.4410\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.51620\n",
      "Epoch 89/100\n",
      "16/16 - 1s - loss: 0.4724 - accuracy: 0.9851 - precision_m: 0.8419 - recall_m: 0.9069 - f1_m: 0.8707 - val_loss: 0.7723 - val_accuracy: 0.9620 - val_precision_m: 0.5478 - val_recall_m: 0.3704 - val_f1_m: 0.4414\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.51620\n",
      "Epoch 90/100\n",
      "16/16 - 1s - loss: 0.4752 - accuracy: 0.9836 - precision_m: 0.8154 - recall_m: 0.9080 - f1_m: 0.8560 - val_loss: 0.7690 - val_accuracy: 0.9603 - val_precision_m: 0.5163 - val_recall_m: 0.3983 - val_f1_m: 0.4493\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.51620\n",
      "Epoch 91/100\n",
      "16/16 - 1s - loss: 0.4802 - accuracy: 0.9850 - precision_m: 0.8332 - recall_m: 0.8927 - f1_m: 0.8560 - val_loss: 0.7615 - val_accuracy: 0.9568 - val_precision_m: 0.4640 - val_recall_m: 0.4445 - val_f1_m: 0.4539\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.51620\n",
      "Epoch 92/100\n",
      "16/16 - 1s - loss: 0.4864 - accuracy: 0.9821 - precision_m: 0.7932 - recall_m: 0.9033 - f1_m: 0.8359 - val_loss: 0.7687 - val_accuracy: 0.9609 - val_precision_m: 0.5298 - val_recall_m: 0.3940 - val_f1_m: 0.4518\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.51620\n",
      "Epoch 93/100\n",
      "16/16 - 1s - loss: 0.4753 - accuracy: 0.9847 - precision_m: 0.8151 - recall_m: 0.9132 - f1_m: 0.8561 - val_loss: 0.7737 - val_accuracy: 0.9600 - val_precision_m: 0.5188 - val_recall_m: 0.3840 - val_f1_m: 0.4412\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.51620\n",
      "Epoch 94/100\n",
      "16/16 - 1s - loss: 0.4701 - accuracy: 0.9853 - precision_m: 0.8430 - recall_m: 0.9063 - f1_m: 0.8714 - val_loss: 0.7808 - val_accuracy: 0.9635 - val_precision_m: 0.6151 - val_recall_m: 0.3330 - val_f1_m: 0.4321\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.51620\n",
      "Epoch 95/100\n",
      "16/16 - 1s - loss: 0.4693 - accuracy: 0.9852 - precision_m: 0.8422 - recall_m: 0.8998 - f1_m: 0.8679 - val_loss: 0.7760 - val_accuracy: 0.9615 - val_precision_m: 0.5445 - val_recall_m: 0.3650 - val_f1_m: 0.4368\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.51620\n",
      "Epoch 96/100\n",
      "16/16 - 1s - loss: 0.4949 - accuracy: 0.9811 - precision_m: 0.7898 - recall_m: 0.8823 - f1_m: 0.8256 - val_loss: 0.7655 - val_accuracy: 0.9575 - val_precision_m: 0.4759 - val_recall_m: 0.4191 - val_f1_m: 0.4455\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.51620\n",
      "Epoch 97/100\n",
      "16/16 - 1s - loss: 0.4783 - accuracy: 0.9831 - precision_m: 0.8029 - recall_m: 0.9064 - f1_m: 0.8463 - val_loss: 0.7732 - val_accuracy: 0.9626 - val_precision_m: 0.5853 - val_recall_m: 0.3538 - val_f1_m: 0.4410\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.51620\n",
      "Epoch 98/100\n",
      "16/16 - 1s - loss: 0.4748 - accuracy: 0.9842 - precision_m: 0.8179 - recall_m: 0.9079 - f1_m: 0.8564 - val_loss: 0.7613 - val_accuracy: 0.9595 - val_precision_m: 0.5124 - val_recall_m: 0.4192 - val_f1_m: 0.4611\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.51620\n",
      "Epoch 99/100\n",
      "16/16 - 1s - loss: 0.4678 - accuracy: 0.9843 - precision_m: 0.8298 - recall_m: 0.9073 - f1_m: 0.8639 - val_loss: 0.7734 - val_accuracy: 0.9633 - val_precision_m: 0.5922 - val_recall_m: 0.3566 - val_f1_m: 0.4450\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.51620\n",
      "Epoch 100/100\n",
      "16/16 - 1s - loss: 0.4585 - accuracy: 0.9867 - precision_m: 0.8553 - recall_m: 0.9122 - f1_m: 0.8804 - val_loss: 0.7684 - val_accuracy: 0.9616 - val_precision_m: 0.5491 - val_recall_m: 0.3760 - val_f1_m: 0.4463\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.51620\n",
      "5/5 - 0s - loss: 0.6019 - accuracy: 0.9584 - precision_m: 0.6673 - recall_m: 0.5612 - f1_m: 0.6055\n",
      "The metrics for the test set with loss tversky, learning rate 0.0001,  filters 32, and batch size 16 is [0.6018820405006409, 0.95841383934021, 0.6672850847244263, 0.5612219572067261, 0.6054551005363464]\n",
      "__________________________________________________________________________________________________________________________________________________________________________\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 128, 128, 5) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 128, 128, 8)  368         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 128, 128, 8)  32          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 128, 128, 8)  0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 8)  584         activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 8)  32          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, 128, 8)  0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 64, 64, 8)    0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 16)   1168        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64, 64, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 64, 64, 16)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 64, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64, 64, 16)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 16)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 32)   4640        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 32)   128         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 32)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 32)   9248        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 32)   128         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 32)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 32)   0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 16, 16, 64)   18496       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 16, 16, 64)   256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 16, 16, 64)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 16, 16, 64)   36928       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 16, 16, 64)   256         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16, 16, 64)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 64)     0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 8, 8, 128)    73856       max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 8, 8, 128)    512         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 8, 8, 128)    0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 8, 8, 128)    147584      activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 8, 8, 128)    512         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 8, 8, 128)    0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 16, 16, 128)  65664       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 16, 16, 192)  0           conv2d_transpose[0][0]           \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 64)   110656      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 64)   256         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 64)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 64)   36928       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 64)   256         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 64)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 32, 32, 64)   16448       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 96)   0           conv2d_transpose_1[0][0]         \n",
      "                                                                 activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 32)   27680       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 32, 32, 32)   128         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 32, 32, 32)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 32, 32)   9248        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 32, 32, 32)   128         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 32, 32, 32)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 64, 64, 32)   4128        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 64, 64, 48)   0           conv2d_transpose_2[0][0]         \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 16)   6928        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64, 64, 16)   64          conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 64, 64, 16)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 16)   2320        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 64, 64, 16)   64          conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 64, 64, 16)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 128, 128, 16) 1040        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 128, 128, 24) 0           conv2d_transpose_3[0][0]         \n",
      "                                                                 activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 128, 128, 8)  1736        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 128, 128, 8)  32          conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 128, 128, 8)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 128, 128, 8)  584         activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 128, 128, 8)  32          conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 128, 128, 8)  0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 128, 128, 1)  9           activation_17[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 581,505\n",
      "Trainable params: 580,033\n",
      "Non-trainable params: 1,472\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "Initialising in 3..........2..........1......................................\n",
      "Training on 284 images with learning rate 1e-05, batch size 8, and number of filters 8\n",
      "Epoch 1/100\n",
      "32/32 - 3s - loss: 0.9256 - accuracy: 0.1125 - precision_m: 0.0572 - recall_m: 0.9544 - f1_m: 0.1062 - val_loss: 0.9476 - val_accuracy: 0.0715 - val_precision_m: 0.0387 - val_recall_m: 0.9811 - val_f1_m: 0.0743\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.51620\n",
      "Epoch 2/100\n",
      "32/32 - 1s - loss: 0.9252 - accuracy: 0.1106 - precision_m: 0.0577 - recall_m: 0.9599 - f1_m: 0.1074 - val_loss: 0.9473 - val_accuracy: 0.0617 - val_precision_m: 0.0386 - val_recall_m: 0.9859 - val_f1_m: 0.0740\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.51620\n",
      "Epoch 3/100\n",
      "32/32 - 1s - loss: 0.9245 - accuracy: 0.1089 - precision_m: 0.0574 - recall_m: 0.9604 - f1_m: 0.1075 - val_loss: 0.9471 - val_accuracy: 0.0591 - val_precision_m: 0.0386 - val_recall_m: 0.9879 - val_f1_m: 0.0741\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.51620\n",
      "Epoch 4/100\n",
      "32/32 - 1s - loss: 0.9244 - accuracy: 0.1077 - precision_m: 0.0576 - recall_m: 0.9643 - f1_m: 0.1074 - val_loss: 0.9469 - val_accuracy: 0.0592 - val_precision_m: 0.0386 - val_recall_m: 0.9874 - val_f1_m: 0.0741\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.51620\n",
      "Epoch 5/100\n",
      "32/32 - 1s - loss: 0.9240 - accuracy: 0.1062 - precision_m: 0.0575 - recall_m: 0.9661 - f1_m: 0.1074 - val_loss: 0.9467 - val_accuracy: 0.0611 - val_precision_m: 0.0386 - val_recall_m: 0.9861 - val_f1_m: 0.0741\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.51620\n",
      "Epoch 6/100\n",
      "32/32 - 1s - loss: 0.9237 - accuracy: 0.1060 - precision_m: 0.0579 - recall_m: 0.9699 - f1_m: 0.1079 - val_loss: 0.9465 - val_accuracy: 0.0618 - val_precision_m: 0.0386 - val_recall_m: 0.9848 - val_f1_m: 0.0741\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.51620\n",
      "Epoch 7/100\n",
      "32/32 - 1s - loss: 0.9234 - accuracy: 0.1044 - precision_m: 0.0576 - recall_m: 0.9718 - f1_m: 0.1073 - val_loss: 0.9464 - val_accuracy: 0.0617 - val_precision_m: 0.0386 - val_recall_m: 0.9844 - val_f1_m: 0.0740\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.51620\n",
      "Epoch 8/100\n",
      "32/32 - 1s - loss: 0.9229 - accuracy: 0.1033 - precision_m: 0.0578 - recall_m: 0.9742 - f1_m: 0.1080 - val_loss: 0.9462 - val_accuracy: 0.0615 - val_precision_m: 0.0385 - val_recall_m: 0.9845 - val_f1_m: 0.0740\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.51620\n",
      "Epoch 9/100\n",
      "32/32 - 1s - loss: 0.9223 - accuracy: 0.1037 - precision_m: 0.0581 - recall_m: 0.9764 - f1_m: 0.1088 - val_loss: 0.9460 - val_accuracy: 0.0615 - val_precision_m: 0.0386 - val_recall_m: 0.9850 - val_f1_m: 0.0740\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.51620\n",
      "Epoch 10/100\n",
      "32/32 - 1s - loss: 0.9222 - accuracy: 0.1036 - precision_m: 0.0581 - recall_m: 0.9793 - f1_m: 0.1085 - val_loss: 0.9458 - val_accuracy: 0.0625 - val_precision_m: 0.0386 - val_recall_m: 0.9848 - val_f1_m: 0.0741\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.51620\n",
      "Epoch 11/100\n",
      "32/32 - 1s - loss: 0.9213 - accuracy: 0.1051 - precision_m: 0.0582 - recall_m: 0.9787 - f1_m: 0.1089 - val_loss: 0.9456 - val_accuracy: 0.0630 - val_precision_m: 0.0386 - val_recall_m: 0.9845 - val_f1_m: 0.0741\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.51620\n",
      "Epoch 12/100\n",
      "32/32 - 1s - loss: 0.9212 - accuracy: 0.1052 - precision_m: 0.0581 - recall_m: 0.9763 - f1_m: 0.1083 - val_loss: 0.9454 - val_accuracy: 0.0639 - val_precision_m: 0.0386 - val_recall_m: 0.9833 - val_f1_m: 0.0741\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.51620\n",
      "Epoch 13/100\n",
      "32/32 - 1s - loss: 0.9206 - accuracy: 0.1081 - precision_m: 0.0583 - recall_m: 0.9776 - f1_m: 0.1086 - val_loss: 0.9451 - val_accuracy: 0.0642 - val_precision_m: 0.0386 - val_recall_m: 0.9832 - val_f1_m: 0.0741\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.51620\n",
      "Epoch 14/100\n",
      "32/32 - 1s - loss: 0.9200 - accuracy: 0.1092 - precision_m: 0.0583 - recall_m: 0.9777 - f1_m: 0.1090 - val_loss: 0.9449 - val_accuracy: 0.0649 - val_precision_m: 0.0386 - val_recall_m: 0.9832 - val_f1_m: 0.0741\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.51620\n",
      "Epoch 15/100\n",
      "32/32 - 1s - loss: 0.9198 - accuracy: 0.1117 - precision_m: 0.0584 - recall_m: 0.9733 - f1_m: 0.1084 - val_loss: 0.9447 - val_accuracy: 0.0646 - val_precision_m: 0.0387 - val_recall_m: 0.9852 - val_f1_m: 0.0743\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.51620\n",
      "Epoch 16/100\n",
      "32/32 - 1s - loss: 0.9191 - accuracy: 0.1124 - precision_m: 0.0585 - recall_m: 0.9731 - f1_m: 0.1091 - val_loss: 0.9445 - val_accuracy: 0.0650 - val_precision_m: 0.0388 - val_recall_m: 0.9864 - val_f1_m: 0.0744\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.51620\n",
      "Epoch 17/100\n",
      "32/32 - 1s - loss: 0.9181 - accuracy: 0.1152 - precision_m: 0.0585 - recall_m: 0.9759 - f1_m: 0.1099 - val_loss: 0.9442 - val_accuracy: 0.0669 - val_precision_m: 0.0388 - val_recall_m: 0.9856 - val_f1_m: 0.0745\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.51620\n",
      "Epoch 18/100\n",
      "32/32 - 1s - loss: 0.9179 - accuracy: 0.1174 - precision_m: 0.0586 - recall_m: 0.9726 - f1_m: 0.1094 - val_loss: 0.9439 - val_accuracy: 0.0683 - val_precision_m: 0.0389 - val_recall_m: 0.9864 - val_f1_m: 0.0746\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.51620\n",
      "Epoch 19/100\n",
      "32/32 - 1s - loss: 0.9173 - accuracy: 0.1207 - precision_m: 0.0588 - recall_m: 0.9717 - f1_m: 0.1099 - val_loss: 0.9437 - val_accuracy: 0.0699 - val_precision_m: 0.0390 - val_recall_m: 0.9863 - val_f1_m: 0.0748\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.51620\n",
      "Epoch 20/100\n",
      "32/32 - 1s - loss: 0.9166 - accuracy: 0.1246 - precision_m: 0.0591 - recall_m: 0.9678 - f1_m: 0.1102 - val_loss: 0.9435 - val_accuracy: 0.0713 - val_precision_m: 0.0390 - val_recall_m: 0.9859 - val_f1_m: 0.0748\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.51620\n",
      "Epoch 21/100\n",
      "32/32 - 1s - loss: 0.9163 - accuracy: 0.1270 - precision_m: 0.0592 - recall_m: 0.9683 - f1_m: 0.1104 - val_loss: 0.9434 - val_accuracy: 0.0766 - val_precision_m: 0.0391 - val_recall_m: 0.9835 - val_f1_m: 0.0751\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.51620\n",
      "Epoch 22/100\n",
      "32/32 - 1s - loss: 0.9159 - accuracy: 0.1311 - precision_m: 0.0594 - recall_m: 0.9648 - f1_m: 0.1103 - val_loss: 0.9431 - val_accuracy: 0.0782 - val_precision_m: 0.0392 - val_recall_m: 0.9824 - val_f1_m: 0.0751\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.51620\n",
      "Epoch 23/100\n",
      "32/32 - 1s - loss: 0.9146 - accuracy: 0.1366 - precision_m: 0.0596 - recall_m: 0.9649 - f1_m: 0.1116 - val_loss: 0.9430 - val_accuracy: 0.0830 - val_precision_m: 0.0393 - val_recall_m: 0.9802 - val_f1_m: 0.0754\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.51620\n",
      "Epoch 24/100\n",
      "32/32 - 1s - loss: 0.9141 - accuracy: 0.1428 - precision_m: 0.0599 - recall_m: 0.9603 - f1_m: 0.1119 - val_loss: 0.9429 - val_accuracy: 0.0876 - val_precision_m: 0.0394 - val_recall_m: 0.9781 - val_f1_m: 0.0756\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.51620\n",
      "Epoch 25/100\n",
      "32/32 - 1s - loss: 0.9139 - accuracy: 0.1477 - precision_m: 0.0599 - recall_m: 0.9589 - f1_m: 0.1111 - val_loss: 0.9425 - val_accuracy: 0.0904 - val_precision_m: 0.0395 - val_recall_m: 0.9770 - val_f1_m: 0.0757\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.51620\n",
      "Epoch 26/100\n",
      "32/32 - 1s - loss: 0.9131 - accuracy: 0.1543 - precision_m: 0.0600 - recall_m: 0.9555 - f1_m: 0.1118 - val_loss: 0.9425 - val_accuracy: 0.0965 - val_precision_m: 0.0396 - val_recall_m: 0.9727 - val_f1_m: 0.0759\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.51620\n",
      "Epoch 27/100\n",
      "32/32 - 1s - loss: 0.9126 - accuracy: 0.1567 - precision_m: 0.0602 - recall_m: 0.9549 - f1_m: 0.1121 - val_loss: 0.9423 - val_accuracy: 0.1016 - val_precision_m: 0.0398 - val_recall_m: 0.9707 - val_f1_m: 0.0762\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.51620\n",
      "Epoch 28/100\n",
      "32/32 - 1s - loss: 0.9118 - accuracy: 0.1650 - precision_m: 0.0605 - recall_m: 0.9529 - f1_m: 0.1129 - val_loss: 0.9421 - val_accuracy: 0.1024 - val_precision_m: 0.0398 - val_recall_m: 0.9701 - val_f1_m: 0.0763\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.51620\n",
      "Epoch 29/100\n",
      "32/32 - 1s - loss: 0.9111 - accuracy: 0.1719 - precision_m: 0.0610 - recall_m: 0.9500 - f1_m: 0.1134 - val_loss: 0.9419 - val_accuracy: 0.1171 - val_precision_m: 0.0401 - val_recall_m: 0.9627 - val_f1_m: 0.0768\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.51620\n",
      "Epoch 30/100\n",
      "32/32 - 1s - loss: 0.9099 - accuracy: 0.1778 - precision_m: 0.0615 - recall_m: 0.9532 - f1_m: 0.1144 - val_loss: 0.9419 - val_accuracy: 0.1234 - val_precision_m: 0.0402 - val_recall_m: 0.9576 - val_f1_m: 0.0770\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.51620\n",
      "Epoch 31/100\n",
      "32/32 - 1s - loss: 0.9093 - accuracy: 0.1899 - precision_m: 0.0624 - recall_m: 0.9520 - f1_m: 0.1160 - val_loss: 0.9418 - val_accuracy: 0.1274 - val_precision_m: 0.0403 - val_recall_m: 0.9560 - val_f1_m: 0.0772\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.51620\n",
      "Epoch 32/100\n",
      "32/32 - 1s - loss: 0.9093 - accuracy: 0.1933 - precision_m: 0.0625 - recall_m: 0.9466 - f1_m: 0.1153 - val_loss: 0.9414 - val_accuracy: 0.1354 - val_precision_m: 0.0406 - val_recall_m: 0.9523 - val_f1_m: 0.0776\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.51620\n",
      "Epoch 33/100\n",
      "32/32 - 1s - loss: 0.9083 - accuracy: 0.2115 - precision_m: 0.0631 - recall_m: 0.9368 - f1_m: 0.1170 - val_loss: 0.9412 - val_accuracy: 0.1441 - val_precision_m: 0.0407 - val_recall_m: 0.9462 - val_f1_m: 0.0778\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.51620\n",
      "Epoch 34/100\n",
      "32/32 - 1s - loss: 0.9080 - accuracy: 0.2217 - precision_m: 0.0641 - recall_m: 0.9359 - f1_m: 0.1185 - val_loss: 0.9406 - val_accuracy: 0.1616 - val_precision_m: 0.0412 - val_recall_m: 0.9389 - val_f1_m: 0.0787\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.51620\n",
      "Epoch 35/100\n",
      "32/32 - 1s - loss: 0.9075 - accuracy: 0.2265 - precision_m: 0.0644 - recall_m: 0.9357 - f1_m: 0.1187 - val_loss: 0.9408 - val_accuracy: 0.1596 - val_precision_m: 0.0413 - val_recall_m: 0.9415 - val_f1_m: 0.0788\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.51620\n",
      "Epoch 36/100\n",
      "32/32 - 1s - loss: 0.9059 - accuracy: 0.2478 - precision_m: 0.0659 - recall_m: 0.9355 - f1_m: 0.1218 - val_loss: 0.9401 - val_accuracy: 0.1674 - val_precision_m: 0.0416 - val_recall_m: 0.9399 - val_f1_m: 0.0794\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.51620\n",
      "Epoch 37/100\n",
      "32/32 - 1s - loss: 0.9057 - accuracy: 0.2584 - precision_m: 0.0661 - recall_m: 0.9235 - f1_m: 0.1219 - val_loss: 0.9400 - val_accuracy: 0.1713 - val_precision_m: 0.0419 - val_recall_m: 0.9419 - val_f1_m: 0.0799\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.51620\n",
      "Epoch 38/100\n",
      "32/32 - 1s - loss: 0.9055 - accuracy: 0.2731 - precision_m: 0.0673 - recall_m: 0.9226 - f1_m: 0.1234 - val_loss: 0.9397 - val_accuracy: 0.1829 - val_precision_m: 0.0421 - val_recall_m: 0.9328 - val_f1_m: 0.0803\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.51620\n",
      "Epoch 39/100\n",
      "32/32 - 1s - loss: 0.9038 - accuracy: 0.2918 - precision_m: 0.0684 - recall_m: 0.9182 - f1_m: 0.1257 - val_loss: 0.9395 - val_accuracy: 0.1973 - val_precision_m: 0.0425 - val_recall_m: 0.9237 - val_f1_m: 0.0809\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.51620\n",
      "Epoch 40/100\n",
      "32/32 - 1s - loss: 0.9038 - accuracy: 0.3038 - precision_m: 0.0698 - recall_m: 0.9138 - f1_m: 0.1276 - val_loss: 0.9396 - val_accuracy: 0.2092 - val_precision_m: 0.0429 - val_recall_m: 0.9188 - val_f1_m: 0.0816\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.51620\n",
      "Epoch 41/100\n",
      "32/32 - 1s - loss: 0.9021 - accuracy: 0.3173 - precision_m: 0.0710 - recall_m: 0.9157 - f1_m: 0.1302 - val_loss: 0.9390 - val_accuracy: 0.2205 - val_precision_m: 0.0431 - val_recall_m: 0.9087 - val_f1_m: 0.0820\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.51620\n",
      "Epoch 42/100\n",
      "32/32 - 1s - loss: 0.9018 - accuracy: 0.3353 - precision_m: 0.0728 - recall_m: 0.9153 - f1_m: 0.1333 - val_loss: 0.9387 - val_accuracy: 0.2351 - val_precision_m: 0.0437 - val_recall_m: 0.9051 - val_f1_m: 0.0831\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.51620\n",
      "Epoch 43/100\n",
      "32/32 - 1s - loss: 0.9031 - accuracy: 0.3522 - precision_m: 0.0730 - recall_m: 0.8905 - f1_m: 0.1324 - val_loss: 0.9386 - val_accuracy: 0.2421 - val_precision_m: 0.0441 - val_recall_m: 0.9025 - val_f1_m: 0.0837\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.51620\n",
      "Epoch 44/100\n",
      "32/32 - 1s - loss: 0.9031 - accuracy: 0.3585 - precision_m: 0.0737 - recall_m: 0.8962 - f1_m: 0.1338 - val_loss: 0.9394 - val_accuracy: 0.2552 - val_precision_m: 0.0446 - val_recall_m: 0.8994 - val_f1_m: 0.0847\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.51620\n",
      "Epoch 45/100\n",
      "32/32 - 1s - loss: 0.9006 - accuracy: 0.3738 - precision_m: 0.0756 - recall_m: 0.8896 - f1_m: 0.1372 - val_loss: 0.9395 - val_accuracy: 0.2776 - val_precision_m: 0.0452 - val_recall_m: 0.8827 - val_f1_m: 0.0856\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.51620\n",
      "Epoch 46/100\n",
      "32/32 - 1s - loss: 0.8997 - accuracy: 0.3998 - precision_m: 0.0781 - recall_m: 0.8855 - f1_m: 0.1419 - val_loss: 0.9384 - val_accuracy: 0.2634 - val_precision_m: 0.0452 - val_recall_m: 0.8978 - val_f1_m: 0.0857\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.51620\n",
      "Epoch 47/100\n",
      "32/32 - 1s - loss: 0.9013 - accuracy: 0.4116 - precision_m: 0.0785 - recall_m: 0.8810 - f1_m: 0.1413 - val_loss: 0.9388 - val_accuracy: 0.2764 - val_precision_m: 0.0457 - val_recall_m: 0.8925 - val_f1_m: 0.0865\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.51620\n",
      "Epoch 48/100\n",
      "32/32 - 1s - loss: 0.8996 - accuracy: 0.4263 - precision_m: 0.0810 - recall_m: 0.8831 - f1_m: 0.1456 - val_loss: 0.9390 - val_accuracy: 0.3155 - val_precision_m: 0.0465 - val_recall_m: 0.8559 - val_f1_m: 0.0878\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.51620\n",
      "Epoch 49/100\n",
      "32/32 - 1s - loss: 0.8988 - accuracy: 0.4350 - precision_m: 0.0817 - recall_m: 0.8758 - f1_m: 0.1466 - val_loss: 0.9379 - val_accuracy: 0.3735 - val_precision_m: 0.0486 - val_recall_m: 0.8196 - val_f1_m: 0.0912\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.51620\n",
      "Epoch 50/100\n",
      "32/32 - 1s - loss: 0.8980 - accuracy: 0.4416 - precision_m: 0.0824 - recall_m: 0.8760 - f1_m: 0.1491 - val_loss: 0.9389 - val_accuracy: 0.3716 - val_precision_m: 0.0479 - val_recall_m: 0.8066 - val_f1_m: 0.0899\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.51620\n",
      "Epoch 51/100\n",
      "32/32 - 1s - loss: 0.8972 - accuracy: 0.4616 - precision_m: 0.0853 - recall_m: 0.8742 - f1_m: 0.1534 - val_loss: 0.9386 - val_accuracy: 0.3772 - val_precision_m: 0.0487 - val_recall_m: 0.8115 - val_f1_m: 0.0913\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.51620\n",
      "Epoch 52/100\n",
      "32/32 - 1s - loss: 0.8967 - accuracy: 0.4850 - precision_m: 0.0872 - recall_m: 0.8492 - f1_m: 0.1564 - val_loss: 0.9382 - val_accuracy: 0.3604 - val_precision_m: 0.0484 - val_recall_m: 0.8272 - val_f1_m: 0.0909\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.51620\n",
      "Epoch 53/100\n",
      "32/32 - 1s - loss: 0.8953 - accuracy: 0.4834 - precision_m: 0.0888 - recall_m: 0.8753 - f1_m: 0.1593 - val_loss: 0.9375 - val_accuracy: 0.3700 - val_precision_m: 0.0488 - val_recall_m: 0.8224 - val_f1_m: 0.0916\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.51620\n",
      "Epoch 54/100\n",
      "32/32 - 1s - loss: 0.8959 - accuracy: 0.5092 - precision_m: 0.0922 - recall_m: 0.8647 - f1_m: 0.1644 - val_loss: 0.9379 - val_accuracy: 0.3571 - val_precision_m: 0.0486 - val_recall_m: 0.8361 - val_f1_m: 0.0914\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.51620\n",
      "Epoch 55/100\n",
      "32/32 - 1s - loss: 0.8954 - accuracy: 0.5119 - precision_m: 0.0930 - recall_m: 0.8643 - f1_m: 0.1649 - val_loss: 0.9381 - val_accuracy: 0.4056 - val_precision_m: 0.0503 - val_recall_m: 0.7961 - val_f1_m: 0.0941\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.51620\n",
      "Epoch 56/100\n",
      "32/32 - 1s - loss: 0.8935 - accuracy: 0.5266 - precision_m: 0.0940 - recall_m: 0.8480 - f1_m: 0.1674 - val_loss: 0.9379 - val_accuracy: 0.4199 - val_precision_m: 0.0512 - val_recall_m: 0.7880 - val_f1_m: 0.0955\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.51620\n",
      "Epoch 57/100\n",
      "32/32 - 1s - loss: 0.8934 - accuracy: 0.5381 - precision_m: 0.0974 - recall_m: 0.8620 - f1_m: 0.1728 - val_loss: 0.9386 - val_accuracy: 0.4208 - val_precision_m: 0.0506 - val_recall_m: 0.7740 - val_f1_m: 0.0943\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.51620\n",
      "Epoch 58/100\n",
      "32/32 - 1s - loss: 0.8933 - accuracy: 0.5570 - precision_m: 0.0993 - recall_m: 0.8388 - f1_m: 0.1743 - val_loss: 0.9378 - val_accuracy: 0.4461 - val_precision_m: 0.0526 - val_recall_m: 0.7751 - val_f1_m: 0.0977\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.51620\n",
      "Epoch 59/100\n",
      "32/32 - 1s - loss: 0.8939 - accuracy: 0.5652 - precision_m: 0.1017 - recall_m: 0.8330 - f1_m: 0.1767 - val_loss: 0.9374 - val_accuracy: 0.4284 - val_precision_m: 0.0522 - val_recall_m: 0.7957 - val_f1_m: 0.0972\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.51620\n",
      "Epoch 60/100\n",
      "32/32 - 1s - loss: 0.8929 - accuracy: 0.5717 - precision_m: 0.1027 - recall_m: 0.8432 - f1_m: 0.1792 - val_loss: 0.9384 - val_accuracy: 0.4390 - val_precision_m: 0.0518 - val_recall_m: 0.7733 - val_f1_m: 0.0963\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.51620\n",
      "Epoch 61/100\n",
      "32/32 - 1s - loss: 0.8921 - accuracy: 0.5908 - precision_m: 0.1062 - recall_m: 0.8330 - f1_m: 0.1854 - val_loss: 0.9380 - val_accuracy: 0.4097 - val_precision_m: 0.0514 - val_recall_m: 0.8081 - val_f1_m: 0.0959\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.51620\n",
      "Epoch 62/100\n",
      "32/32 - 1s - loss: 0.8908 - accuracy: 0.5993 - precision_m: 0.1083 - recall_m: 0.8385 - f1_m: 0.1887 - val_loss: 0.9381 - val_accuracy: 0.4465 - val_precision_m: 0.0529 - val_recall_m: 0.7767 - val_f1_m: 0.0983\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.51620\n",
      "Epoch 63/100\n",
      "32/32 - 1s - loss: 0.8911 - accuracy: 0.5874 - precision_m: 0.1065 - recall_m: 0.8441 - f1_m: 0.1857 - val_loss: 0.9370 - val_accuracy: 0.4238 - val_precision_m: 0.0527 - val_recall_m: 0.8093 - val_f1_m: 0.0982\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.51620\n",
      "Epoch 64/100\n",
      "32/32 - 1s - loss: 0.8913 - accuracy: 0.6122 - precision_m: 0.1091 - recall_m: 0.8220 - f1_m: 0.1896 - val_loss: 0.9379 - val_accuracy: 0.4408 - val_precision_m: 0.0527 - val_recall_m: 0.7823 - val_f1_m: 0.0980\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.51620\n",
      "Epoch 65/100\n",
      "32/32 - 1s - loss: 0.8893 - accuracy: 0.6126 - precision_m: 0.1129 - recall_m: 0.8501 - f1_m: 0.1949 - val_loss: 0.9370 - val_accuracy: 0.4748 - val_precision_m: 0.0551 - val_recall_m: 0.7673 - val_f1_m: 0.1019\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.51620\n",
      "Epoch 66/100\n",
      "32/32 - 1s - loss: 0.8896 - accuracy: 0.6218 - precision_m: 0.1129 - recall_m: 0.8324 - f1_m: 0.1949 - val_loss: 0.9368 - val_accuracy: 0.5091 - val_precision_m: 0.0564 - val_recall_m: 0.7282 - val_f1_m: 0.1037\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.51620\n",
      "Epoch 67/100\n",
      "32/32 - 1s - loss: 0.8887 - accuracy: 0.6168 - precision_m: 0.1136 - recall_m: 0.8384 - f1_m: 0.1975 - val_loss: 0.9367 - val_accuracy: 0.4610 - val_precision_m: 0.0544 - val_recall_m: 0.7747 - val_f1_m: 0.1008\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.51620\n",
      "Epoch 68/100\n",
      "32/32 - 1s - loss: 0.8884 - accuracy: 0.6335 - precision_m: 0.1166 - recall_m: 0.8370 - f1_m: 0.2004 - val_loss: 0.9363 - val_accuracy: 0.4357 - val_precision_m: 0.0539 - val_recall_m: 0.8075 - val_f1_m: 0.1002\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.51620\n",
      "Epoch 69/100\n",
      "32/32 - 1s - loss: 0.8895 - accuracy: 0.6354 - precision_m: 0.1163 - recall_m: 0.8339 - f1_m: 0.1988 - val_loss: 0.9367 - val_accuracy: 0.4907 - val_precision_m: 0.0557 - val_recall_m: 0.7475 - val_f1_m: 0.1026\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.51620\n",
      "Epoch 70/100\n",
      "32/32 - 1s - loss: 0.8881 - accuracy: 0.6297 - precision_m: 0.1166 - recall_m: 0.8479 - f1_m: 0.2022 - val_loss: 0.9375 - val_accuracy: 0.4797 - val_precision_m: 0.0540 - val_recall_m: 0.7538 - val_f1_m: 0.0997\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.51620\n",
      "Epoch 71/100\n",
      "32/32 - 1s - loss: 0.8858 - accuracy: 0.6404 - precision_m: 0.1209 - recall_m: 0.8459 - f1_m: 0.2073 - val_loss: 0.9366 - val_accuracy: 0.5151 - val_precision_m: 0.0561 - val_recall_m: 0.7281 - val_f1_m: 0.1030\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.51620\n",
      "Epoch 72/100\n",
      "32/32 - 1s - loss: 0.8865 - accuracy: 0.6407 - precision_m: 0.1210 - recall_m: 0.8538 - f1_m: 0.2065 - val_loss: 0.9370 - val_accuracy: 0.5025 - val_precision_m: 0.0552 - val_recall_m: 0.7290 - val_f1_m: 0.1015\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.51620\n",
      "Epoch 73/100\n",
      "32/32 - 1s - loss: 0.8859 - accuracy: 0.6573 - precision_m: 0.1252 - recall_m: 0.8338 - f1_m: 0.2140 - val_loss: 0.9363 - val_accuracy: 0.4972 - val_precision_m: 0.0556 - val_recall_m: 0.7463 - val_f1_m: 0.1024\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.51620\n",
      "Epoch 74/100\n",
      "32/32 - 1s - loss: 0.8875 - accuracy: 0.6585 - precision_m: 0.1227 - recall_m: 0.8198 - f1_m: 0.2081 - val_loss: 0.9367 - val_accuracy: 0.4760 - val_precision_m: 0.0550 - val_recall_m: 0.7653 - val_f1_m: 0.1016\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.51620\n",
      "Epoch 75/100\n",
      "32/32 - 1s - loss: 0.8845 - accuracy: 0.6620 - precision_m: 0.1258 - recall_m: 0.8392 - f1_m: 0.2146 - val_loss: 0.9383 - val_accuracy: 0.5366 - val_precision_m: 0.0544 - val_recall_m: 0.6721 - val_f1_m: 0.0992\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.51620\n",
      "Epoch 76/100\n",
      "32/32 - 1s - loss: 0.8858 - accuracy: 0.6631 - precision_m: 0.1264 - recall_m: 0.8331 - f1_m: 0.2145 - val_loss: 0.9356 - val_accuracy: 0.5546 - val_precision_m: 0.0590 - val_recall_m: 0.6885 - val_f1_m: 0.1074\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.51620\n",
      "Epoch 77/100\n",
      "32/32 - 1s - loss: 0.8881 - accuracy: 0.6588 - precision_m: 0.1215 - recall_m: 0.8256 - f1_m: 0.2060 - val_loss: 0.9355 - val_accuracy: 0.5169 - val_precision_m: 0.0578 - val_recall_m: 0.7422 - val_f1_m: 0.1061\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.51620\n",
      "Epoch 78/100\n",
      "32/32 - 1s - loss: 0.8847 - accuracy: 0.6691 - precision_m: 0.1297 - recall_m: 0.8434 - f1_m: 0.2199 - val_loss: 0.9368 - val_accuracy: 0.4993 - val_precision_m: 0.0555 - val_recall_m: 0.7351 - val_f1_m: 0.1019\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.51620\n",
      "Epoch 79/100\n",
      "32/32 - 1s - loss: 0.8849 - accuracy: 0.6654 - precision_m: 0.1285 - recall_m: 0.8399 - f1_m: 0.2183 - val_loss: 0.9370 - val_accuracy: 0.4825 - val_precision_m: 0.0551 - val_recall_m: 0.7601 - val_f1_m: 0.1016\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.51620\n",
      "Epoch 80/100\n",
      "32/32 - 1s - loss: 0.8834 - accuracy: 0.6793 - precision_m: 0.1325 - recall_m: 0.8482 - f1_m: 0.2244 - val_loss: 0.9363 - val_accuracy: 0.5055 - val_precision_m: 0.0562 - val_recall_m: 0.7426 - val_f1_m: 0.1032\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.51620\n",
      "Epoch 81/100\n",
      "32/32 - 1s - loss: 0.8842 - accuracy: 0.6711 - precision_m: 0.1295 - recall_m: 0.8383 - f1_m: 0.2190 - val_loss: 0.9372 - val_accuracy: 0.4861 - val_precision_m: 0.0542 - val_recall_m: 0.7518 - val_f1_m: 0.1000\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.51620\n",
      "Epoch 82/100\n",
      "32/32 - 1s - loss: 0.8849 - accuracy: 0.6823 - precision_m: 0.1309 - recall_m: 0.8365 - f1_m: 0.2198 - val_loss: 0.9381 - val_accuracy: 0.5500 - val_precision_m: 0.0551 - val_recall_m: 0.6632 - val_f1_m: 0.1003\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.51620\n",
      "Epoch 83/100\n",
      "32/32 - 1s - loss: 0.8826 - accuracy: 0.6777 - precision_m: 0.1337 - recall_m: 0.8388 - f1_m: 0.2246 - val_loss: 0.9375 - val_accuracy: 0.5462 - val_precision_m: 0.0559 - val_recall_m: 0.6685 - val_f1_m: 0.1020\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.51620\n",
      "Epoch 84/100\n",
      "32/32 - 1s - loss: 0.8807 - accuracy: 0.6887 - precision_m: 0.1382 - recall_m: 0.8510 - f1_m: 0.2332 - val_loss: 0.9370 - val_accuracy: 0.4940 - val_precision_m: 0.0545 - val_recall_m: 0.7387 - val_f1_m: 0.1004\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.51620\n",
      "Epoch 85/100\n",
      "32/32 - 1s - loss: 0.8839 - accuracy: 0.6878 - precision_m: 0.1346 - recall_m: 0.8337 - f1_m: 0.2261 - val_loss: 0.9367 - val_accuracy: 0.4991 - val_precision_m: 0.0554 - val_recall_m: 0.7416 - val_f1_m: 0.1019\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.51620\n",
      "Epoch 86/100\n",
      "32/32 - 1s - loss: 0.8822 - accuracy: 0.6910 - precision_m: 0.1383 - recall_m: 0.8353 - f1_m: 0.2326 - val_loss: 0.9379 - val_accuracy: 0.5336 - val_precision_m: 0.0538 - val_recall_m: 0.6712 - val_f1_m: 0.0984\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.51620\n",
      "Epoch 87/100\n",
      "32/32 - 1s - loss: 0.8828 - accuracy: 0.6874 - precision_m: 0.1340 - recall_m: 0.8091 - f1_m: 0.2248 - val_loss: 0.9370 - val_accuracy: 0.5532 - val_precision_m: 0.0565 - val_recall_m: 0.6757 - val_f1_m: 0.1028\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.51620\n",
      "Epoch 88/100\n",
      "32/32 - 1s - loss: 0.8817 - accuracy: 0.6875 - precision_m: 0.1357 - recall_m: 0.8531 - f1_m: 0.2298 - val_loss: 0.9364 - val_accuracy: 0.5108 - val_precision_m: 0.0559 - val_recall_m: 0.7363 - val_f1_m: 0.1027\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.51620\n",
      "Epoch 89/100\n",
      "32/32 - 1s - loss: 0.8812 - accuracy: 0.6962 - precision_m: 0.1389 - recall_m: 0.8512 - f1_m: 0.2332 - val_loss: 0.9359 - val_accuracy: 0.5711 - val_precision_m: 0.0594 - val_recall_m: 0.6718 - val_f1_m: 0.1075\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.51620\n",
      "Epoch 90/100\n",
      "32/32 - 1s - loss: 0.8843 - accuracy: 0.6982 - precision_m: 0.1359 - recall_m: 0.8221 - f1_m: 0.2250 - val_loss: 0.9383 - val_accuracy: 0.5096 - val_precision_m: 0.0536 - val_recall_m: 0.7166 - val_f1_m: 0.0985\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.51620\n",
      "Epoch 91/100\n",
      "32/32 - 1s - loss: 0.8815 - accuracy: 0.7058 - precision_m: 0.1435 - recall_m: 0.8395 - f1_m: 0.2370 - val_loss: 0.9369 - val_accuracy: 0.5686 - val_precision_m: 0.0569 - val_recall_m: 0.6524 - val_f1_m: 0.1030\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.51620\n",
      "Epoch 92/100\n",
      "32/32 - 1s - loss: 0.8796 - accuracy: 0.6954 - precision_m: 0.1402 - recall_m: 0.8610 - f1_m: 0.2369 - val_loss: 0.9365 - val_accuracy: 0.5278 - val_precision_m: 0.0571 - val_recall_m: 0.7113 - val_f1_m: 0.1045\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.51620\n",
      "Epoch 93/100\n",
      "32/32 - 1s - loss: 0.8824 - accuracy: 0.7062 - precision_m: 0.1404 - recall_m: 0.8306 - f1_m: 0.2321 - val_loss: 0.9367 - val_accuracy: 0.5449 - val_precision_m: 0.0569 - val_recall_m: 0.6882 - val_f1_m: 0.1037\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.51620\n",
      "Epoch 94/100\n",
      "32/32 - 1s - loss: 0.8795 - accuracy: 0.7038 - precision_m: 0.1432 - recall_m: 0.8614 - f1_m: 0.2406 - val_loss: 0.9366 - val_accuracy: 0.5372 - val_precision_m: 0.0567 - val_recall_m: 0.6964 - val_f1_m: 0.1035\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.51620\n",
      "Epoch 95/100\n",
      "32/32 - 1s - loss: 0.8798 - accuracy: 0.7110 - precision_m: 0.1454 - recall_m: 0.8409 - f1_m: 0.2419 - val_loss: 0.9372 - val_accuracy: 0.5571 - val_precision_m: 0.0567 - val_recall_m: 0.6591 - val_f1_m: 0.1029\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.51620\n",
      "Epoch 96/100\n",
      "32/32 - 1s - loss: 0.8817 - accuracy: 0.7042 - precision_m: 0.1408 - recall_m: 0.8286 - f1_m: 0.2332 - val_loss: 0.9364 - val_accuracy: 0.5399 - val_precision_m: 0.0572 - val_recall_m: 0.6992 - val_f1_m: 0.1043\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.51620\n",
      "Epoch 97/100\n",
      "32/32 - 1s - loss: 0.8788 - accuracy: 0.7117 - precision_m: 0.1485 - recall_m: 0.8388 - f1_m: 0.2453 - val_loss: 0.9370 - val_accuracy: 0.5410 - val_precision_m: 0.0563 - val_recall_m: 0.6855 - val_f1_m: 0.1026\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.51620\n",
      "Epoch 98/100\n",
      "32/32 - 1s - loss: 0.8805 - accuracy: 0.7096 - precision_m: 0.1436 - recall_m: 0.8465 - f1_m: 0.2400 - val_loss: 0.9366 - val_accuracy: 0.5506 - val_precision_m: 0.0568 - val_recall_m: 0.6805 - val_f1_m: 0.1033\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.51620\n",
      "Epoch 99/100\n",
      "32/32 - 1s - loss: 0.8794 - accuracy: 0.7052 - precision_m: 0.1455 - recall_m: 0.8400 - f1_m: 0.2398 - val_loss: 0.9370 - val_accuracy: 0.5331 - val_precision_m: 0.0562 - val_recall_m: 0.7013 - val_f1_m: 0.1027\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.51620\n",
      "Epoch 100/100\n",
      "32/32 - 1s - loss: 0.8792 - accuracy: 0.7049 - precision_m: 0.1438 - recall_m: 0.8569 - f1_m: 0.2421 - val_loss: 0.9360 - val_accuracy: 0.5352 - val_precision_m: 0.0573 - val_recall_m: 0.7093 - val_f1_m: 0.1047\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.51620\n",
      "5/5 - 0s - loss: 0.8907 - accuracy: 0.6551 - precision_m: 0.1138 - recall_m: 0.7275 - f1_m: 0.1965\n",
      "The metrics for the test set with loss tversky, learning rate 1e-05,  filters 8, and batch size 8 is [0.8907405138015747, 0.6550595760345459, 0.11376820504665375, 0.7274520993232727, 0.19650423526763916]\n",
      "__________________________________________________________________________________________________________________________________________________________________________\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 128, 128, 5) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 128, 128, 16) 736         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 128, 128, 16) 64          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 128, 128, 16) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 16) 2320        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 16) 64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, 128, 16) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 64, 64, 16)   0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 32)   4640        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64, 64, 32)   128         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 64, 64, 32)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 64, 32)   9248        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 32)   128         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64, 64, 32)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 32)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 64)   18496       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 64)   256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 64)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 64)   36928       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 64)   256         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 64)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 64)   0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 16, 16, 128)  73856       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 16, 16, 128)  512         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 16, 16, 128)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 16, 16, 128)  147584      activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 16, 16, 128)  512         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16, 16, 128)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 128)    0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 8, 8, 256)    295168      max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 8, 8, 256)    1024        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 8, 8, 256)    0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 8, 8, 256)    590080      activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 8, 8, 256)    1024        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 8, 8, 256)    0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 16, 16, 128)  131200      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 16, 16, 256)  0           conv2d_transpose[0][0]           \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 128)  295040      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 128)  512         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 128)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 128)  147584      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 128)  512         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 128)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 32, 32, 64)   32832       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 128)  0           conv2d_transpose_1[0][0]         \n",
      "                                                                 activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 64)   73792       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 32, 32, 64)   256         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 32, 32, 64)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 32, 64)   36928       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 32, 32, 64)   256         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 32, 32, 64)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 64, 64, 32)   8224        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 64, 64, 64)   0           conv2d_transpose_2[0][0]         \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 32)   18464       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64, 64, 32)   128         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 64, 64, 32)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 32)   9248        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 64, 64, 32)   128         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 64, 64, 32)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 128, 128, 16) 2064        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 128, 128, 32) 0           conv2d_transpose_3[0][0]         \n",
      "                                                                 activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 128, 128, 16) 4624        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 128, 128, 16) 64          conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 128, 128, 16) 0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 128, 128, 16) 2320        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 128, 128, 16) 64          conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 128, 128, 16) 0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 128, 128, 1)  17          activation_17[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,947,281\n",
      "Trainable params: 1,944,337\n",
      "Non-trainable params: 2,944\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "Initialising in 3..........2..........1......................................\n",
      "Training on 284 images with learning rate 1e-05, batch size 8, and number of filters 16\n",
      "Epoch 1/100\n",
      "32/32 - 3s - loss: 0.9286 - accuracy: 0.7700 - precision_m: 0.0145 - recall_m: 0.0553 - f1_m: 0.0223 - val_loss: 0.9489 - val_accuracy: 0.7200 - val_precision_m: 0.0174 - val_recall_m: 0.1151 - val_f1_m: 0.0300\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.51620\n",
      "Epoch 2/100\n",
      "32/32 - 1s - loss: 0.9283 - accuracy: 0.7466 - precision_m: 0.0272 - recall_m: 0.1035 - f1_m: 0.0412 - val_loss: 0.9484 - val_accuracy: 0.8221 - val_precision_m: 0.0107 - val_recall_m: 0.0443 - val_f1_m: 0.0171\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.51620\n",
      "Epoch 3/100\n",
      "32/32 - 1s - loss: 0.9274 - accuracy: 0.6795 - precision_m: 0.0418 - recall_m: 0.2135 - f1_m: 0.0686 - val_loss: 0.9481 - val_accuracy: 0.8278 - val_precision_m: 0.0193 - val_recall_m: 0.0677 - val_f1_m: 0.0300\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.51620\n",
      "Epoch 4/100\n",
      "32/32 - 1s - loss: 0.9270 - accuracy: 0.5895 - precision_m: 0.0521 - recall_m: 0.3637 - f1_m: 0.0884 - val_loss: 0.9478 - val_accuracy: 0.7628 - val_precision_m: 0.0304 - val_recall_m: 0.1695 - val_f1_m: 0.0514\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.51620\n",
      "Epoch 5/100\n",
      "32/32 - 1s - loss: 0.9260 - accuracy: 0.5042 - precision_m: 0.0578 - recall_m: 0.5031 - f1_m: 0.1019 - val_loss: 0.9476 - val_accuracy: 0.6662 - val_precision_m: 0.0340 - val_recall_m: 0.2867 - val_f1_m: 0.0605\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.51620\n",
      "Epoch 6/100\n",
      "32/32 - 1s - loss: 0.9249 - accuracy: 0.4550 - precision_m: 0.0610 - recall_m: 0.5952 - f1_m: 0.1092 - val_loss: 0.9473 - val_accuracy: 0.5311 - val_precision_m: 0.0384 - val_recall_m: 0.4770 - val_f1_m: 0.0707\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.51620\n",
      "Epoch 7/100\n",
      "32/32 - 1s - loss: 0.9241 - accuracy: 0.4285 - precision_m: 0.0621 - recall_m: 0.6358 - f1_m: 0.1119 - val_loss: 0.9470 - val_accuracy: 0.4538 - val_precision_m: 0.0396 - val_recall_m: 0.5789 - val_f1_m: 0.0737\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.51620\n",
      "Epoch 8/100\n",
      "32/32 - 1s - loss: 0.9231 - accuracy: 0.4148 - precision_m: 0.0646 - recall_m: 0.6959 - f1_m: 0.1165 - val_loss: 0.9466 - val_accuracy: 0.3953 - val_precision_m: 0.0404 - val_recall_m: 0.6583 - val_f1_m: 0.0758\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.51620\n",
      "Epoch 9/100\n",
      "32/32 - 1s - loss: 0.9221 - accuracy: 0.4089 - precision_m: 0.0668 - recall_m: 0.7348 - f1_m: 0.1205 - val_loss: 0.9461 - val_accuracy: 0.3355 - val_precision_m: 0.0415 - val_recall_m: 0.7457 - val_f1_m: 0.0782\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.51620\n",
      "Epoch 10/100\n",
      "32/32 - 1s - loss: 0.9208 - accuracy: 0.4141 - precision_m: 0.0695 - recall_m: 0.7510 - f1_m: 0.1248 - val_loss: 0.9455 - val_accuracy: 0.3106 - val_precision_m: 0.0426 - val_recall_m: 0.7977 - val_f1_m: 0.0805\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.51620\n",
      "Epoch 11/100\n",
      "32/32 - 1s - loss: 0.9193 - accuracy: 0.4347 - precision_m: 0.0715 - recall_m: 0.7493 - f1_m: 0.1279 - val_loss: 0.9449 - val_accuracy: 0.2914 - val_precision_m: 0.0432 - val_recall_m: 0.8347 - val_f1_m: 0.0818\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.51620\n",
      "Epoch 12/100\n",
      "32/32 - 1s - loss: 0.9179 - accuracy: 0.4563 - precision_m: 0.0751 - recall_m: 0.7310 - f1_m: 0.1331 - val_loss: 0.9443 - val_accuracy: 0.2966 - val_precision_m: 0.0441 - val_recall_m: 0.8439 - val_f1_m: 0.0834\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.51620\n",
      "Epoch 13/100\n",
      "32/32 - 1s - loss: 0.9165 - accuracy: 0.4686 - precision_m: 0.0756 - recall_m: 0.7488 - f1_m: 0.1351 - val_loss: 0.9436 - val_accuracy: 0.2975 - val_precision_m: 0.0448 - val_recall_m: 0.8552 - val_f1_m: 0.0847\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.51620\n",
      "Epoch 14/100\n",
      "32/32 - 1s - loss: 0.9143 - accuracy: 0.4936 - precision_m: 0.0805 - recall_m: 0.7416 - f1_m: 0.1427 - val_loss: 0.9431 - val_accuracy: 0.3024 - val_precision_m: 0.0452 - val_recall_m: 0.8571 - val_f1_m: 0.0855\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.51620\n",
      "Epoch 15/100\n",
      "32/32 - 1s - loss: 0.9131 - accuracy: 0.5190 - precision_m: 0.0815 - recall_m: 0.7273 - f1_m: 0.1444 - val_loss: 0.9425 - val_accuracy: 0.3080 - val_precision_m: 0.0457 - val_recall_m: 0.8575 - val_f1_m: 0.0863\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.51620\n",
      "Epoch 16/100\n",
      "32/32 - 1s - loss: 0.9112 - accuracy: 0.5252 - precision_m: 0.0831 - recall_m: 0.7306 - f1_m: 0.1466 - val_loss: 0.9418 - val_accuracy: 0.3362 - val_precision_m: 0.0470 - val_recall_m: 0.8393 - val_f1_m: 0.0885\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.51620\n",
      "Epoch 17/100\n",
      "32/32 - 1s - loss: 0.9094 - accuracy: 0.5404 - precision_m: 0.0847 - recall_m: 0.7338 - f1_m: 0.1501 - val_loss: 0.9410 - val_accuracy: 0.3661 - val_precision_m: 0.0486 - val_recall_m: 0.8243 - val_f1_m: 0.0911\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.51620\n",
      "Epoch 18/100\n",
      "32/32 - 1s - loss: 0.9075 - accuracy: 0.5503 - precision_m: 0.0880 - recall_m: 0.7461 - f1_m: 0.1546 - val_loss: 0.9401 - val_accuracy: 0.3815 - val_precision_m: 0.0496 - val_recall_m: 0.8220 - val_f1_m: 0.0930\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.51620\n",
      "Epoch 19/100\n",
      "32/32 - 1s - loss: 0.9043 - accuracy: 0.5646 - precision_m: 0.0931 - recall_m: 0.7635 - f1_m: 0.1638 - val_loss: 0.9398 - val_accuracy: 0.3878 - val_precision_m: 0.0497 - val_recall_m: 0.8128 - val_f1_m: 0.0931\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.51620\n",
      "Epoch 20/100\n",
      "32/32 - 1s - loss: 0.9030 - accuracy: 0.5634 - precision_m: 0.0935 - recall_m: 0.7453 - f1_m: 0.1634 - val_loss: 0.9394 - val_accuracy: 0.4207 - val_precision_m: 0.0508 - val_recall_m: 0.7837 - val_f1_m: 0.0947\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.51620\n",
      "Epoch 21/100\n",
      "32/32 - 1s - loss: 0.9009 - accuracy: 0.5775 - precision_m: 0.0961 - recall_m: 0.7772 - f1_m: 0.1681 - val_loss: 0.9385 - val_accuracy: 0.4205 - val_precision_m: 0.0517 - val_recall_m: 0.7984 - val_f1_m: 0.0962\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.51620\n",
      "Epoch 22/100\n",
      "32/32 - 1s - loss: 0.8984 - accuracy: 0.6027 - precision_m: 0.1031 - recall_m: 0.7603 - f1_m: 0.1776 - val_loss: 0.9382 - val_accuracy: 0.4648 - val_precision_m: 0.0516 - val_recall_m: 0.7313 - val_f1_m: 0.0954\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.51620\n",
      "Epoch 23/100\n",
      "32/32 - 1s - loss: 0.8978 - accuracy: 0.5643 - precision_m: 0.0975 - recall_m: 0.7991 - f1_m: 0.1715 - val_loss: 0.9380 - val_accuracy: 0.4491 - val_precision_m: 0.0517 - val_recall_m: 0.7593 - val_f1_m: 0.0958\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.51620\n",
      "Epoch 24/100\n",
      "32/32 - 1s - loss: 0.8959 - accuracy: 0.5904 - precision_m: 0.1002 - recall_m: 0.7794 - f1_m: 0.1738 - val_loss: 0.9368 - val_accuracy: 0.4683 - val_precision_m: 0.0535 - val_recall_m: 0.7576 - val_f1_m: 0.0988\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.51620\n",
      "Epoch 25/100\n",
      "32/32 - 1s - loss: 0.8927 - accuracy: 0.5955 - precision_m: 0.1049 - recall_m: 0.8111 - f1_m: 0.1832 - val_loss: 0.9364 - val_accuracy: 0.4375 - val_precision_m: 0.0535 - val_recall_m: 0.7957 - val_f1_m: 0.0994\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.51620\n",
      "Epoch 26/100\n",
      "32/32 - 1s - loss: 0.8906 - accuracy: 0.6152 - precision_m: 0.1096 - recall_m: 0.8043 - f1_m: 0.1885 - val_loss: 0.9361 - val_accuracy: 0.4179 - val_precision_m: 0.0533 - val_recall_m: 0.8244 - val_f1_m: 0.0993\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.51620\n",
      "Epoch 27/100\n",
      "32/32 - 1s - loss: 0.8886 - accuracy: 0.5993 - precision_m: 0.1078 - recall_m: 0.8393 - f1_m: 0.1878 - val_loss: 0.9360 - val_accuracy: 0.3787 - val_precision_m: 0.0523 - val_recall_m: 0.8620 - val_f1_m: 0.0979\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.51620\n",
      "Epoch 28/100\n",
      "32/32 - 1s - loss: 0.8859 - accuracy: 0.6136 - precision_m: 0.1136 - recall_m: 0.8461 - f1_m: 0.1968 - val_loss: 0.9359 - val_accuracy: 0.3671 - val_precision_m: 0.0522 - val_recall_m: 0.8843 - val_f1_m: 0.0980\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.51620\n",
      "Epoch 29/100\n",
      "32/32 - 1s - loss: 0.8843 - accuracy: 0.6289 - precision_m: 0.1169 - recall_m: 0.8283 - f1_m: 0.2009 - val_loss: 0.9360 - val_accuracy: 0.3835 - val_precision_m: 0.0524 - val_recall_m: 0.8731 - val_f1_m: 0.0981\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.51620\n",
      "Epoch 30/100\n",
      "32/32 - 1s - loss: 0.8819 - accuracy: 0.6443 - precision_m: 0.1218 - recall_m: 0.8293 - f1_m: 0.2076 - val_loss: 0.9355 - val_accuracy: 0.3758 - val_precision_m: 0.0528 - val_recall_m: 0.8853 - val_f1_m: 0.0989\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.51620\n",
      "Epoch 31/100\n",
      "32/32 - 1s - loss: 0.8817 - accuracy: 0.6404 - precision_m: 0.1211 - recall_m: 0.8428 - f1_m: 0.2067 - val_loss: 0.9359 - val_accuracy: 0.3718 - val_precision_m: 0.0520 - val_recall_m: 0.8793 - val_f1_m: 0.0974\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.51620\n",
      "Epoch 32/100\n",
      "32/32 - 1s - loss: 0.8788 - accuracy: 0.6479 - precision_m: 0.1246 - recall_m: 0.8585 - f1_m: 0.2142 - val_loss: 0.9356 - val_accuracy: 0.3321 - val_precision_m: 0.0515 - val_recall_m: 0.9242 - val_f1_m: 0.0969\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.51620\n",
      "Epoch 33/100\n",
      "32/32 - 1s - loss: 0.8788 - accuracy: 0.6585 - precision_m: 0.1248 - recall_m: 0.8469 - f1_m: 0.2137 - val_loss: 0.9362 - val_accuracy: 0.3365 - val_precision_m: 0.0508 - val_recall_m: 0.9084 - val_f1_m: 0.0955\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.51620\n",
      "Epoch 34/100\n",
      "32/32 - 1s - loss: 0.8752 - accuracy: 0.6619 - precision_m: 0.1307 - recall_m: 0.8751 - f1_m: 0.2224 - val_loss: 0.9366 - val_accuracy: 0.3079 - val_precision_m: 0.0497 - val_recall_m: 0.9311 - val_f1_m: 0.0938\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.51620\n",
      "Epoch 35/100\n",
      "32/32 - 1s - loss: 0.8712 - accuracy: 0.6715 - precision_m: 0.1377 - recall_m: 0.8907 - f1_m: 0.2343 - val_loss: 0.9360 - val_accuracy: 0.3214 - val_precision_m: 0.0507 - val_recall_m: 0.9284 - val_f1_m: 0.0955\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.51620\n",
      "Epoch 36/100\n",
      "32/32 - 1s - loss: 0.8743 - accuracy: 0.6881 - precision_m: 0.1379 - recall_m: 0.8700 - f1_m: 0.2323 - val_loss: 0.9361 - val_accuracy: 0.3160 - val_precision_m: 0.0504 - val_recall_m: 0.9301 - val_f1_m: 0.0951\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.51620\n",
      "Epoch 37/100\n",
      "32/32 - 1s - loss: 0.8721 - accuracy: 0.6769 - precision_m: 0.1354 - recall_m: 0.8824 - f1_m: 0.2305 - val_loss: 0.9361 - val_accuracy: 0.3173 - val_precision_m: 0.0502 - val_recall_m: 0.9262 - val_f1_m: 0.0946\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.51620\n",
      "Epoch 38/100\n",
      "32/32 - 1s - loss: 0.8700 - accuracy: 0.6855 - precision_m: 0.1401 - recall_m: 0.8869 - f1_m: 0.2368 - val_loss: 0.9361 - val_accuracy: 0.3201 - val_precision_m: 0.0502 - val_recall_m: 0.9205 - val_f1_m: 0.0945\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.51620\n",
      "Epoch 39/100\n",
      "32/32 - 1s - loss: 0.8679 - accuracy: 0.6885 - precision_m: 0.1429 - recall_m: 0.8942 - f1_m: 0.2425 - val_loss: 0.9359 - val_accuracy: 0.3025 - val_precision_m: 0.0501 - val_recall_m: 0.9425 - val_f1_m: 0.0945\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.51620\n",
      "Epoch 40/100\n",
      "32/32 - 1s - loss: 0.8700 - accuracy: 0.6858 - precision_m: 0.1401 - recall_m: 0.8854 - f1_m: 0.2368 - val_loss: 0.9352 - val_accuracy: 0.3160 - val_precision_m: 0.0514 - val_recall_m: 0.9406 - val_f1_m: 0.0968\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.51620\n",
      "Epoch 41/100\n",
      "32/32 - 1s - loss: 0.8651 - accuracy: 0.7048 - precision_m: 0.1498 - recall_m: 0.8941 - f1_m: 0.2513 - val_loss: 0.9352 - val_accuracy: 0.3453 - val_precision_m: 0.0515 - val_recall_m: 0.9109 - val_f1_m: 0.0969\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.51620\n",
      "Epoch 42/100\n",
      "32/32 - 1s - loss: 0.8650 - accuracy: 0.7047 - precision_m: 0.1514 - recall_m: 0.9031 - f1_m: 0.2551 - val_loss: 0.9345 - val_accuracy: 0.3719 - val_precision_m: 0.0527 - val_recall_m: 0.8916 - val_f1_m: 0.0988\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.51620\n",
      "Epoch 43/100\n",
      "32/32 - 1s - loss: 0.8649 - accuracy: 0.7091 - precision_m: 0.1499 - recall_m: 0.9059 - f1_m: 0.2519 - val_loss: 0.9346 - val_accuracy: 0.3481 - val_precision_m: 0.0521 - val_recall_m: 0.9146 - val_f1_m: 0.0979\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.51620\n",
      "Epoch 44/100\n",
      "32/32 - 1s - loss: 0.8666 - accuracy: 0.7123 - precision_m: 0.1510 - recall_m: 0.8978 - f1_m: 0.2512 - val_loss: 0.9343 - val_accuracy: 0.3449 - val_precision_m: 0.0523 - val_recall_m: 0.9236 - val_f1_m: 0.0984\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.51620\n",
      "Epoch 45/100\n",
      "32/32 - 1s - loss: 0.8611 - accuracy: 0.7123 - precision_m: 0.1552 - recall_m: 0.9147 - f1_m: 0.2609 - val_loss: 0.9336 - val_accuracy: 0.3818 - val_precision_m: 0.0535 - val_recall_m: 0.8892 - val_f1_m: 0.1003\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.51620\n",
      "Epoch 46/100\n",
      "32/32 - 1s - loss: 0.8605 - accuracy: 0.7193 - precision_m: 0.1572 - recall_m: 0.9091 - f1_m: 0.2626 - val_loss: 0.9346 - val_accuracy: 0.3482 - val_precision_m: 0.0520 - val_recall_m: 0.9178 - val_f1_m: 0.0978\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.51620\n",
      "Epoch 47/100\n",
      "32/32 - 1s - loss: 0.8654 - accuracy: 0.7064 - precision_m: 0.1488 - recall_m: 0.8787 - f1_m: 0.2467 - val_loss: 0.9337 - val_accuracy: 0.4064 - val_precision_m: 0.0530 - val_recall_m: 0.8486 - val_f1_m: 0.0990\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.51620\n",
      "Epoch 48/100\n",
      "32/32 - 1s - loss: 0.8637 - accuracy: 0.7145 - precision_m: 0.1539 - recall_m: 0.8983 - f1_m: 0.2567 - val_loss: 0.9338 - val_accuracy: 0.3648 - val_precision_m: 0.0530 - val_recall_m: 0.9030 - val_f1_m: 0.0994\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.51620\n",
      "Epoch 49/100\n",
      "32/32 - 1s - loss: 0.8616 - accuracy: 0.7095 - precision_m: 0.1543 - recall_m: 0.9145 - f1_m: 0.2570 - val_loss: 0.9344 - val_accuracy: 0.3326 - val_precision_m: 0.0518 - val_recall_m: 0.9316 - val_f1_m: 0.0976\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.51620\n",
      "Epoch 50/100\n",
      "32/32 - 1s - loss: 0.8581 - accuracy: 0.7084 - precision_m: 0.1552 - recall_m: 0.9219 - f1_m: 0.2601 - val_loss: 0.9332 - val_accuracy: 0.3530 - val_precision_m: 0.0533 - val_recall_m: 0.9296 - val_f1_m: 0.1002\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.51620\n",
      "Epoch 51/100\n",
      "32/32 - 1s - loss: 0.8593 - accuracy: 0.7249 - precision_m: 0.1603 - recall_m: 0.9191 - f1_m: 0.2650 - val_loss: 0.9340 - val_accuracy: 0.3536 - val_precision_m: 0.0525 - val_recall_m: 0.9187 - val_f1_m: 0.0987\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.51620\n",
      "Epoch 52/100\n",
      "32/32 - 1s - loss: 0.8572 - accuracy: 0.7262 - precision_m: 0.1605 - recall_m: 0.9234 - f1_m: 0.2686 - val_loss: 0.9341 - val_accuracy: 0.3279 - val_precision_m: 0.0520 - val_recall_m: 0.9390 - val_f1_m: 0.0979\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.51620\n",
      "Epoch 53/100\n",
      "32/32 - 1s - loss: 0.8580 - accuracy: 0.7222 - precision_m: 0.1597 - recall_m: 0.9197 - f1_m: 0.2659 - val_loss: 0.9338 - val_accuracy: 0.3375 - val_precision_m: 0.0524 - val_recall_m: 0.9341 - val_f1_m: 0.0986\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.51620\n",
      "Epoch 54/100\n",
      "32/32 - 1s - loss: 0.8572 - accuracy: 0.7335 - precision_m: 0.1628 - recall_m: 0.8937 - f1_m: 0.2693 - val_loss: 0.9324 - val_accuracy: 0.3671 - val_precision_m: 0.0542 - val_recall_m: 0.9247 - val_f1_m: 0.1018\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.51620\n",
      "Epoch 55/100\n",
      "32/32 - 1s - loss: 0.8553 - accuracy: 0.7259 - precision_m: 0.1615 - recall_m: 0.9294 - f1_m: 0.2676 - val_loss: 0.9342 - val_accuracy: 0.3379 - val_precision_m: 0.0521 - val_recall_m: 0.9302 - val_f1_m: 0.0981\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.51620\n",
      "Epoch 56/100\n",
      "32/32 - 1s - loss: 0.8524 - accuracy: 0.7411 - precision_m: 0.1726 - recall_m: 0.9165 - f1_m: 0.2823 - val_loss: 0.9320 - val_accuracy: 0.3861 - val_precision_m: 0.0552 - val_recall_m: 0.9116 - val_f1_m: 0.1033\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.51620\n",
      "Epoch 57/100\n",
      "32/32 - 1s - loss: 0.8513 - accuracy: 0.7363 - precision_m: 0.1699 - recall_m: 0.9418 - f1_m: 0.2816 - val_loss: 0.9319 - val_accuracy: 0.3963 - val_precision_m: 0.0554 - val_recall_m: 0.8985 - val_f1_m: 0.1036\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.51620\n",
      "Epoch 58/100\n",
      "32/32 - 1s - loss: 0.8549 - accuracy: 0.7393 - precision_m: 0.1684 - recall_m: 0.9316 - f1_m: 0.2764 - val_loss: 0.9311 - val_accuracy: 0.4146 - val_precision_m: 0.0563 - val_recall_m: 0.8820 - val_f1_m: 0.1050\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.51620\n",
      "Epoch 59/100\n",
      "32/32 - 1s - loss: 0.8540 - accuracy: 0.7415 - precision_m: 0.1687 - recall_m: 0.9153 - f1_m: 0.2765 - val_loss: 0.9293 - val_accuracy: 0.4538 - val_precision_m: 0.0592 - val_recall_m: 0.8637 - val_f1_m: 0.1100\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.51620\n",
      "Epoch 60/100\n",
      "32/32 - 1s - loss: 0.8524 - accuracy: 0.7407 - precision_m: 0.1679 - recall_m: 0.9254 - f1_m: 0.2768 - val_loss: 0.9316 - val_accuracy: 0.3824 - val_precision_m: 0.0556 - val_recall_m: 0.9171 - val_f1_m: 0.1042\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.51620\n",
      "Epoch 61/100\n",
      "32/32 - 1s - loss: 0.8561 - accuracy: 0.7379 - precision_m: 0.1633 - recall_m: 0.9149 - f1_m: 0.2700 - val_loss: 0.9282 - val_accuracy: 0.4565 - val_precision_m: 0.0610 - val_recall_m: 0.8789 - val_f1_m: 0.1133\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.51620\n",
      "Epoch 62/100\n",
      "32/32 - 1s - loss: 0.8498 - accuracy: 0.7434 - precision_m: 0.1720 - recall_m: 0.9336 - f1_m: 0.2848 - val_loss: 0.9295 - val_accuracy: 0.4246 - val_precision_m: 0.0587 - val_recall_m: 0.9056 - val_f1_m: 0.1096\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.51620\n",
      "Epoch 63/100\n",
      "32/32 - 1s - loss: 0.8493 - accuracy: 0.7478 - precision_m: 0.1749 - recall_m: 0.9307 - f1_m: 0.2863 - val_loss: 0.9291 - val_accuracy: 0.4223 - val_precision_m: 0.0592 - val_recall_m: 0.9134 - val_f1_m: 0.1105\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.51620\n",
      "Epoch 64/100\n",
      "32/32 - 1s - loss: 0.8479 - accuracy: 0.7536 - precision_m: 0.1779 - recall_m: 0.9374 - f1_m: 0.2919 - val_loss: 0.9296 - val_accuracy: 0.4198 - val_precision_m: 0.0587 - val_recall_m: 0.9088 - val_f1_m: 0.1096\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.51620\n",
      "Epoch 65/100\n",
      "32/32 - 1s - loss: 0.8518 - accuracy: 0.7463 - precision_m: 0.1734 - recall_m: 0.9180 - f1_m: 0.2833 - val_loss: 0.9295 - val_accuracy: 0.4004 - val_precision_m: 0.0587 - val_recall_m: 0.9309 - val_f1_m: 0.1098\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.51620\n",
      "Epoch 66/100\n",
      "32/32 - 1s - loss: 0.8472 - accuracy: 0.7503 - precision_m: 0.1789 - recall_m: 0.9394 - f1_m: 0.2925 - val_loss: 0.9266 - val_accuracy: 0.4529 - val_precision_m: 0.0627 - val_recall_m: 0.9060 - val_f1_m: 0.1165\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.51620\n",
      "Epoch 67/100\n",
      "32/32 - 1s - loss: 0.8466 - accuracy: 0.7558 - precision_m: 0.1808 - recall_m: 0.9394 - f1_m: 0.2958 - val_loss: 0.9249 - val_accuracy: 0.4857 - val_precision_m: 0.0655 - val_recall_m: 0.8793 - val_f1_m: 0.1211\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.51620\n",
      "Epoch 68/100\n",
      "32/32 - 1s - loss: 0.8459 - accuracy: 0.7647 - precision_m: 0.1833 - recall_m: 0.9337 - f1_m: 0.3002 - val_loss: 0.9252 - val_accuracy: 0.4661 - val_precision_m: 0.0647 - val_recall_m: 0.9039 - val_f1_m: 0.1199\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.51620\n",
      "Epoch 69/100\n",
      "32/32 - 1s - loss: 0.8467 - accuracy: 0.7652 - precision_m: 0.1835 - recall_m: 0.9268 - f1_m: 0.2995 - val_loss: 0.9222 - val_accuracy: 0.5506 - val_precision_m: 0.0725 - val_recall_m: 0.8386 - val_f1_m: 0.1322\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.51620\n",
      "Epoch 70/100\n",
      "32/32 - 1s - loss: 0.8457 - accuracy: 0.7649 - precision_m: 0.1847 - recall_m: 0.9159 - f1_m: 0.3008 - val_loss: 0.9227 - val_accuracy: 0.5270 - val_precision_m: 0.0706 - val_recall_m: 0.8616 - val_f1_m: 0.1294\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.51620\n",
      "Epoch 71/100\n",
      "32/32 - 1s - loss: 0.8438 - accuracy: 0.7683 - precision_m: 0.1879 - recall_m: 0.9300 - f1_m: 0.3067 - val_loss: 0.9214 - val_accuracy: 0.5593 - val_precision_m: 0.0740 - val_recall_m: 0.8326 - val_f1_m: 0.1347\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.51620\n",
      "Epoch 72/100\n",
      "32/32 - 1s - loss: 0.8486 - accuracy: 0.7587 - precision_m: 0.1780 - recall_m: 0.9173 - f1_m: 0.2921 - val_loss: 0.9272 - val_accuracy: 0.4364 - val_precision_m: 0.0617 - val_recall_m: 0.9112 - val_f1_m: 0.1148\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.51620\n",
      "Epoch 73/100\n",
      "32/32 - 1s - loss: 0.8445 - accuracy: 0.7598 - precision_m: 0.1841 - recall_m: 0.9194 - f1_m: 0.3002 - val_loss: 0.9231 - val_accuracy: 0.5038 - val_precision_m: 0.0685 - val_recall_m: 0.8807 - val_f1_m: 0.1261\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.51620\n",
      "Epoch 74/100\n",
      "32/32 - 1s - loss: 0.8450 - accuracy: 0.7653 - precision_m: 0.1858 - recall_m: 0.9290 - f1_m: 0.3030 - val_loss: 0.9239 - val_accuracy: 0.5016 - val_precision_m: 0.0671 - val_recall_m: 0.8764 - val_f1_m: 0.1237\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.51620\n",
      "Epoch 75/100\n",
      "32/32 - 1s - loss: 0.8453 - accuracy: 0.7642 - precision_m: 0.1859 - recall_m: 0.9316 - f1_m: 0.3036 - val_loss: 0.9225 - val_accuracy: 0.5530 - val_precision_m: 0.0710 - val_recall_m: 0.8229 - val_f1_m: 0.1295\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.51620\n",
      "Epoch 76/100\n",
      "32/32 - 1s - loss: 0.8431 - accuracy: 0.7690 - precision_m: 0.1914 - recall_m: 0.9329 - f1_m: 0.3101 - val_loss: 0.9245 - val_accuracy: 0.5003 - val_precision_m: 0.0663 - val_recall_m: 0.8717 - val_f1_m: 0.1222\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.51620\n",
      "Epoch 77/100\n",
      "32/32 - 1s - loss: 0.8457 - accuracy: 0.7579 - precision_m: 0.1806 - recall_m: 0.9213 - f1_m: 0.2922 - val_loss: 0.9241 - val_accuracy: 0.4888 - val_precision_m: 0.0661 - val_recall_m: 0.8855 - val_f1_m: 0.1221\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.51620\n",
      "Epoch 78/100\n",
      "32/32 - 1s - loss: 0.8448 - accuracy: 0.7600 - precision_m: 0.1833 - recall_m: 0.9266 - f1_m: 0.2973 - val_loss: 0.9260 - val_accuracy: 0.4757 - val_precision_m: 0.0636 - val_recall_m: 0.8803 - val_f1_m: 0.1178\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.51620\n",
      "Epoch 79/100\n",
      "32/32 - 1s - loss: 0.8426 - accuracy: 0.7708 - precision_m: 0.1894 - recall_m: 0.9333 - f1_m: 0.3071 - val_loss: 0.9224 - val_accuracy: 0.5543 - val_precision_m: 0.0714 - val_recall_m: 0.8234 - val_f1_m: 0.1302\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.51620\n",
      "Epoch 80/100\n",
      "32/32 - 1s - loss: 0.8400 - accuracy: 0.7769 - precision_m: 0.1951 - recall_m: 0.9464 - f1_m: 0.3144 - val_loss: 0.9231 - val_accuracy: 0.5239 - val_precision_m: 0.0689 - val_recall_m: 0.8573 - val_f1_m: 0.1265\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.51620\n",
      "Epoch 81/100\n",
      "32/32 - 1s - loss: 0.8422 - accuracy: 0.7724 - precision_m: 0.1921 - recall_m: 0.9230 - f1_m: 0.3083 - val_loss: 0.9264 - val_accuracy: 0.4476 - val_precision_m: 0.0623 - val_recall_m: 0.9124 - val_f1_m: 0.1159\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.51620\n",
      "Epoch 82/100\n",
      "32/32 - 1s - loss: 0.8391 - accuracy: 0.7801 - precision_m: 0.1983 - recall_m: 0.9407 - f1_m: 0.3207 - val_loss: 0.9265 - val_accuracy: 0.4599 - val_precision_m: 0.0625 - val_recall_m: 0.8952 - val_f1_m: 0.1161\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.51620\n",
      "Epoch 83/100\n",
      "32/32 - 1s - loss: 0.8385 - accuracy: 0.7854 - precision_m: 0.2006 - recall_m: 0.9496 - f1_m: 0.3224 - val_loss: 0.9218 - val_accuracy: 0.5461 - val_precision_m: 0.0713 - val_recall_m: 0.8440 - val_f1_m: 0.1302\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.51620\n",
      "Epoch 84/100\n",
      "32/32 - 1s - loss: 0.8398 - accuracy: 0.7791 - precision_m: 0.1957 - recall_m: 0.9427 - f1_m: 0.3187 - val_loss: 0.9219 - val_accuracy: 0.5438 - val_precision_m: 0.0716 - val_recall_m: 0.8421 - val_f1_m: 0.1308\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.51620\n",
      "Epoch 85/100\n",
      "32/32 - 1s - loss: 0.8397 - accuracy: 0.7849 - precision_m: 0.1989 - recall_m: 0.9404 - f1_m: 0.3211 - val_loss: 0.9218 - val_accuracy: 0.5546 - val_precision_m: 0.0717 - val_recall_m: 0.8305 - val_f1_m: 0.1307\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.51620\n",
      "Epoch 86/100\n",
      "32/32 - 1s - loss: 0.8370 - accuracy: 0.7863 - precision_m: 0.2037 - recall_m: 0.9325 - f1_m: 0.3248 - val_loss: 0.9211 - val_accuracy: 0.5544 - val_precision_m: 0.0731 - val_recall_m: 0.8454 - val_f1_m: 0.1335\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.51620\n",
      "Epoch 87/100\n",
      "32/32 - 1s - loss: 0.8429 - accuracy: 0.7701 - precision_m: 0.1879 - recall_m: 0.9365 - f1_m: 0.3021 - val_loss: 0.9255 - val_accuracy: 0.4607 - val_precision_m: 0.0637 - val_recall_m: 0.9081 - val_f1_m: 0.1183\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.51620\n",
      "Epoch 88/100\n",
      "32/32 - 1s - loss: 0.8421 - accuracy: 0.7746 - precision_m: 0.1937 - recall_m: 0.9293 - f1_m: 0.3077 - val_loss: 0.9218 - val_accuracy: 0.5320 - val_precision_m: 0.0704 - val_recall_m: 0.8612 - val_f1_m: 0.1291\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.51620\n",
      "Epoch 89/100\n",
      "32/32 - 1s - loss: 0.8384 - accuracy: 0.7884 - precision_m: 0.2019 - recall_m: 0.9392 - f1_m: 0.3239 - val_loss: 0.9233 - val_accuracy: 0.5118 - val_precision_m: 0.0674 - val_recall_m: 0.8648 - val_f1_m: 0.1241\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.51620\n",
      "Epoch 90/100\n",
      "32/32 - 1s - loss: 0.8389 - accuracy: 0.7781 - precision_m: 0.1975 - recall_m: 0.9401 - f1_m: 0.3190 - val_loss: 0.9213 - val_accuracy: 0.5384 - val_precision_m: 0.0709 - val_recall_m: 0.8585 - val_f1_m: 0.1299\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.51620\n",
      "Epoch 91/100\n",
      "32/32 - 1s - loss: 0.8377 - accuracy: 0.7845 - precision_m: 0.2000 - recall_m: 0.9426 - f1_m: 0.3210 - val_loss: 0.9206 - val_accuracy: 0.5610 - val_precision_m: 0.0732 - val_recall_m: 0.8298 - val_f1_m: 0.1332\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.51620\n",
      "Epoch 92/100\n",
      "32/32 - 1s - loss: 0.8358 - accuracy: 0.7793 - precision_m: 0.1993 - recall_m: 0.9540 - f1_m: 0.3217 - val_loss: 0.9231 - val_accuracy: 0.5094 - val_precision_m: 0.0675 - val_recall_m: 0.8705 - val_f1_m: 0.1243\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.51620\n",
      "Epoch 93/100\n",
      "32/32 - 1s - loss: 0.8375 - accuracy: 0.7820 - precision_m: 0.1980 - recall_m: 0.9363 - f1_m: 0.3202 - val_loss: 0.9222 - val_accuracy: 0.5183 - val_precision_m: 0.0690 - val_recall_m: 0.8728 - val_f1_m: 0.1268\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.51620\n",
      "Epoch 94/100\n",
      "32/32 - 1s - loss: 0.8335 - accuracy: 0.7973 - precision_m: 0.2131 - recall_m: 0.9427 - f1_m: 0.3390 - val_loss: 0.9222 - val_accuracy: 0.5129 - val_precision_m: 0.0688 - val_recall_m: 0.8799 - val_f1_m: 0.1266\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.51620\n",
      "Epoch 95/100\n",
      "32/32 - 1s - loss: 0.8364 - accuracy: 0.7819 - precision_m: 0.1995 - recall_m: 0.9455 - f1_m: 0.3224 - val_loss: 0.9264 - val_accuracy: 0.4535 - val_precision_m: 0.0624 - val_recall_m: 0.9030 - val_f1_m: 0.1159\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.51620\n",
      "Epoch 96/100\n",
      "32/32 - 1s - loss: 0.8366 - accuracy: 0.7890 - precision_m: 0.2017 - recall_m: 0.9331 - f1_m: 0.3228 - val_loss: 0.9222 - val_accuracy: 0.5346 - val_precision_m: 0.0693 - val_recall_m: 0.8514 - val_f1_m: 0.1272\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.51620\n",
      "Epoch 97/100\n",
      "32/32 - 1s - loss: 0.8355 - accuracy: 0.7988 - precision_m: 0.2067 - recall_m: 0.9260 - f1_m: 0.3304 - val_loss: 0.9242 - val_accuracy: 0.4898 - val_precision_m: 0.0658 - val_recall_m: 0.8904 - val_f1_m: 0.1216\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.51620\n",
      "Epoch 98/100\n",
      "32/32 - 1s - loss: 0.8350 - accuracy: 0.7956 - precision_m: 0.2085 - recall_m: 0.9460 - f1_m: 0.3317 - val_loss: 0.9227 - val_accuracy: 0.5220 - val_precision_m: 0.0684 - val_recall_m: 0.8579 - val_f1_m: 0.1256\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.51620\n",
      "Epoch 99/100\n",
      "32/32 - 1s - loss: 0.8333 - accuracy: 0.7990 - precision_m: 0.2094 - recall_m: 0.9479 - f1_m: 0.3351 - val_loss: 0.9219 - val_accuracy: 0.5145 - val_precision_m: 0.0693 - val_recall_m: 0.8781 - val_f1_m: 0.1274\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.51620\n",
      "Epoch 100/100\n",
      "32/32 - 1s - loss: 0.8366 - accuracy: 0.7939 - precision_m: 0.2042 - recall_m: 0.9281 - f1_m: 0.3256 - val_loss: 0.9199 - val_accuracy: 0.5484 - val_precision_m: 0.0735 - val_recall_m: 0.8637 - val_f1_m: 0.1342\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.51620\n",
      "5/5 - 0s - loss: 0.8655 - accuracy: 0.6499 - precision_m: 0.1295 - recall_m: 0.8745 - f1_m: 0.2253\n",
      "The metrics for the test set with loss tversky, learning rate 1e-05,  filters 16, and batch size 8 is [0.86551433801651, 0.64986252784729, 0.12945909798145294, 0.8744634389877319, 0.22531375288963318]\n",
      "__________________________________________________________________________________________________________________________________________________________________________\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 128, 128, 5) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 128, 128, 32) 1472        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 128, 128, 32) 128         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 128, 128, 32) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 32) 9248        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 32) 128         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, 128, 32) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 64, 64, 32)   0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 64)   18496       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64, 64, 64)   256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 64, 64, 64)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 64, 64)   36928       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 64)   256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64, 64, 64)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 64)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 128)  73856       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 128)  512         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 128)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 128)  147584      activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 128)  512         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 128)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 128)  0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 16, 16, 256)  295168      max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 16, 16, 256)  1024        conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 16, 16, 256)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 16, 16, 256)  590080      activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 16, 16, 256)  1024        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16, 16, 256)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 256)    0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 8, 8, 512)    1180160     max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 8, 8, 512)    2048        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 8, 8, 512)    0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 8, 8, 512)    2359808     activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 8, 8, 512)    2048        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 8, 8, 512)    0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 16, 16, 128)  262272      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 16, 16, 384)  0           conv2d_transpose[0][0]           \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 256)  884992      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 256)  1024        conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 256)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 256)  590080      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 256)  1024        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 256)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 32, 32, 64)   65600       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 192)  0           conv2d_transpose_1[0][0]         \n",
      "                                                                 activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 128)  221312      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 32, 32, 128)  512         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 32, 32, 128)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 32, 128)  147584      activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 32, 32, 128)  512         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 32, 32, 128)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 64, 64, 32)   16416       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 64, 64, 96)   0           conv2d_transpose_2[0][0]         \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 64)   55360       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64, 64, 64)   256         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 64, 64, 64)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 64)   36928       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 64, 64, 64)   256         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 64, 64, 64)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 128, 128, 16) 4112        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 128, 128, 48) 0           conv2d_transpose_3[0][0]         \n",
      "                                                                 activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 128, 128, 32) 13856       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 128, 128, 32) 128         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 128, 128, 32) 0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 128, 128, 32) 9248        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 128, 128, 32) 128         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 128, 128, 32) 0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 128, 128, 1)  33          activation_17[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 7,032,369\n",
      "Trainable params: 7,026,481\n",
      "Non-trainable params: 5,888\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "Initialising in 3..........2..........1......................................\n",
      "Training on 284 images with learning rate 1e-05, batch size 8, and number of filters 32\n",
      "Epoch 1/100\n",
      "32/32 - 3s - loss: 0.9231 - accuracy: 0.1156 - precision_m: 0.0592 - recall_m: 0.9892 - f1_m: 0.1109 - val_loss: 0.9456 - val_accuracy: 0.1768 - val_precision_m: 0.0414 - val_recall_m: 0.9415 - val_f1_m: 0.0790\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.51620\n",
      "Epoch 2/100\n",
      "32/32 - 1s - loss: 0.9221 - accuracy: 0.1325 - precision_m: 0.0603 - recall_m: 0.9889 - f1_m: 0.1121 - val_loss: 0.9454 - val_accuracy: 0.1411 - val_precision_m: 0.0414 - val_recall_m: 0.9716 - val_f1_m: 0.0792\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.51620\n",
      "Epoch 3/100\n",
      "32/32 - 1s - loss: 0.9210 - accuracy: 0.1516 - precision_m: 0.0614 - recall_m: 0.9836 - f1_m: 0.1142 - val_loss: 0.9451 - val_accuracy: 0.1346 - val_precision_m: 0.0416 - val_recall_m: 0.9806 - val_f1_m: 0.0796\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.51620\n",
      "Epoch 4/100\n",
      "32/32 - 1s - loss: 0.9196 - accuracy: 0.1774 - precision_m: 0.0627 - recall_m: 0.9809 - f1_m: 0.1167 - val_loss: 0.9447 - val_accuracy: 0.1419 - val_precision_m: 0.0419 - val_recall_m: 0.9789 - val_f1_m: 0.0801\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.51620\n",
      "Epoch 5/100\n",
      "32/32 - 1s - loss: 0.9184 - accuracy: 0.2067 - precision_m: 0.0641 - recall_m: 0.9682 - f1_m: 0.1187 - val_loss: 0.9444 - val_accuracy: 0.1503 - val_precision_m: 0.0423 - val_recall_m: 0.9775 - val_f1_m: 0.0807\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.51620\n",
      "Epoch 6/100\n",
      "32/32 - 1s - loss: 0.9163 - accuracy: 0.2392 - precision_m: 0.0668 - recall_m: 0.9647 - f1_m: 0.1237 - val_loss: 0.9440 - val_accuracy: 0.1643 - val_precision_m: 0.0425 - val_recall_m: 0.9700 - val_f1_m: 0.0811\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.51620\n",
      "Epoch 7/100\n",
      "32/32 - 1s - loss: 0.9140 - accuracy: 0.2846 - precision_m: 0.0695 - recall_m: 0.9420 - f1_m: 0.1279 - val_loss: 0.9434 - val_accuracy: 0.1874 - val_precision_m: 0.0430 - val_recall_m: 0.9578 - val_f1_m: 0.0820\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.51620\n",
      "Epoch 8/100\n",
      "32/32 - 1s - loss: 0.9111 - accuracy: 0.3254 - precision_m: 0.0732 - recall_m: 0.9415 - f1_m: 0.1341 - val_loss: 0.9422 - val_accuracy: 0.2222 - val_precision_m: 0.0457 - val_recall_m: 0.9671 - val_f1_m: 0.0868\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.51620\n",
      "Epoch 9/100\n",
      "32/32 - 1s - loss: 0.9082 - accuracy: 0.3623 - precision_m: 0.0762 - recall_m: 0.9273 - f1_m: 0.1393 - val_loss: 0.9404 - val_accuracy: 0.2531 - val_precision_m: 0.0480 - val_recall_m: 0.9716 - val_f1_m: 0.0911\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.51620\n",
      "Epoch 10/100\n",
      "32/32 - 1s - loss: 0.9051 - accuracy: 0.3890 - precision_m: 0.0801 - recall_m: 0.9412 - f1_m: 0.1449 - val_loss: 0.9402 - val_accuracy: 0.2438 - val_precision_m: 0.0478 - val_recall_m: 0.9776 - val_f1_m: 0.0907\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.51620\n",
      "Epoch 11/100\n",
      "32/32 - 1s - loss: 0.9029 - accuracy: 0.4095 - precision_m: 0.0813 - recall_m: 0.9248 - f1_m: 0.1474 - val_loss: 0.9395 - val_accuracy: 0.2392 - val_precision_m: 0.0474 - val_recall_m: 0.9782 - val_f1_m: 0.0900\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.51620\n",
      "Epoch 12/100\n",
      "32/32 - 1s - loss: 0.8982 - accuracy: 0.4295 - precision_m: 0.0860 - recall_m: 0.9386 - f1_m: 0.1554 - val_loss: 0.9403 - val_accuracy: 0.2081 - val_precision_m: 0.0455 - val_recall_m: 0.9766 - val_f1_m: 0.0866\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.51620\n",
      "Epoch 13/100\n",
      "32/32 - 1s - loss: 0.8954 - accuracy: 0.4496 - precision_m: 0.0892 - recall_m: 0.9370 - f1_m: 0.1602 - val_loss: 0.9393 - val_accuracy: 0.2238 - val_precision_m: 0.0467 - val_recall_m: 0.9800 - val_f1_m: 0.0887\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.51620\n",
      "Epoch 14/100\n",
      "32/32 - 1s - loss: 0.8908 - accuracy: 0.4756 - precision_m: 0.0943 - recall_m: 0.9586 - f1_m: 0.1684 - val_loss: 0.9397 - val_accuracy: 0.2152 - val_precision_m: 0.0455 - val_recall_m: 0.9694 - val_f1_m: 0.0865\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.51620\n",
      "Epoch 15/100\n",
      "32/32 - 1s - loss: 0.8876 - accuracy: 0.4867 - precision_m: 0.0964 - recall_m: 0.9646 - f1_m: 0.1725 - val_loss: 0.9413 - val_accuracy: 0.1362 - val_precision_m: 0.0423 - val_recall_m: 0.9896 - val_f1_m: 0.0808\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.51620\n",
      "Epoch 16/100\n",
      "32/32 - 1s - loss: 0.8860 - accuracy: 0.5034 - precision_m: 0.0978 - recall_m: 0.9477 - f1_m: 0.1747 - val_loss: 0.9390 - val_accuracy: 0.1895 - val_precision_m: 0.0450 - val_recall_m: 0.9855 - val_f1_m: 0.0856\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.51620\n",
      "Epoch 17/100\n",
      "32/32 - 1s - loss: 0.8800 - accuracy: 0.5295 - precision_m: 0.1039 - recall_m: 0.9626 - f1_m: 0.1846 - val_loss: 0.9410 - val_accuracy: 0.1313 - val_precision_m: 0.0422 - val_recall_m: 0.9930 - val_f1_m: 0.0807\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.51620\n",
      "Epoch 18/100\n",
      "32/32 - 2s - loss: 0.8746 - accuracy: 0.5573 - precision_m: 0.1103 - recall_m: 0.9703 - f1_m: 0.1953 - val_loss: 0.9403 - val_accuracy: 0.1443 - val_precision_m: 0.0429 - val_recall_m: 0.9922 - val_f1_m: 0.0819\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.51620\n",
      "Epoch 19/100\n",
      "32/32 - 1s - loss: 0.8741 - accuracy: 0.5754 - precision_m: 0.1119 - recall_m: 0.9554 - f1_m: 0.1962 - val_loss: 0.9417 - val_accuracy: 0.0998 - val_precision_m: 0.0408 - val_recall_m: 0.9955 - val_f1_m: 0.0781\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.51620\n",
      "Epoch 20/100\n",
      "32/32 - 1s - loss: 0.8680 - accuracy: 0.5934 - precision_m: 0.1178 - recall_m: 0.9566 - f1_m: 0.2062 - val_loss: 0.9377 - val_accuracy: 0.2125 - val_precision_m: 0.0462 - val_recall_m: 0.9819 - val_f1_m: 0.0879\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.51620\n",
      "Epoch 21/100\n",
      "32/32 - 1s - loss: 0.8643 - accuracy: 0.6310 - precision_m: 0.1252 - recall_m: 0.9381 - f1_m: 0.2168 - val_loss: 0.9392 - val_accuracy: 0.1623 - val_precision_m: 0.0439 - val_recall_m: 0.9907 - val_f1_m: 0.0836\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.51620\n",
      "Epoch 22/100\n",
      "32/32 - 1s - loss: 0.8632 - accuracy: 0.6375 - precision_m: 0.1270 - recall_m: 0.9315 - f1_m: 0.2200 - val_loss: 0.9353 - val_accuracy: 0.3148 - val_precision_m: 0.0481 - val_recall_m: 0.9080 - val_f1_m: 0.0908\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.51620\n",
      "Epoch 23/100\n",
      "32/32 - 1s - loss: 0.8744 - accuracy: 0.6109 - precision_m: 0.1116 - recall_m: 0.8481 - f1_m: 0.1928 - val_loss: 0.9370 - val_accuracy: 0.2181 - val_precision_m: 0.0468 - val_recall_m: 0.9831 - val_f1_m: 0.0888\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.51620\n",
      "Epoch 24/100\n",
      "32/32 - 1s - loss: 0.8573 - accuracy: 0.6542 - precision_m: 0.1346 - recall_m: 0.9234 - f1_m: 0.2276 - val_loss: 0.9332 - val_accuracy: 0.3424 - val_precision_m: 0.0510 - val_recall_m: 0.9124 - val_f1_m: 0.0959\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.51620\n",
      "Epoch 25/100\n",
      "32/32 - 1s - loss: 0.8542 - accuracy: 0.6737 - precision_m: 0.1385 - recall_m: 0.9324 - f1_m: 0.2361 - val_loss: 0.9369 - val_accuracy: 0.2315 - val_precision_m: 0.0475 - val_recall_m: 0.9806 - val_f1_m: 0.0901\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.51620\n",
      "Epoch 26/100\n",
      "32/32 - 1s - loss: 0.8522 - accuracy: 0.6932 - precision_m: 0.1440 - recall_m: 0.9135 - f1_m: 0.2446 - val_loss: 0.9328 - val_accuracy: 0.3469 - val_precision_m: 0.0522 - val_recall_m: 0.9242 - val_f1_m: 0.0983\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.51620\n",
      "Epoch 27/100\n",
      "32/32 - 1s - loss: 0.8468 - accuracy: 0.6957 - precision_m: 0.1494 - recall_m: 0.9429 - f1_m: 0.2537 - val_loss: 0.9337 - val_accuracy: 0.3038 - val_precision_m: 0.0516 - val_recall_m: 0.9638 - val_f1_m: 0.0974\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.51620\n",
      "Epoch 28/100\n",
      "32/32 - 1s - loss: 0.8433 - accuracy: 0.7156 - precision_m: 0.1564 - recall_m: 0.9336 - f1_m: 0.2618 - val_loss: 0.9323 - val_accuracy: 0.3344 - val_precision_m: 0.0536 - val_recall_m: 0.9573 - val_f1_m: 0.1010\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.51620\n",
      "Epoch 29/100\n",
      "32/32 - 1s - loss: 0.8386 - accuracy: 0.7249 - precision_m: 0.1627 - recall_m: 0.9356 - f1_m: 0.2727 - val_loss: 0.9284 - val_accuracy: 0.4185 - val_precision_m: 0.0581 - val_recall_m: 0.9079 - val_f1_m: 0.1084\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.51620\n",
      "Epoch 30/100\n",
      "32/32 - 1s - loss: 0.8419 - accuracy: 0.7288 - precision_m: 0.1608 - recall_m: 0.9073 - f1_m: 0.2651 - val_loss: 0.9293 - val_accuracy: 0.4174 - val_precision_m: 0.0572 - val_recall_m: 0.8945 - val_f1_m: 0.1068\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.51620\n",
      "Epoch 31/100\n",
      "32/32 - 1s - loss: 0.8416 - accuracy: 0.7255 - precision_m: 0.1606 - recall_m: 0.9077 - f1_m: 0.2647 - val_loss: 0.9239 - val_accuracy: 0.5165 - val_precision_m: 0.0638 - val_recall_m: 0.8189 - val_f1_m: 0.1174\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.51620\n",
      "Epoch 32/100\n",
      "32/32 - 1s - loss: 0.8462 - accuracy: 0.7169 - precision_m: 0.1527 - recall_m: 0.8791 - f1_m: 0.2529 - val_loss: 0.9248 - val_accuracy: 0.5089 - val_precision_m: 0.0612 - val_recall_m: 0.8003 - val_f1_m: 0.1126\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.51620\n",
      "Epoch 33/100\n",
      "32/32 - 1s - loss: 0.8364 - accuracy: 0.7332 - precision_m: 0.1647 - recall_m: 0.9086 - f1_m: 0.2734 - val_loss: 0.9242 - val_accuracy: 0.4856 - val_precision_m: 0.0617 - val_recall_m: 0.8394 - val_f1_m: 0.1140\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.51620\n",
      "Epoch 34/100\n",
      "32/32 - 1s - loss: 0.8310 - accuracy: 0.7528 - precision_m: 0.1763 - recall_m: 0.9159 - f1_m: 0.2886 - val_loss: 0.9259 - val_accuracy: 0.4524 - val_precision_m: 0.0603 - val_recall_m: 0.8859 - val_f1_m: 0.1122\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.51620\n",
      "Epoch 35/100\n",
      "32/32 - 1s - loss: 0.8326 - accuracy: 0.7515 - precision_m: 0.1725 - recall_m: 0.9147 - f1_m: 0.2824 - val_loss: 0.9276 - val_accuracy: 0.4247 - val_precision_m: 0.0582 - val_recall_m: 0.8894 - val_f1_m: 0.1085\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.51620\n",
      "Epoch 36/100\n",
      "32/32 - 1s - loss: 0.8314 - accuracy: 0.7667 - precision_m: 0.1787 - recall_m: 0.8962 - f1_m: 0.2920 - val_loss: 0.9254 - val_accuracy: 0.4611 - val_precision_m: 0.0603 - val_recall_m: 0.8614 - val_f1_m: 0.1118\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.51620\n",
      "Epoch 37/100\n",
      "32/32 - 1s - loss: 0.8250 - accuracy: 0.7721 - precision_m: 0.1874 - recall_m: 0.9182 - f1_m: 0.3039 - val_loss: 0.9243 - val_accuracy: 0.4607 - val_precision_m: 0.0629 - val_recall_m: 0.8951 - val_f1_m: 0.1168\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.51620\n",
      "Epoch 38/100\n",
      "32/32 - 1s - loss: 0.8327 - accuracy: 0.7673 - precision_m: 0.1764 - recall_m: 0.8877 - f1_m: 0.2859 - val_loss: 0.9248 - val_accuracy: 0.4401 - val_precision_m: 0.0614 - val_recall_m: 0.9033 - val_f1_m: 0.1142\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.51620\n",
      "Epoch 39/100\n",
      "32/32 - 1s - loss: 0.8284 - accuracy: 0.7722 - precision_m: 0.1824 - recall_m: 0.8908 - f1_m: 0.2952 - val_loss: 0.9228 - val_accuracy: 0.4657 - val_precision_m: 0.0646 - val_recall_m: 0.8973 - val_f1_m: 0.1196\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.51620\n",
      "Epoch 40/100\n",
      "32/32 - 1s - loss: 0.8236 - accuracy: 0.7773 - precision_m: 0.1897 - recall_m: 0.9013 - f1_m: 0.3093 - val_loss: 0.9230 - val_accuracy: 0.4588 - val_precision_m: 0.0641 - val_recall_m: 0.9035 - val_f1_m: 0.1190\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.51620\n",
      "Epoch 41/100\n",
      "32/32 - 1s - loss: 0.8284 - accuracy: 0.7670 - precision_m: 0.1818 - recall_m: 0.8896 - f1_m: 0.2948 - val_loss: 0.9260 - val_accuracy: 0.4141 - val_precision_m: 0.0587 - val_recall_m: 0.9057 - val_f1_m: 0.1095\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.51620\n",
      "Epoch 42/100\n",
      "32/32 - 1s - loss: 0.8252 - accuracy: 0.7731 - precision_m: 0.1849 - recall_m: 0.9009 - f1_m: 0.3015 - val_loss: 0.9204 - val_accuracy: 0.4885 - val_precision_m: 0.0663 - val_recall_m: 0.8829 - val_f1_m: 0.1225\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.51620\n",
      "Epoch 43/100\n",
      "32/32 - 1s - loss: 0.8240 - accuracy: 0.7761 - precision_m: 0.1867 - recall_m: 0.9035 - f1_m: 0.3003 - val_loss: 0.9222 - val_accuracy: 0.4954 - val_precision_m: 0.0642 - val_recall_m: 0.8477 - val_f1_m: 0.1183\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.51620\n",
      "Epoch 44/100\n",
      "32/32 - 1s - loss: 0.8172 - accuracy: 0.7988 - precision_m: 0.2069 - recall_m: 0.8955 - f1_m: 0.3260 - val_loss: 0.9200 - val_accuracy: 0.5266 - val_precision_m: 0.0674 - val_recall_m: 0.8252 - val_f1_m: 0.1234\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.51620\n",
      "Epoch 45/100\n",
      "32/32 - 1s - loss: 0.8181 - accuracy: 0.7881 - precision_m: 0.1970 - recall_m: 0.9118 - f1_m: 0.3157 - val_loss: 0.9197 - val_accuracy: 0.5231 - val_precision_m: 0.0677 - val_recall_m: 0.8336 - val_f1_m: 0.1242\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.51620\n",
      "Epoch 46/100\n",
      "32/32 - 2s - loss: 0.8147 - accuracy: 0.8001 - precision_m: 0.2071 - recall_m: 0.8999 - f1_m: 0.3299 - val_loss: 0.9195 - val_accuracy: 0.5205 - val_precision_m: 0.0697 - val_recall_m: 0.8553 - val_f1_m: 0.1278\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.51620\n",
      "Epoch 47/100\n",
      "32/32 - 1s - loss: 0.8144 - accuracy: 0.8020 - precision_m: 0.2082 - recall_m: 0.9134 - f1_m: 0.3313 - val_loss: 0.9200 - val_accuracy: 0.4996 - val_precision_m: 0.0670 - val_recall_m: 0.8713 - val_f1_m: 0.1235\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.51620\n",
      "Epoch 48/100\n",
      "32/32 - 1s - loss: 0.8129 - accuracy: 0.8117 - precision_m: 0.2150 - recall_m: 0.8821 - f1_m: 0.3384 - val_loss: 0.9184 - val_accuracy: 0.5566 - val_precision_m: 0.0717 - val_recall_m: 0.8080 - val_f1_m: 0.1303\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.51620\n",
      "Epoch 49/100\n",
      "32/32 - 1s - loss: 0.8138 - accuracy: 0.8090 - precision_m: 0.2137 - recall_m: 0.8730 - f1_m: 0.3368 - val_loss: 0.9154 - val_accuracy: 0.6008 - val_precision_m: 0.0774 - val_recall_m: 0.7790 - val_f1_m: 0.1392\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.51620\n",
      "Epoch 50/100\n",
      "32/32 - 1s - loss: 0.8114 - accuracy: 0.8148 - precision_m: 0.2175 - recall_m: 0.8956 - f1_m: 0.3419 - val_loss: 0.9171 - val_accuracy: 0.5335 - val_precision_m: 0.0726 - val_recall_m: 0.8614 - val_f1_m: 0.1327\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.51620\n",
      "Epoch 51/100\n",
      "32/32 - 1s - loss: 0.8091 - accuracy: 0.8150 - precision_m: 0.2208 - recall_m: 0.9023 - f1_m: 0.3451 - val_loss: 0.9137 - val_accuracy: 0.5918 - val_precision_m: 0.0775 - val_recall_m: 0.8049 - val_f1_m: 0.1399\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.51620\n",
      "Epoch 52/100\n",
      "32/32 - 1s - loss: 0.8175 - accuracy: 0.8058 - precision_m: 0.2076 - recall_m: 0.8839 - f1_m: 0.3273 - val_loss: 0.9182 - val_accuracy: 0.5066 - val_precision_m: 0.0701 - val_recall_m: 0.8832 - val_f1_m: 0.1290\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.51620\n",
      "Epoch 53/100\n",
      "32/32 - 1s - loss: 0.8131 - accuracy: 0.8151 - precision_m: 0.2171 - recall_m: 0.8668 - f1_m: 0.3383 - val_loss: 0.9156 - val_accuracy: 0.5490 - val_precision_m: 0.0725 - val_recall_m: 0.8334 - val_f1_m: 0.1322\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.51620\n",
      "Epoch 54/100\n",
      "32/32 - 1s - loss: 0.8067 - accuracy: 0.8222 - precision_m: 0.2276 - recall_m: 0.9040 - f1_m: 0.3502 - val_loss: 0.9180 - val_accuracy: 0.5206 - val_precision_m: 0.0700 - val_recall_m: 0.8604 - val_f1_m: 0.1285\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.51620\n",
      "Epoch 55/100\n",
      "32/32 - 1s - loss: 0.8085 - accuracy: 0.8242 - precision_m: 0.2256 - recall_m: 0.8889 - f1_m: 0.3477 - val_loss: 0.9136 - val_accuracy: 0.5786 - val_precision_m: 0.0772 - val_recall_m: 0.8212 - val_f1_m: 0.1398\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.51620\n",
      "Epoch 56/100\n",
      "32/32 - 1s - loss: 0.8066 - accuracy: 0.8180 - precision_m: 0.2231 - recall_m: 0.8902 - f1_m: 0.3454 - val_loss: 0.9134 - val_accuracy: 0.6011 - val_precision_m: 0.0781 - val_recall_m: 0.7887 - val_f1_m: 0.1406\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.51620\n",
      "Epoch 57/100\n",
      "32/32 - 1s - loss: 0.8191 - accuracy: 0.7989 - precision_m: 0.2023 - recall_m: 0.8759 - f1_m: 0.3158 - val_loss: 0.9172 - val_accuracy: 0.5549 - val_precision_m: 0.0702 - val_recall_m: 0.8037 - val_f1_m: 0.1279\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.51620\n",
      "Epoch 58/100\n",
      "32/32 - 1s - loss: 0.8151 - accuracy: 0.7968 - precision_m: 0.2004 - recall_m: 0.8795 - f1_m: 0.3191 - val_loss: 0.9131 - val_accuracy: 0.5788 - val_precision_m: 0.0753 - val_recall_m: 0.8030 - val_f1_m: 0.1361\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.51620\n",
      "Epoch 59/100\n",
      "32/32 - 1s - loss: 0.8063 - accuracy: 0.8133 - precision_m: 0.2184 - recall_m: 0.8937 - f1_m: 0.3369 - val_loss: 0.9168 - val_accuracy: 0.5549 - val_precision_m: 0.0710 - val_recall_m: 0.8115 - val_f1_m: 0.1293\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.51620\n",
      "Epoch 60/100\n",
      "32/32 - 1s - loss: 0.8060 - accuracy: 0.8248 - precision_m: 0.2269 - recall_m: 0.8869 - f1_m: 0.3466 - val_loss: 0.9128 - val_accuracy: 0.6076 - val_precision_m: 0.0781 - val_recall_m: 0.7795 - val_f1_m: 0.1402\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.51620\n",
      "Epoch 61/100\n",
      "32/32 - 1s - loss: 0.8082 - accuracy: 0.8177 - precision_m: 0.2189 - recall_m: 0.8822 - f1_m: 0.3460 - val_loss: 0.9146 - val_accuracy: 0.5609 - val_precision_m: 0.0743 - val_recall_m: 0.8299 - val_f1_m: 0.1351\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.51620\n",
      "Epoch 62/100\n",
      "32/32 - 1s - loss: 0.8037 - accuracy: 0.8240 - precision_m: 0.2271 - recall_m: 0.8963 - f1_m: 0.3500 - val_loss: 0.9139 - val_accuracy: 0.5637 - val_precision_m: 0.0752 - val_recall_m: 0.8331 - val_f1_m: 0.1368\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.51620\n",
      "Epoch 63/100\n",
      "32/32 - 1s - loss: 0.7985 - accuracy: 0.8391 - precision_m: 0.2462 - recall_m: 0.8841 - f1_m: 0.3757 - val_loss: 0.9179 - val_accuracy: 0.5109 - val_precision_m: 0.0692 - val_recall_m: 0.8761 - val_f1_m: 0.1273\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.51620\n",
      "Epoch 64/100\n",
      "32/32 - 1s - loss: 0.7948 - accuracy: 0.8412 - precision_m: 0.2488 - recall_m: 0.9052 - f1_m: 0.3794 - val_loss: 0.9148 - val_accuracy: 0.5490 - val_precision_m: 0.0747 - val_recall_m: 0.8561 - val_f1_m: 0.1363\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.51620\n",
      "Epoch 65/100\n",
      "32/32 - 1s - loss: 0.8019 - accuracy: 0.8332 - precision_m: 0.2355 - recall_m: 0.8660 - f1_m: 0.3599 - val_loss: 0.9129 - val_accuracy: 0.5485 - val_precision_m: 0.0764 - val_recall_m: 0.8769 - val_f1_m: 0.1395\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.51620\n",
      "Epoch 66/100\n",
      "32/32 - 1s - loss: 0.7959 - accuracy: 0.8400 - precision_m: 0.2494 - recall_m: 0.8969 - f1_m: 0.3759 - val_loss: 0.9109 - val_accuracy: 0.6047 - val_precision_m: 0.0808 - val_recall_m: 0.8051 - val_f1_m: 0.1454\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.51620\n",
      "Epoch 67/100\n",
      "32/32 - 1s - loss: 0.7970 - accuracy: 0.8374 - precision_m: 0.2464 - recall_m: 0.8712 - f1_m: 0.3771 - val_loss: 0.9092 - val_accuracy: 0.6103 - val_precision_m: 0.0829 - val_recall_m: 0.8092 - val_f1_m: 0.1488\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.51620\n",
      "Epoch 68/100\n",
      "32/32 - 1s - loss: 0.8008 - accuracy: 0.8214 - precision_m: 0.2248 - recall_m: 0.8960 - f1_m: 0.3505 - val_loss: 0.9153 - val_accuracy: 0.5368 - val_precision_m: 0.0725 - val_recall_m: 0.8550 - val_f1_m: 0.1327\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.51620\n",
      "Epoch 69/100\n",
      "32/32 - 1s - loss: 0.7991 - accuracy: 0.8360 - precision_m: 0.2434 - recall_m: 0.8862 - f1_m: 0.3671 - val_loss: 0.9108 - val_accuracy: 0.6250 - val_precision_m: 0.0812 - val_recall_m: 0.7690 - val_f1_m: 0.1450\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.51620\n",
      "Epoch 70/100\n",
      "32/32 - 1s - loss: 0.7950 - accuracy: 0.8389 - precision_m: 0.2470 - recall_m: 0.8850 - f1_m: 0.3715 - val_loss: 0.9153 - val_accuracy: 0.5472 - val_precision_m: 0.0726 - val_recall_m: 0.8391 - val_f1_m: 0.1324\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.51620\n",
      "Epoch 71/100\n",
      "32/32 - 1s - loss: 0.7917 - accuracy: 0.8518 - precision_m: 0.2591 - recall_m: 0.8875 - f1_m: 0.3893 - val_loss: 0.9108 - val_accuracy: 0.6142 - val_precision_m: 0.0807 - val_recall_m: 0.7873 - val_f1_m: 0.1445\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.51620\n",
      "Epoch 72/100\n",
      "32/32 - 1s - loss: 0.7942 - accuracy: 0.8410 - precision_m: 0.2493 - recall_m: 0.8963 - f1_m: 0.3756 - val_loss: 0.9133 - val_accuracy: 0.5832 - val_precision_m: 0.0768 - val_recall_m: 0.8104 - val_f1_m: 0.1388\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.51620\n",
      "Epoch 73/100\n",
      "32/32 - 1s - loss: 0.7917 - accuracy: 0.8457 - precision_m: 0.2558 - recall_m: 0.8911 - f1_m: 0.3815 - val_loss: 0.9154 - val_accuracy: 0.5416 - val_precision_m: 0.0722 - val_recall_m: 0.8517 - val_f1_m: 0.1319\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.51620\n",
      "Epoch 74/100\n",
      "32/32 - 1s - loss: 0.7848 - accuracy: 0.8682 - precision_m: 0.2882 - recall_m: 0.8705 - f1_m: 0.4226 - val_loss: 0.9125 - val_accuracy: 0.5793 - val_precision_m: 0.0773 - val_recall_m: 0.8244 - val_f1_m: 0.1400\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.51620\n",
      "Epoch 75/100\n",
      "32/32 - 1s - loss: 0.8016 - accuracy: 0.8211 - precision_m: 0.2225 - recall_m: 0.8848 - f1_m: 0.3443 - val_loss: 0.9082 - val_accuracy: 0.5838 - val_precision_m: 0.0817 - val_recall_m: 0.8477 - val_f1_m: 0.1477\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.51620\n",
      "Epoch 76/100\n",
      "32/32 - 1s - loss: 0.7936 - accuracy: 0.8484 - precision_m: 0.2563 - recall_m: 0.8788 - f1_m: 0.3816 - val_loss: 0.9071 - val_accuracy: 0.6478 - val_precision_m: 0.0860 - val_recall_m: 0.7557 - val_f1_m: 0.1524\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.51620\n",
      "Epoch 77/100\n",
      "32/32 - 1s - loss: 0.7933 - accuracy: 0.8574 - precision_m: 0.2676 - recall_m: 0.8658 - f1_m: 0.3914 - val_loss: 0.9154 - val_accuracy: 0.5391 - val_precision_m: 0.0712 - val_recall_m: 0.8444 - val_f1_m: 0.1302\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.51620\n",
      "Epoch 78/100\n",
      "32/32 - 1s - loss: 0.7901 - accuracy: 0.8542 - precision_m: 0.2629 - recall_m: 0.8779 - f1_m: 0.3912 - val_loss: 0.9060 - val_accuracy: 0.6311 - val_precision_m: 0.0867 - val_recall_m: 0.7834 - val_f1_m: 0.1543\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.51620\n",
      "Epoch 79/100\n",
      "32/32 - 1s - loss: 0.7897 - accuracy: 0.8536 - precision_m: 0.2610 - recall_m: 0.8851 - f1_m: 0.3929 - val_loss: 0.9074 - val_accuracy: 0.7090 - val_precision_m: 0.0905 - val_recall_m: 0.6508 - val_f1_m: 0.1551\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.51620\n",
      "Epoch 80/100\n",
      "32/32 - 1s - loss: 0.7915 - accuracy: 0.8494 - precision_m: 0.2528 - recall_m: 0.8776 - f1_m: 0.3845 - val_loss: 0.9035 - val_accuracy: 0.6516 - val_precision_m: 0.0903 - val_recall_m: 0.7774 - val_f1_m: 0.1595\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.51620\n",
      "Epoch 81/100\n",
      "32/32 - 1s - loss: 0.7769 - accuracy: 0.8698 - precision_m: 0.2929 - recall_m: 0.8994 - f1_m: 0.4274 - val_loss: 0.9046 - val_accuracy: 0.6312 - val_precision_m: 0.0887 - val_recall_m: 0.8136 - val_f1_m: 0.1580\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.51620\n",
      "Epoch 82/100\n",
      "32/32 - 1s - loss: 0.7819 - accuracy: 0.8617 - precision_m: 0.2831 - recall_m: 0.8941 - f1_m: 0.4090 - val_loss: 0.9097 - val_accuracy: 0.5713 - val_precision_m: 0.0798 - val_recall_m: 0.8655 - val_f1_m: 0.1448\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.51620\n",
      "Epoch 83/100\n",
      "32/32 - 1s - loss: 0.7856 - accuracy: 0.8616 - precision_m: 0.2754 - recall_m: 0.8683 - f1_m: 0.4029 - val_loss: 0.9177 - val_accuracy: 0.5298 - val_precision_m: 0.0680 - val_recall_m: 0.8400 - val_f1_m: 0.1246\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.51620\n",
      "Epoch 84/100\n",
      "32/32 - 1s - loss: 0.8004 - accuracy: 0.8517 - precision_m: 0.2453 - recall_m: 0.8227 - f1_m: 0.3686 - val_loss: 0.9078 - val_accuracy: 0.6040 - val_precision_m: 0.0816 - val_recall_m: 0.7951 - val_f1_m: 0.1467\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.51620\n",
      "Epoch 85/100\n",
      "32/32 - 1s - loss: 0.7796 - accuracy: 0.8619 - precision_m: 0.2794 - recall_m: 0.8914 - f1_m: 0.4148 - val_loss: 0.9060 - val_accuracy: 0.6288 - val_precision_m: 0.0860 - val_recall_m: 0.7881 - val_f1_m: 0.1533\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.51620\n",
      "Epoch 86/100\n",
      "32/32 - 1s - loss: 0.7816 - accuracy: 0.8636 - precision_m: 0.2759 - recall_m: 0.8881 - f1_m: 0.4069 - val_loss: 0.9060 - val_accuracy: 0.6754 - val_precision_m: 0.0901 - val_recall_m: 0.7112 - val_f1_m: 0.1572\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.51620\n",
      "Epoch 87/100\n",
      "32/32 - 1s - loss: 0.7767 - accuracy: 0.8674 - precision_m: 0.2863 - recall_m: 0.9019 - f1_m: 0.4205 - val_loss: 0.9041 - val_accuracy: 0.6812 - val_precision_m: 0.0931 - val_recall_m: 0.7237 - val_f1_m: 0.1622\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.51620\n",
      "Epoch 88/100\n",
      "32/32 - 2s - loss: 0.7785 - accuracy: 0.8752 - precision_m: 0.2947 - recall_m: 0.8746 - f1_m: 0.4293 - val_loss: 0.9033 - val_accuracy: 0.6948 - val_precision_m: 0.0957 - val_recall_m: 0.7203 - val_f1_m: 0.1659\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.51620\n",
      "Epoch 89/100\n",
      "32/32 - 1s - loss: 0.7687 - accuracy: 0.8817 - precision_m: 0.3171 - recall_m: 0.9013 - f1_m: 0.4574 - val_loss: 0.8968 - val_accuracy: 0.7250 - val_precision_m: 0.1058 - val_recall_m: 0.7452 - val_f1_m: 0.1829\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.51620\n",
      "Epoch 90/100\n",
      "32/32 - 1s - loss: 0.7815 - accuracy: 0.8680 - precision_m: 0.2877 - recall_m: 0.8829 - f1_m: 0.4164 - val_loss: 0.8984 - val_accuracy: 0.6905 - val_precision_m: 0.0991 - val_recall_m: 0.7783 - val_f1_m: 0.1737\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.51620\n",
      "Epoch 91/100\n",
      "32/32 - 1s - loss: 0.7726 - accuracy: 0.8836 - precision_m: 0.3163 - recall_m: 0.8653 - f1_m: 0.4475 - val_loss: 0.8904 - val_accuracy: 0.7830 - val_precision_m: 0.1225 - val_recall_m: 0.6738 - val_f1_m: 0.2033\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.51620\n",
      "Epoch 92/100\n",
      "32/32 - 1s - loss: 0.7799 - accuracy: 0.8721 - precision_m: 0.2932 - recall_m: 0.8756 - f1_m: 0.4229 - val_loss: 0.9034 - val_accuracy: 0.6410 - val_precision_m: 0.0891 - val_recall_m: 0.7987 - val_f1_m: 0.1584\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.51620\n",
      "Epoch 93/100\n",
      "32/32 - 1s - loss: 0.7717 - accuracy: 0.8893 - precision_m: 0.3233 - recall_m: 0.8709 - f1_m: 0.4502 - val_loss: 0.9022 - val_accuracy: 0.6665 - val_precision_m: 0.0932 - val_recall_m: 0.7764 - val_f1_m: 0.1641\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.51620\n",
      "Epoch 94/100\n",
      "32/32 - 1s - loss: 0.7636 - accuracy: 0.8923 - precision_m: 0.3356 - recall_m: 0.8953 - f1_m: 0.4739 - val_loss: 0.8982 - val_accuracy: 0.6973 - val_precision_m: 0.0997 - val_recall_m: 0.7702 - val_f1_m: 0.1741\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.51620\n",
      "Epoch 95/100\n",
      "32/32 - 1s - loss: 0.7694 - accuracy: 0.8801 - precision_m: 0.3078 - recall_m: 0.8980 - f1_m: 0.4467 - val_loss: 0.8958 - val_accuracy: 0.7051 - val_precision_m: 0.1026 - val_recall_m: 0.7663 - val_f1_m: 0.1788\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.51620\n",
      "Epoch 96/100\n",
      "32/32 - 1s - loss: 0.7741 - accuracy: 0.8849 - precision_m: 0.3125 - recall_m: 0.8702 - f1_m: 0.4420 - val_loss: 0.9074 - val_accuracy: 0.6499 - val_precision_m: 0.0846 - val_recall_m: 0.7610 - val_f1_m: 0.1500\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.51620\n",
      "Epoch 97/100\n",
      "32/32 - 1s - loss: 0.7924 - accuracy: 0.8626 - precision_m: 0.2757 - recall_m: 0.8079 - f1_m: 0.3983 - val_loss: 0.9039 - val_accuracy: 0.6570 - val_precision_m: 0.0893 - val_recall_m: 0.7488 - val_f1_m: 0.1575\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.51620\n",
      "Epoch 98/100\n",
      "32/32 - 1s - loss: 0.7897 - accuracy: 0.8508 - precision_m: 0.2533 - recall_m: 0.8523 - f1_m: 0.3762 - val_loss: 0.8983 - val_accuracy: 0.6837 - val_precision_m: 0.0976 - val_recall_m: 0.7407 - val_f1_m: 0.1698\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.51620\n",
      "Epoch 99/100\n",
      "32/32 - 1s - loss: 0.7716 - accuracy: 0.8838 - precision_m: 0.3128 - recall_m: 0.8526 - f1_m: 0.4395 - val_loss: 0.9008 - val_accuracy: 0.6498 - val_precision_m: 0.0928 - val_recall_m: 0.7911 - val_f1_m: 0.1640\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.51620\n",
      "Epoch 100/100\n",
      "32/32 - 1s - loss: 0.7740 - accuracy: 0.8652 - precision_m: 0.2845 - recall_m: 0.9038 - f1_m: 0.4136 - val_loss: 0.8933 - val_accuracy: 0.7039 - val_precision_m: 0.1076 - val_recall_m: 0.7564 - val_f1_m: 0.1857\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.51620\n",
      "5/5 - 0s - loss: 0.8237 - accuracy: 0.7532 - precision_m: 0.1713 - recall_m: 0.8485 - f1_m: 0.2845\n",
      "The metrics for the test set with loss tversky, learning rate 1e-05,  filters 32, and batch size 8 is [0.823663055896759, 0.7532162666320801, 0.17131073772907257, 0.8485276103019714, 0.2844533324241638]\n",
      "__________________________________________________________________________________________________________________________________________________________________________\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 128, 128, 5) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 128, 128, 8)  368         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 128, 128, 8)  32          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 128, 128, 8)  0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 8)  584         activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 8)  32          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, 128, 8)  0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 64, 64, 8)    0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 16)   1168        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64, 64, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 64, 64, 16)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 64, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64, 64, 16)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 16)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 32)   4640        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 32)   128         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 32)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 32)   9248        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 32)   128         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 32)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 32)   0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 16, 16, 64)   18496       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 16, 16, 64)   256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 16, 16, 64)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 16, 16, 64)   36928       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 16, 16, 64)   256         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16, 16, 64)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 64)     0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 8, 8, 128)    73856       max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 8, 8, 128)    512         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 8, 8, 128)    0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 8, 8, 128)    147584      activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 8, 8, 128)    512         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 8, 8, 128)    0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 16, 16, 128)  65664       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 16, 16, 192)  0           conv2d_transpose[0][0]           \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 64)   110656      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 64)   256         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 64)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 64)   36928       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 64)   256         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 64)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 32, 32, 64)   16448       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 96)   0           conv2d_transpose_1[0][0]         \n",
      "                                                                 activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 32)   27680       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 32, 32, 32)   128         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 32, 32, 32)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 32, 32)   9248        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 32, 32, 32)   128         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 32, 32, 32)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 64, 64, 32)   4128        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 64, 64, 48)   0           conv2d_transpose_2[0][0]         \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 16)   6928        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64, 64, 16)   64          conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 64, 64, 16)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 16)   2320        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 64, 64, 16)   64          conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 64, 64, 16)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 128, 128, 16) 1040        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 128, 128, 24) 0           conv2d_transpose_3[0][0]         \n",
      "                                                                 activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 128, 128, 8)  1736        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 128, 128, 8)  32          conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 128, 128, 8)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 128, 128, 8)  584         activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 128, 128, 8)  32          conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 128, 128, 8)  0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 128, 128, 1)  9           activation_17[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 581,505\n",
      "Trainable params: 580,033\n",
      "Non-trainable params: 1,472\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "Initialising in 3..........2..........1......................................\n",
      "Training on 284 images with learning rate 1e-05, batch size 16, and number of filters 8\n",
      "Epoch 1/100\n",
      "16/16 - 2s - loss: 0.9221 - accuracy: 0.5703 - precision_m: 0.0583 - recall_m: 0.4370 - f1_m: 0.1024 - val_loss: 0.9459 - val_accuracy: 0.4183 - val_precision_m: 0.0413 - val_recall_m: 0.6499 - val_f1_m: 0.0776\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.51620\n",
      "Epoch 2/100\n",
      "16/16 - 0s - loss: 0.9219 - accuracy: 0.5556 - precision_m: 0.0595 - recall_m: 0.4629 - f1_m: 0.1046 - val_loss: 0.9459 - val_accuracy: 0.4047 - val_precision_m: 0.0413 - val_recall_m: 0.6657 - val_f1_m: 0.0777\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.51620\n",
      "Epoch 3/100\n",
      "16/16 - 0s - loss: 0.9215 - accuracy: 0.5394 - precision_m: 0.0602 - recall_m: 0.4917 - f1_m: 0.1067 - val_loss: 0.9458 - val_accuracy: 0.3985 - val_precision_m: 0.0409 - val_recall_m: 0.6650 - val_f1_m: 0.0769\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.51620\n",
      "Epoch 4/100\n",
      "16/16 - 0s - loss: 0.9214 - accuracy: 0.5208 - precision_m: 0.0617 - recall_m: 0.5264 - f1_m: 0.1090 - val_loss: 0.9458 - val_accuracy: 0.3984 - val_precision_m: 0.0406 - val_recall_m: 0.6602 - val_f1_m: 0.0764\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.51620\n",
      "Epoch 5/100\n",
      "16/16 - 0s - loss: 0.9211 - accuracy: 0.4987 - precision_m: 0.0626 - recall_m: 0.5645 - f1_m: 0.1117 - val_loss: 0.9458 - val_accuracy: 0.3914 - val_precision_m: 0.0400 - val_recall_m: 0.6558 - val_f1_m: 0.0752\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.51620\n",
      "Epoch 6/100\n",
      "16/16 - 0s - loss: 0.9207 - accuracy: 0.4759 - precision_m: 0.0636 - recall_m: 0.6047 - f1_m: 0.1143 - val_loss: 0.9457 - val_accuracy: 0.3751 - val_precision_m: 0.0398 - val_recall_m: 0.6720 - val_f1_m: 0.0751\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.51620\n",
      "Epoch 7/100\n",
      "16/16 - 1s - loss: 0.9205 - accuracy: 0.4550 - precision_m: 0.0646 - recall_m: 0.6404 - f1_m: 0.1162 - val_loss: 0.9456 - val_accuracy: 0.3599 - val_precision_m: 0.0399 - val_recall_m: 0.6904 - val_f1_m: 0.0753\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.51620\n",
      "Epoch 8/100\n",
      "16/16 - 0s - loss: 0.9201 - accuracy: 0.4353 - precision_m: 0.0654 - recall_m: 0.6762 - f1_m: 0.1183 - val_loss: 0.9456 - val_accuracy: 0.3490 - val_precision_m: 0.0400 - val_recall_m: 0.7063 - val_f1_m: 0.0756\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.51620\n",
      "Epoch 9/100\n",
      "16/16 - 0s - loss: 0.9196 - accuracy: 0.4218 - precision_m: 0.0664 - recall_m: 0.7043 - f1_m: 0.1205 - val_loss: 0.9455 - val_accuracy: 0.3406 - val_precision_m: 0.0403 - val_recall_m: 0.7223 - val_f1_m: 0.0763\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.51620\n",
      "Epoch 10/100\n",
      "16/16 - 0s - loss: 0.9199 - accuracy: 0.4076 - precision_m: 0.0657 - recall_m: 0.7234 - f1_m: 0.1190 - val_loss: 0.9454 - val_accuracy: 0.3351 - val_precision_m: 0.0406 - val_recall_m: 0.7348 - val_f1_m: 0.0769\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.51620\n",
      "Epoch 11/100\n",
      "16/16 - 0s - loss: 0.9192 - accuracy: 0.3966 - precision_m: 0.0671 - recall_m: 0.7550 - f1_m: 0.1220 - val_loss: 0.9454 - val_accuracy: 0.3303 - val_precision_m: 0.0410 - val_recall_m: 0.7478 - val_f1_m: 0.0776\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.51620\n",
      "Epoch 12/100\n",
      "16/16 - 0s - loss: 0.9188 - accuracy: 0.3873 - precision_m: 0.0678 - recall_m: 0.7623 - f1_m: 0.1233 - val_loss: 0.9453 - val_accuracy: 0.3236 - val_precision_m: 0.0411 - val_recall_m: 0.7578 - val_f1_m: 0.0778\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.51620\n",
      "Epoch 13/100\n",
      "16/16 - 0s - loss: 0.9184 - accuracy: 0.3784 - precision_m: 0.0682 - recall_m: 0.7908 - f1_m: 0.1244 - val_loss: 0.9452 - val_accuracy: 0.3148 - val_precision_m: 0.0413 - val_recall_m: 0.7740 - val_f1_m: 0.0784\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.51620\n",
      "Epoch 14/100\n",
      "16/16 - 0s - loss: 0.9180 - accuracy: 0.3713 - precision_m: 0.0686 - recall_m: 0.8051 - f1_m: 0.1257 - val_loss: 0.9451 - val_accuracy: 0.3055 - val_precision_m: 0.0415 - val_recall_m: 0.7889 - val_f1_m: 0.0787\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.51620\n",
      "Epoch 15/100\n",
      "16/16 - 0s - loss: 0.9180 - accuracy: 0.3660 - precision_m: 0.0684 - recall_m: 0.8093 - f1_m: 0.1249 - val_loss: 0.9451 - val_accuracy: 0.2956 - val_precision_m: 0.0417 - val_recall_m: 0.8043 - val_f1_m: 0.0791\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.51620\n",
      "Epoch 16/100\n",
      "16/16 - 0s - loss: 0.9173 - accuracy: 0.3646 - precision_m: 0.0693 - recall_m: 0.8260 - f1_m: 0.1269 - val_loss: 0.9450 - val_accuracy: 0.2885 - val_precision_m: 0.0419 - val_recall_m: 0.8184 - val_f1_m: 0.0796\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.51620\n",
      "Epoch 17/100\n",
      "16/16 - 0s - loss: 0.9173 - accuracy: 0.3625 - precision_m: 0.0690 - recall_m: 0.8281 - f1_m: 0.1264 - val_loss: 0.9449 - val_accuracy: 0.2823 - val_precision_m: 0.0422 - val_recall_m: 0.8316 - val_f1_m: 0.0801\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.51620\n",
      "Epoch 18/100\n",
      "16/16 - 0s - loss: 0.9166 - accuracy: 0.3622 - precision_m: 0.0700 - recall_m: 0.8390 - f1_m: 0.1282 - val_loss: 0.9448 - val_accuracy: 0.2773 - val_precision_m: 0.0424 - val_recall_m: 0.8437 - val_f1_m: 0.0807\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.51620\n",
      "Epoch 19/100\n",
      "16/16 - 0s - loss: 0.9163 - accuracy: 0.3632 - precision_m: 0.0705 - recall_m: 0.8497 - f1_m: 0.1290 - val_loss: 0.9446 - val_accuracy: 0.2745 - val_precision_m: 0.0428 - val_recall_m: 0.8543 - val_f1_m: 0.0813\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.51620\n",
      "Epoch 20/100\n",
      "16/16 - 0s - loss: 0.9152 - accuracy: 0.3665 - precision_m: 0.0716 - recall_m: 0.8514 - f1_m: 0.1315 - val_loss: 0.9445 - val_accuracy: 0.2694 - val_precision_m: 0.0430 - val_recall_m: 0.8659 - val_f1_m: 0.0818\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.51620\n",
      "Epoch 21/100\n",
      "16/16 - 0s - loss: 0.9149 - accuracy: 0.3682 - precision_m: 0.0721 - recall_m: 0.8612 - f1_m: 0.1322 - val_loss: 0.9443 - val_accuracy: 0.2662 - val_precision_m: 0.0434 - val_recall_m: 0.8786 - val_f1_m: 0.0826\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.51620\n",
      "Epoch 22/100\n",
      "16/16 - 0s - loss: 0.9142 - accuracy: 0.3708 - precision_m: 0.0733 - recall_m: 0.8725 - f1_m: 0.1345 - val_loss: 0.9441 - val_accuracy: 0.2665 - val_precision_m: 0.0438 - val_recall_m: 0.8862 - val_f1_m: 0.0833\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.51620\n",
      "Epoch 23/100\n",
      "16/16 - 0s - loss: 0.9134 - accuracy: 0.3731 - precision_m: 0.0736 - recall_m: 0.8770 - f1_m: 0.1354 - val_loss: 0.9438 - val_accuracy: 0.2678 - val_precision_m: 0.0441 - val_recall_m: 0.8925 - val_f1_m: 0.0840\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.51620\n",
      "Epoch 24/100\n",
      "16/16 - 0s - loss: 0.9130 - accuracy: 0.3806 - precision_m: 0.0746 - recall_m: 0.8759 - f1_m: 0.1364 - val_loss: 0.9435 - val_accuracy: 0.2692 - val_precision_m: 0.0445 - val_recall_m: 0.8988 - val_f1_m: 0.0847\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.51620\n",
      "Epoch 25/100\n",
      "16/16 - 0s - loss: 0.9122 - accuracy: 0.3875 - precision_m: 0.0752 - recall_m: 0.8686 - f1_m: 0.1377 - val_loss: 0.9432 - val_accuracy: 0.2758 - val_precision_m: 0.0450 - val_recall_m: 0.9002 - val_f1_m: 0.0856\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.51620\n",
      "Epoch 26/100\n",
      "16/16 - 0s - loss: 0.9117 - accuracy: 0.3940 - precision_m: 0.0756 - recall_m: 0.8665 - f1_m: 0.1384 - val_loss: 0.9428 - val_accuracy: 0.2809 - val_precision_m: 0.0454 - val_recall_m: 0.9019 - val_f1_m: 0.0864\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.51620\n",
      "Epoch 27/100\n",
      "16/16 - 0s - loss: 0.9119 - accuracy: 0.3965 - precision_m: 0.0753 - recall_m: 0.8651 - f1_m: 0.1368 - val_loss: 0.9424 - val_accuracy: 0.2869 - val_precision_m: 0.0459 - val_recall_m: 0.9035 - val_f1_m: 0.0872\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.51620\n",
      "Epoch 28/100\n",
      "16/16 - 0s - loss: 0.9116 - accuracy: 0.3991 - precision_m: 0.0754 - recall_m: 0.8663 - f1_m: 0.1373 - val_loss: 0.9421 - val_accuracy: 0.2906 - val_precision_m: 0.0462 - val_recall_m: 0.9052 - val_f1_m: 0.0878\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.51620\n",
      "Epoch 29/100\n",
      "16/16 - 0s - loss: 0.9103 - accuracy: 0.4089 - precision_m: 0.0782 - recall_m: 0.8733 - f1_m: 0.1418 - val_loss: 0.9418 - val_accuracy: 0.2945 - val_precision_m: 0.0465 - val_recall_m: 0.9063 - val_f1_m: 0.0884\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.51620\n",
      "Epoch 30/100\n",
      "16/16 - 0s - loss: 0.9100 - accuracy: 0.4103 - precision_m: 0.0777 - recall_m: 0.8759 - f1_m: 0.1415 - val_loss: 0.9413 - val_accuracy: 0.3019 - val_precision_m: 0.0470 - val_recall_m: 0.9056 - val_f1_m: 0.0892\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.51620\n",
      "Epoch 31/100\n",
      "16/16 - 0s - loss: 0.9085 - accuracy: 0.4158 - precision_m: 0.0794 - recall_m: 0.8696 - f1_m: 0.1445 - val_loss: 0.9408 - val_accuracy: 0.3096 - val_precision_m: 0.0473 - val_recall_m: 0.9022 - val_f1_m: 0.0898\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.51620\n",
      "Epoch 32/100\n",
      "16/16 - 0s - loss: 0.9082 - accuracy: 0.4206 - precision_m: 0.0799 - recall_m: 0.8787 - f1_m: 0.1454 - val_loss: 0.9406 - val_accuracy: 0.3157 - val_precision_m: 0.0475 - val_recall_m: 0.8970 - val_f1_m: 0.0900\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.51620\n",
      "Epoch 33/100\n",
      "16/16 - 0s - loss: 0.9071 - accuracy: 0.4243 - precision_m: 0.0812 - recall_m: 0.8851 - f1_m: 0.1477 - val_loss: 0.9403 - val_accuracy: 0.3196 - val_precision_m: 0.0478 - val_recall_m: 0.8977 - val_f1_m: 0.0906\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.51620\n",
      "Epoch 34/100\n",
      "16/16 - 0s - loss: 0.9067 - accuracy: 0.4314 - precision_m: 0.0819 - recall_m: 0.8917 - f1_m: 0.1486 - val_loss: 0.9401 - val_accuracy: 0.3203 - val_precision_m: 0.0479 - val_recall_m: 0.8994 - val_f1_m: 0.0909\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.51620\n",
      "Epoch 35/100\n",
      "16/16 - 0s - loss: 0.9062 - accuracy: 0.4342 - precision_m: 0.0825 - recall_m: 0.8849 - f1_m: 0.1496 - val_loss: 0.9397 - val_accuracy: 0.3278 - val_precision_m: 0.0482 - val_recall_m: 0.8941 - val_f1_m: 0.0913\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.51620\n",
      "Epoch 36/100\n",
      "16/16 - 0s - loss: 0.9057 - accuracy: 0.4434 - precision_m: 0.0832 - recall_m: 0.8865 - f1_m: 0.1510 - val_loss: 0.9397 - val_accuracy: 0.3257 - val_precision_m: 0.0481 - val_recall_m: 0.8958 - val_f1_m: 0.0912\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.51620\n",
      "Epoch 37/100\n",
      "16/16 - 0s - loss: 0.9044 - accuracy: 0.4465 - precision_m: 0.0845 - recall_m: 0.9012 - f1_m: 0.1542 - val_loss: 0.9393 - val_accuracy: 0.3334 - val_precision_m: 0.0485 - val_recall_m: 0.8931 - val_f1_m: 0.0919\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.51620\n",
      "Epoch 38/100\n",
      "16/16 - 0s - loss: 0.9032 - accuracy: 0.4517 - precision_m: 0.0874 - recall_m: 0.9021 - f1_m: 0.1580 - val_loss: 0.9392 - val_accuracy: 0.3353 - val_precision_m: 0.0486 - val_recall_m: 0.8922 - val_f1_m: 0.0921\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.51620\n",
      "Epoch 39/100\n",
      "16/16 - 0s - loss: 0.9037 - accuracy: 0.4586 - precision_m: 0.0863 - recall_m: 0.8933 - f1_m: 0.1556 - val_loss: 0.9393 - val_accuracy: 0.3336 - val_precision_m: 0.0486 - val_recall_m: 0.8944 - val_f1_m: 0.0921\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.51620\n",
      "Epoch 40/100\n",
      "16/16 - 0s - loss: 0.9025 - accuracy: 0.4668 - precision_m: 0.0880 - recall_m: 0.9004 - f1_m: 0.1589 - val_loss: 0.9391 - val_accuracy: 0.3363 - val_precision_m: 0.0487 - val_recall_m: 0.8928 - val_f1_m: 0.0922\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.51620\n",
      "Epoch 41/100\n",
      "16/16 - 0s - loss: 0.9018 - accuracy: 0.4755 - precision_m: 0.0896 - recall_m: 0.8992 - f1_m: 0.1609 - val_loss: 0.9391 - val_accuracy: 0.3374 - val_precision_m: 0.0486 - val_recall_m: 0.8907 - val_f1_m: 0.0921\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.51620\n",
      "Epoch 42/100\n",
      "16/16 - 0s - loss: 0.9005 - accuracy: 0.4798 - precision_m: 0.0910 - recall_m: 0.9113 - f1_m: 0.1648 - val_loss: 0.9389 - val_accuracy: 0.3407 - val_precision_m: 0.0487 - val_recall_m: 0.8883 - val_f1_m: 0.0923\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.51620\n",
      "Epoch 43/100\n",
      "16/16 - 0s - loss: 0.9000 - accuracy: 0.4848 - precision_m: 0.0915 - recall_m: 0.9093 - f1_m: 0.1655 - val_loss: 0.9389 - val_accuracy: 0.3417 - val_precision_m: 0.0488 - val_recall_m: 0.8886 - val_f1_m: 0.0925\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.51620\n",
      "Epoch 44/100\n",
      "16/16 - 0s - loss: 0.9006 - accuracy: 0.4924 - precision_m: 0.0905 - recall_m: 0.8943 - f1_m: 0.1636 - val_loss: 0.9390 - val_accuracy: 0.3392 - val_precision_m: 0.0489 - val_recall_m: 0.8927 - val_f1_m: 0.0926\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.51620\n",
      "Epoch 45/100\n",
      "16/16 - 0s - loss: 0.8983 - accuracy: 0.5075 - precision_m: 0.0945 - recall_m: 0.8995 - f1_m: 0.1702 - val_loss: 0.9392 - val_accuracy: 0.3378 - val_precision_m: 0.0487 - val_recall_m: 0.8910 - val_f1_m: 0.0922\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.51620\n",
      "Epoch 46/100\n",
      "16/16 - 0s - loss: 0.8974 - accuracy: 0.5043 - precision_m: 0.0956 - recall_m: 0.9154 - f1_m: 0.1718 - val_loss: 0.9384 - val_accuracy: 0.3594 - val_precision_m: 0.0493 - val_recall_m: 0.8721 - val_f1_m: 0.0932\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.51620\n",
      "Epoch 47/100\n",
      "16/16 - 0s - loss: 0.8975 - accuracy: 0.5108 - precision_m: 0.0958 - recall_m: 0.9052 - f1_m: 0.1714 - val_loss: 0.9385 - val_accuracy: 0.3546 - val_precision_m: 0.0490 - val_recall_m: 0.8739 - val_f1_m: 0.0928\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.51620\n",
      "Epoch 48/100\n",
      "16/16 - 0s - loss: 0.8962 - accuracy: 0.5150 - precision_m: 0.0972 - recall_m: 0.9211 - f1_m: 0.1748 - val_loss: 0.9387 - val_accuracy: 0.3537 - val_precision_m: 0.0488 - val_recall_m: 0.8713 - val_f1_m: 0.0924\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.51620\n",
      "Epoch 49/100\n",
      "16/16 - 0s - loss: 0.8956 - accuracy: 0.5282 - precision_m: 0.0989 - recall_m: 0.9120 - f1_m: 0.1769 - val_loss: 0.9386 - val_accuracy: 0.3638 - val_precision_m: 0.0489 - val_recall_m: 0.8572 - val_f1_m: 0.0924\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.51620\n",
      "Epoch 50/100\n",
      "16/16 - 0s - loss: 0.8945 - accuracy: 0.5248 - precision_m: 0.0996 - recall_m: 0.9078 - f1_m: 0.1786 - val_loss: 0.9386 - val_accuracy: 0.3603 - val_precision_m: 0.0489 - val_recall_m: 0.8612 - val_f1_m: 0.0924\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.51620\n",
      "Epoch 51/100\n",
      "16/16 - 0s - loss: 0.8937 - accuracy: 0.5353 - precision_m: 0.1010 - recall_m: 0.9195 - f1_m: 0.1812 - val_loss: 0.9383 - val_accuracy: 0.3703 - val_precision_m: 0.0489 - val_recall_m: 0.8487 - val_f1_m: 0.0924\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.51620\n",
      "Epoch 52/100\n",
      "16/16 - 0s - loss: 0.8924 - accuracy: 0.5453 - precision_m: 0.1037 - recall_m: 0.9122 - f1_m: 0.1847 - val_loss: 0.9387 - val_accuracy: 0.3649 - val_precision_m: 0.0487 - val_recall_m: 0.8515 - val_f1_m: 0.0920\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.51620\n",
      "Epoch 53/100\n",
      "16/16 - 0s - loss: 0.8922 - accuracy: 0.5524 - precision_m: 0.1045 - recall_m: 0.9067 - f1_m: 0.1855 - val_loss: 0.9384 - val_accuracy: 0.3717 - val_precision_m: 0.0489 - val_recall_m: 0.8449 - val_f1_m: 0.0923\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.51620\n",
      "Epoch 54/100\n",
      "16/16 - 0s - loss: 0.8908 - accuracy: 0.5613 - precision_m: 0.1070 - recall_m: 0.9137 - f1_m: 0.1893 - val_loss: 0.9383 - val_accuracy: 0.3769 - val_precision_m: 0.0489 - val_recall_m: 0.8377 - val_f1_m: 0.0923\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.51620\n",
      "Epoch 55/100\n",
      "16/16 - 0s - loss: 0.8908 - accuracy: 0.5574 - precision_m: 0.1053 - recall_m: 0.9208 - f1_m: 0.1877 - val_loss: 0.9381 - val_accuracy: 0.3764 - val_precision_m: 0.0493 - val_recall_m: 0.8449 - val_f1_m: 0.0930\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.51620\n",
      "Epoch 56/100\n",
      "16/16 - 0s - loss: 0.8895 - accuracy: 0.5647 - precision_m: 0.1075 - recall_m: 0.9152 - f1_m: 0.1911 - val_loss: 0.9383 - val_accuracy: 0.3736 - val_precision_m: 0.0489 - val_recall_m: 0.8434 - val_f1_m: 0.0924\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.51620\n",
      "Epoch 57/100\n",
      "16/16 - 0s - loss: 0.8896 - accuracy: 0.5778 - precision_m: 0.1085 - recall_m: 0.8971 - f1_m: 0.1911 - val_loss: 0.9379 - val_accuracy: 0.3838 - val_precision_m: 0.0493 - val_recall_m: 0.8356 - val_f1_m: 0.0930\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.51620\n",
      "Epoch 58/100\n",
      "16/16 - 0s - loss: 0.8881 - accuracy: 0.5703 - precision_m: 0.1087 - recall_m: 0.9245 - f1_m: 0.1934 - val_loss: 0.9380 - val_accuracy: 0.3817 - val_precision_m: 0.0493 - val_recall_m: 0.8369 - val_f1_m: 0.0930\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.51620\n",
      "Epoch 59/100\n",
      "16/16 - 0s - loss: 0.8880 - accuracy: 0.5795 - precision_m: 0.1100 - recall_m: 0.9124 - f1_m: 0.1940 - val_loss: 0.9380 - val_accuracy: 0.3758 - val_precision_m: 0.0492 - val_recall_m: 0.8444 - val_f1_m: 0.0930\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.51620\n",
      "Epoch 60/100\n",
      "16/16 - 0s - loss: 0.8874 - accuracy: 0.5858 - precision_m: 0.1106 - recall_m: 0.9006 - f1_m: 0.1956 - val_loss: 0.9377 - val_accuracy: 0.3815 - val_precision_m: 0.0494 - val_recall_m: 0.8394 - val_f1_m: 0.0932\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.51620\n",
      "Epoch 61/100\n",
      "16/16 - 0s - loss: 0.8872 - accuracy: 0.5840 - precision_m: 0.1096 - recall_m: 0.9085 - f1_m: 0.1940 - val_loss: 0.9383 - val_accuracy: 0.3753 - val_precision_m: 0.0489 - val_recall_m: 0.8382 - val_f1_m: 0.0922\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.51620\n",
      "Epoch 62/100\n",
      "16/16 - 0s - loss: 0.8854 - accuracy: 0.5918 - precision_m: 0.1131 - recall_m: 0.9128 - f1_m: 0.1999 - val_loss: 0.9379 - val_accuracy: 0.3776 - val_precision_m: 0.0494 - val_recall_m: 0.8444 - val_f1_m: 0.0933\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.51620\n",
      "Epoch 63/100\n",
      "16/16 - 0s - loss: 0.8852 - accuracy: 0.6009 - precision_m: 0.1137 - recall_m: 0.8928 - f1_m: 0.2003 - val_loss: 0.9363 - val_accuracy: 0.4241 - val_precision_m: 0.0507 - val_recall_m: 0.8016 - val_f1_m: 0.0953\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.51620\n",
      "Epoch 64/100\n",
      "16/16 - 0s - loss: 0.8843 - accuracy: 0.5931 - precision_m: 0.1136 - recall_m: 0.9045 - f1_m: 0.2003 - val_loss: 0.9374 - val_accuracy: 0.3967 - val_precision_m: 0.0502 - val_recall_m: 0.8315 - val_f1_m: 0.0947\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.51620\n",
      "Epoch 65/100\n",
      "16/16 - 0s - loss: 0.8835 - accuracy: 0.6128 - precision_m: 0.1167 - recall_m: 0.8977 - f1_m: 0.2047 - val_loss: 0.9370 - val_accuracy: 0.4159 - val_precision_m: 0.0504 - val_recall_m: 0.8070 - val_f1_m: 0.0948\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.51620\n",
      "Epoch 66/100\n",
      "16/16 - 0s - loss: 0.8818 - accuracy: 0.6074 - precision_m: 0.1170 - recall_m: 0.9135 - f1_m: 0.2062 - val_loss: 0.9370 - val_accuracy: 0.4236 - val_precision_m: 0.0508 - val_recall_m: 0.8022 - val_f1_m: 0.0955\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.51620\n",
      "Epoch 67/100\n",
      "16/16 - 0s - loss: 0.8817 - accuracy: 0.6176 - precision_m: 0.1181 - recall_m: 0.8958 - f1_m: 0.2075 - val_loss: 0.9369 - val_accuracy: 0.4334 - val_precision_m: 0.0508 - val_recall_m: 0.7868 - val_f1_m: 0.0953\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.51620\n",
      "Epoch 68/100\n",
      "16/16 - 0s - loss: 0.8811 - accuracy: 0.6194 - precision_m: 0.1187 - recall_m: 0.9006 - f1_m: 0.2068 - val_loss: 0.9372 - val_accuracy: 0.4165 - val_precision_m: 0.0508 - val_recall_m: 0.8121 - val_f1_m: 0.0956\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.51620\n",
      "Epoch 69/100\n",
      "16/16 - 0s - loss: 0.8802 - accuracy: 0.6221 - precision_m: 0.1201 - recall_m: 0.8979 - f1_m: 0.2095 - val_loss: 0.9370 - val_accuracy: 0.4252 - val_precision_m: 0.0508 - val_recall_m: 0.7988 - val_f1_m: 0.0954\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.51620\n",
      "Epoch 70/100\n",
      "16/16 - 0s - loss: 0.8802 - accuracy: 0.6263 - precision_m: 0.1203 - recall_m: 0.8868 - f1_m: 0.2102 - val_loss: 0.9365 - val_accuracy: 0.4519 - val_precision_m: 0.0513 - val_recall_m: 0.7671 - val_f1_m: 0.0961\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.51620\n",
      "Epoch 71/100\n",
      "16/16 - 0s - loss: 0.8806 - accuracy: 0.6364 - precision_m: 0.1204 - recall_m: 0.8708 - f1_m: 0.2089 - val_loss: 0.9360 - val_accuracy: 0.4320 - val_precision_m: 0.0516 - val_recall_m: 0.8025 - val_f1_m: 0.0969\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.51620\n",
      "Epoch 72/100\n",
      "16/16 - 0s - loss: 0.8782 - accuracy: 0.6279 - precision_m: 0.1219 - recall_m: 0.9087 - f1_m: 0.2128 - val_loss: 0.9364 - val_accuracy: 0.4250 - val_precision_m: 0.0517 - val_recall_m: 0.8124 - val_f1_m: 0.0971\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.51620\n",
      "Epoch 73/100\n",
      "16/16 - 0s - loss: 0.8781 - accuracy: 0.6331 - precision_m: 0.1228 - recall_m: 0.8934 - f1_m: 0.2139 - val_loss: 0.9362 - val_accuracy: 0.4564 - val_precision_m: 0.0521 - val_recall_m: 0.7731 - val_f1_m: 0.0975\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.51620\n",
      "Epoch 74/100\n",
      "16/16 - 0s - loss: 0.8763 - accuracy: 0.6438 - precision_m: 0.1255 - recall_m: 0.8841 - f1_m: 0.2182 - val_loss: 0.9361 - val_accuracy: 0.4755 - val_precision_m: 0.0524 - val_recall_m: 0.7483 - val_f1_m: 0.0979\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.51620\n",
      "Epoch 75/100\n",
      "16/16 - 0s - loss: 0.8765 - accuracy: 0.6389 - precision_m: 0.1243 - recall_m: 0.8974 - f1_m: 0.2164 - val_loss: 0.9354 - val_accuracy: 0.4728 - val_precision_m: 0.0527 - val_recall_m: 0.7585 - val_f1_m: 0.0984\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.51620\n",
      "Epoch 76/100\n",
      "16/16 - 0s - loss: 0.8744 - accuracy: 0.6573 - precision_m: 0.1286 - recall_m: 0.8759 - f1_m: 0.2222 - val_loss: 0.9354 - val_accuracy: 0.5030 - val_precision_m: 0.0526 - val_recall_m: 0.7098 - val_f1_m: 0.0979\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.51620\n",
      "Epoch 77/100\n",
      "16/16 - 0s - loss: 0.8744 - accuracy: 0.6500 - precision_m: 0.1272 - recall_m: 0.8889 - f1_m: 0.2208 - val_loss: 0.9356 - val_accuracy: 0.4750 - val_precision_m: 0.0526 - val_recall_m: 0.7519 - val_f1_m: 0.0982\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.51620\n",
      "Epoch 78/100\n",
      "16/16 - 0s - loss: 0.8723 - accuracy: 0.6537 - precision_m: 0.1292 - recall_m: 0.8862 - f1_m: 0.2241 - val_loss: 0.9355 - val_accuracy: 0.4739 - val_precision_m: 0.0527 - val_recall_m: 0.7551 - val_f1_m: 0.0985\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.51620\n",
      "Epoch 79/100\n",
      "16/16 - 0s - loss: 0.8744 - accuracy: 0.6583 - precision_m: 0.1279 - recall_m: 0.8799 - f1_m: 0.2198 - val_loss: 0.9354 - val_accuracy: 0.4645 - val_precision_m: 0.0526 - val_recall_m: 0.7680 - val_f1_m: 0.0984\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.51620\n",
      "Epoch 80/100\n",
      "16/16 - 0s - loss: 0.8718 - accuracy: 0.6637 - precision_m: 0.1301 - recall_m: 0.8719 - f1_m: 0.2247 - val_loss: 0.9351 - val_accuracy: 0.4787 - val_precision_m: 0.0529 - val_recall_m: 0.7509 - val_f1_m: 0.0988\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.51620\n",
      "Epoch 81/100\n",
      "16/16 - 0s - loss: 0.8704 - accuracy: 0.6681 - precision_m: 0.1324 - recall_m: 0.8807 - f1_m: 0.2287 - val_loss: 0.9356 - val_accuracy: 0.4637 - val_precision_m: 0.0523 - val_recall_m: 0.7638 - val_f1_m: 0.0979\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.51620\n",
      "Epoch 82/100\n",
      "16/16 - 0s - loss: 0.8724 - accuracy: 0.6585 - precision_m: 0.1275 - recall_m: 0.8640 - f1_m: 0.2206 - val_loss: 0.9346 - val_accuracy: 0.4863 - val_precision_m: 0.0525 - val_recall_m: 0.7354 - val_f1_m: 0.0980\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.51620\n",
      "Epoch 83/100\n",
      "16/16 - 0s - loss: 0.8715 - accuracy: 0.6723 - precision_m: 0.1310 - recall_m: 0.8644 - f1_m: 0.2246 - val_loss: 0.9353 - val_accuracy: 0.4498 - val_precision_m: 0.0522 - val_recall_m: 0.7824 - val_f1_m: 0.0979\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.51620\n",
      "Epoch 84/100\n",
      "16/16 - 0s - loss: 0.8693 - accuracy: 0.6694 - precision_m: 0.1324 - recall_m: 0.8751 - f1_m: 0.2278 - val_loss: 0.9338 - val_accuracy: 0.5068 - val_precision_m: 0.0541 - val_recall_m: 0.7239 - val_f1_m: 0.1005\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.51620\n",
      "Epoch 85/100\n",
      "16/16 - 0s - loss: 0.8689 - accuracy: 0.6823 - precision_m: 0.1332 - recall_m: 0.8399 - f1_m: 0.2286 - val_loss: 0.9339 - val_accuracy: 0.4820 - val_precision_m: 0.0537 - val_recall_m: 0.7607 - val_f1_m: 0.1002\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.51620\n",
      "Epoch 86/100\n",
      "16/16 - 0s - loss: 0.8683 - accuracy: 0.6827 - precision_m: 0.1361 - recall_m: 0.8648 - f1_m: 0.2319 - val_loss: 0.9344 - val_accuracy: 0.4964 - val_precision_m: 0.0532 - val_recall_m: 0.7284 - val_f1_m: 0.0991\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.51620\n",
      "Epoch 87/100\n",
      "16/16 - 0s - loss: 0.8656 - accuracy: 0.6695 - precision_m: 0.1345 - recall_m: 0.9001 - f1_m: 0.2309 - val_loss: 0.9338 - val_accuracy: 0.5089 - val_precision_m: 0.0541 - val_recall_m: 0.7199 - val_f1_m: 0.1005\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.51620\n",
      "Epoch 88/100\n",
      "16/16 - 0s - loss: 0.8645 - accuracy: 0.6816 - precision_m: 0.1388 - recall_m: 0.8817 - f1_m: 0.2363 - val_loss: 0.9332 - val_accuracy: 0.5123 - val_precision_m: 0.0547 - val_recall_m: 0.7260 - val_f1_m: 0.1017\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.51620\n",
      "Epoch 89/100\n",
      "16/16 - 0s - loss: 0.8646 - accuracy: 0.6958 - precision_m: 0.1400 - recall_m: 0.8624 - f1_m: 0.2388 - val_loss: 0.9332 - val_accuracy: 0.5000 - val_precision_m: 0.0543 - val_recall_m: 0.7392 - val_f1_m: 0.1011\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.51620\n",
      "Epoch 90/100\n",
      "16/16 - 0s - loss: 0.8642 - accuracy: 0.6839 - precision_m: 0.1391 - recall_m: 0.8886 - f1_m: 0.2373 - val_loss: 0.9333 - val_accuracy: 0.5099 - val_precision_m: 0.0544 - val_recall_m: 0.7236 - val_f1_m: 0.1011\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.51620\n",
      "Epoch 91/100\n",
      "16/16 - 0s - loss: 0.8622 - accuracy: 0.6954 - precision_m: 0.1422 - recall_m: 0.8830 - f1_m: 0.2433 - val_loss: 0.9319 - val_accuracy: 0.5305 - val_precision_m: 0.0562 - val_recall_m: 0.7174 - val_f1_m: 0.1042\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.51620\n",
      "Epoch 92/100\n",
      "16/16 - 0s - loss: 0.8602 - accuracy: 0.7062 - precision_m: 0.1467 - recall_m: 0.8700 - f1_m: 0.2494 - val_loss: 0.9328 - val_accuracy: 0.5044 - val_precision_m: 0.0544 - val_recall_m: 0.7316 - val_f1_m: 0.1011\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.51620\n",
      "Epoch 93/100\n",
      "16/16 - 0s - loss: 0.8627 - accuracy: 0.7018 - precision_m: 0.1422 - recall_m: 0.8609 - f1_m: 0.2395 - val_loss: 0.9324 - val_accuracy: 0.5295 - val_precision_m: 0.0553 - val_recall_m: 0.7089 - val_f1_m: 0.1025\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.51620\n",
      "Epoch 94/100\n",
      "16/16 - 0s - loss: 0.8617 - accuracy: 0.7074 - precision_m: 0.1459 - recall_m: 0.8579 - f1_m: 0.2443 - val_loss: 0.9333 - val_accuracy: 0.4910 - val_precision_m: 0.0537 - val_recall_m: 0.7423 - val_f1_m: 0.1001\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.51620\n",
      "Epoch 95/100\n",
      "16/16 - 0s - loss: 0.8601 - accuracy: 0.6990 - precision_m: 0.1447 - recall_m: 0.8879 - f1_m: 0.2459 - val_loss: 0.9320 - val_accuracy: 0.5324 - val_precision_m: 0.0554 - val_recall_m: 0.7021 - val_f1_m: 0.1027\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.51620\n",
      "Epoch 96/100\n",
      "16/16 - 0s - loss: 0.8573 - accuracy: 0.7101 - precision_m: 0.1503 - recall_m: 0.8858 - f1_m: 0.2538 - val_loss: 0.9317 - val_accuracy: 0.5273 - val_precision_m: 0.0556 - val_recall_m: 0.7127 - val_f1_m: 0.1030\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.51620\n",
      "Epoch 97/100\n",
      "16/16 - 0s - loss: 0.8554 - accuracy: 0.7123 - precision_m: 0.1510 - recall_m: 0.8893 - f1_m: 0.2572 - val_loss: 0.9327 - val_accuracy: 0.5036 - val_precision_m: 0.0541 - val_recall_m: 0.7303 - val_f1_m: 0.1007\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.51620\n",
      "Epoch 98/100\n",
      "16/16 - 0s - loss: 0.8572 - accuracy: 0.7155 - precision_m: 0.1516 - recall_m: 0.8786 - f1_m: 0.2551 - val_loss: 0.9327 - val_accuracy: 0.4810 - val_precision_m: 0.0536 - val_recall_m: 0.7573 - val_f1_m: 0.1001\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.51620\n",
      "Epoch 99/100\n",
      "16/16 - 0s - loss: 0.8545 - accuracy: 0.7235 - precision_m: 0.1536 - recall_m: 0.8768 - f1_m: 0.2592 - val_loss: 0.9307 - val_accuracy: 0.5343 - val_precision_m: 0.0565 - val_recall_m: 0.7129 - val_f1_m: 0.1046\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.51620\n",
      "Epoch 100/100\n",
      "16/16 - 0s - loss: 0.8576 - accuracy: 0.7167 - precision_m: 0.1502 - recall_m: 0.8645 - f1_m: 0.2520 - val_loss: 0.9329 - val_accuracy: 0.5016 - val_precision_m: 0.0535 - val_recall_m: 0.7231 - val_f1_m: 0.0996\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.51620\n",
      "5/5 - 0s - loss: 0.8795 - accuracy: 0.6193 - precision_m: 0.1109 - recall_m: 0.7838 - f1_m: 0.1942\n",
      "The metrics for the test set with loss tversky, learning rate 1e-05,  filters 8, and batch size 16 is [0.8794735074043274, 0.6193263530731201, 0.11094846576452255, 0.7838050723075867, 0.19423452019691467]\n",
      "__________________________________________________________________________________________________________________________________________________________________________\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 128, 128, 5) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 128, 128, 16) 736         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 128, 128, 16) 64          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 128, 128, 16) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 16) 2320        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 16) 64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, 128, 16) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 64, 64, 16)   0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 32)   4640        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64, 64, 32)   128         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 64, 64, 32)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 64, 32)   9248        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 32)   128         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64, 64, 32)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 32)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 64)   18496       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 64)   256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 64)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 64)   36928       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 64)   256         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 64)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 64)   0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 16, 16, 128)  73856       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 16, 16, 128)  512         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 16, 16, 128)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 16, 16, 128)  147584      activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 16, 16, 128)  512         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16, 16, 128)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 128)    0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 8, 8, 256)    295168      max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 8, 8, 256)    1024        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 8, 8, 256)    0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 8, 8, 256)    590080      activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 8, 8, 256)    1024        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 8, 8, 256)    0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 16, 16, 128)  131200      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 16, 16, 256)  0           conv2d_transpose[0][0]           \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 128)  295040      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 128)  512         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 128)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 128)  147584      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 128)  512         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 128)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 32, 32, 64)   32832       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 128)  0           conv2d_transpose_1[0][0]         \n",
      "                                                                 activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 64)   73792       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 32, 32, 64)   256         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 32, 32, 64)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 32, 64)   36928       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 32, 32, 64)   256         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 32, 32, 64)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 64, 64, 32)   8224        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 64, 64, 64)   0           conv2d_transpose_2[0][0]         \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 32)   18464       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64, 64, 32)   128         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 64, 64, 32)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 32)   9248        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 64, 64, 32)   128         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 64, 64, 32)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 128, 128, 16) 2064        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 128, 128, 32) 0           conv2d_transpose_3[0][0]         \n",
      "                                                                 activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 128, 128, 16) 4624        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 128, 128, 16) 64          conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 128, 128, 16) 0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 128, 128, 16) 2320        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 128, 128, 16) 64          conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 128, 128, 16) 0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 128, 128, 1)  17          activation_17[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,947,281\n",
      "Trainable params: 1,944,337\n",
      "Non-trainable params: 2,944\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "Initialising in 3..........2..........1......................................\n",
      "Training on 284 images with learning rate 1e-05, batch size 16, and number of filters 16\n",
      "Epoch 1/100\n",
      "16/16 - 3s - loss: 0.9200 - accuracy: 0.9265 - precision_m: 0.0204 - recall_m: 0.0062 - f1_m: 0.0093 - val_loss: 0.9445 - val_accuracy: 0.9321 - val_precision_m: 0.0279 - val_recall_m: 0.0203 - val_f1_m: 0.0233\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.51620\n",
      "Epoch 2/100\n",
      "16/16 - 1s - loss: 0.9195 - accuracy: 0.9270 - precision_m: 0.0225 - recall_m: 0.0074 - f1_m: 0.0110 - val_loss: 0.9448 - val_accuracy: 0.9438 - val_precision_m: 0.0258 - val_recall_m: 0.0104 - val_f1_m: 0.0148\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.51620\n",
      "Epoch 3/100\n",
      "16/16 - 1s - loss: 0.9195 - accuracy: 0.9266 - precision_m: 0.0285 - recall_m: 0.0091 - f1_m: 0.0133 - val_loss: 0.9450 - val_accuracy: 0.9493 - val_precision_m: 0.0235 - val_recall_m: 0.0064 - val_f1_m: 0.0101\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.51620\n",
      "Epoch 4/100\n",
      "16/16 - 1s - loss: 0.9191 - accuracy: 0.9244 - precision_m: 0.0347 - recall_m: 0.0131 - f1_m: 0.0185 - val_loss: 0.9450 - val_accuracy: 0.9523 - val_precision_m: 0.0231 - val_recall_m: 0.0048 - val_f1_m: 0.0079\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.51620\n",
      "Epoch 5/100\n",
      "16/16 - 1s - loss: 0.9187 - accuracy: 0.9196 - precision_m: 0.0484 - recall_m: 0.0225 - f1_m: 0.0302 - val_loss: 0.9450 - val_accuracy: 0.9537 - val_precision_m: 0.0225 - val_recall_m: 0.0039 - val_f1_m: 0.0066\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.51620\n",
      "Epoch 6/100\n",
      "16/16 - 1s - loss: 0.9183 - accuracy: 0.9134 - precision_m: 0.0547 - recall_m: 0.0326 - f1_m: 0.0397 - val_loss: 0.9450 - val_accuracy: 0.9545 - val_precision_m: 0.0221 - val_recall_m: 0.0035 - val_f1_m: 0.0061\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.51620\n",
      "Epoch 7/100\n",
      "16/16 - 1s - loss: 0.9180 - accuracy: 0.9060 - precision_m: 0.0639 - recall_m: 0.0487 - f1_m: 0.0537 - val_loss: 0.9450 - val_accuracy: 0.9548 - val_precision_m: 0.0198 - val_recall_m: 0.0031 - val_f1_m: 0.0053\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.51620\n",
      "Epoch 8/100\n",
      "16/16 - 1s - loss: 0.9174 - accuracy: 0.8967 - precision_m: 0.0686 - recall_m: 0.0659 - f1_m: 0.0661 - val_loss: 0.9449 - val_accuracy: 0.9531 - val_precision_m: 0.0212 - val_recall_m: 0.0043 - val_f1_m: 0.0071\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.51620\n",
      "Epoch 9/100\n",
      "16/16 - 1s - loss: 0.9175 - accuracy: 0.8811 - precision_m: 0.0742 - recall_m: 0.0959 - f1_m: 0.0814 - val_loss: 0.9449 - val_accuracy: 0.9504 - val_precision_m: 0.0295 - val_recall_m: 0.0087 - val_f1_m: 0.0133\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.51620\n",
      "Epoch 10/100\n",
      "16/16 - 1s - loss: 0.9166 - accuracy: 0.8659 - precision_m: 0.0761 - recall_m: 0.1242 - f1_m: 0.0926 - val_loss: 0.9448 - val_accuracy: 0.9461 - val_precision_m: 0.0315 - val_recall_m: 0.0131 - val_f1_m: 0.0184\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.51620\n",
      "Epoch 11/100\n",
      "16/16 - 1s - loss: 0.9161 - accuracy: 0.8438 - precision_m: 0.0775 - recall_m: 0.1624 - f1_m: 0.1033 - val_loss: 0.9447 - val_accuracy: 0.9379 - val_precision_m: 0.0331 - val_recall_m: 0.0215 - val_f1_m: 0.0259\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.51620\n",
      "Epoch 12/100\n",
      "16/16 - 1s - loss: 0.9156 - accuracy: 0.8260 - precision_m: 0.0782 - recall_m: 0.1896 - f1_m: 0.1080 - val_loss: 0.9446 - val_accuracy: 0.9230 - val_precision_m: 0.0373 - val_recall_m: 0.0405 - val_f1_m: 0.0386\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.51620\n",
      "Epoch 13/100\n",
      "16/16 - 1s - loss: 0.9154 - accuracy: 0.8091 - precision_m: 0.0781 - recall_m: 0.2185 - f1_m: 0.1124 - val_loss: 0.9445 - val_accuracy: 0.9014 - val_precision_m: 0.0422 - val_recall_m: 0.0729 - val_f1_m: 0.0531\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.51620\n",
      "Epoch 14/100\n",
      "16/16 - 1s - loss: 0.9144 - accuracy: 0.7919 - precision_m: 0.0795 - recall_m: 0.2528 - f1_m: 0.1181 - val_loss: 0.9443 - val_accuracy: 0.8791 - val_precision_m: 0.0434 - val_recall_m: 0.1032 - val_f1_m: 0.0608\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.51620\n",
      "Epoch 15/100\n",
      "16/16 - 1s - loss: 0.9136 - accuracy: 0.7746 - precision_m: 0.0794 - recall_m: 0.2821 - f1_m: 0.1227 - val_loss: 0.9442 - val_accuracy: 0.8524 - val_precision_m: 0.0434 - val_recall_m: 0.1365 - val_f1_m: 0.0656\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.51620\n",
      "Epoch 16/100\n",
      "16/16 - 1s - loss: 0.9129 - accuracy: 0.7593 - precision_m: 0.0806 - recall_m: 0.3085 - f1_m: 0.1257 - val_loss: 0.9440 - val_accuracy: 0.8275 - val_precision_m: 0.0438 - val_recall_m: 0.1696 - val_f1_m: 0.0694\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.51620\n",
      "Epoch 17/100\n",
      "16/16 - 1s - loss: 0.9120 - accuracy: 0.7464 - precision_m: 0.0802 - recall_m: 0.3319 - f1_m: 0.1281 - val_loss: 0.9437 - val_accuracy: 0.8001 - val_precision_m: 0.0448 - val_recall_m: 0.2097 - val_f1_m: 0.0736\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.51620\n",
      "Epoch 18/100\n",
      "16/16 - 1s - loss: 0.9109 - accuracy: 0.7380 - precision_m: 0.0820 - recall_m: 0.3562 - f1_m: 0.1322 - val_loss: 0.9434 - val_accuracy: 0.7778 - val_precision_m: 0.0456 - val_recall_m: 0.2441 - val_f1_m: 0.0766\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.51620\n",
      "Epoch 19/100\n",
      "16/16 - 1s - loss: 0.9103 - accuracy: 0.7295 - precision_m: 0.0812 - recall_m: 0.3682 - f1_m: 0.1319 - val_loss: 0.9430 - val_accuracy: 0.7634 - val_precision_m: 0.0462 - val_recall_m: 0.2667 - val_f1_m: 0.0785\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.51620\n",
      "Epoch 20/100\n",
      "16/16 - 1s - loss: 0.9090 - accuracy: 0.7224 - precision_m: 0.0832 - recall_m: 0.3893 - f1_m: 0.1348 - val_loss: 0.9426 - val_accuracy: 0.7466 - val_precision_m: 0.0468 - val_recall_m: 0.2937 - val_f1_m: 0.0805\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.51620\n",
      "Epoch 21/100\n",
      "16/16 - 1s - loss: 0.9081 - accuracy: 0.7174 - precision_m: 0.0839 - recall_m: 0.4043 - f1_m: 0.1385 - val_loss: 0.9422 - val_accuracy: 0.7232 - val_precision_m: 0.0470 - val_recall_m: 0.3277 - val_f1_m: 0.0821\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.51620\n",
      "Epoch 22/100\n",
      "16/16 - 1s - loss: 0.9066 - accuracy: 0.7105 - precision_m: 0.0861 - recall_m: 0.4194 - f1_m: 0.1415 - val_loss: 0.9418 - val_accuracy: 0.6973 - val_precision_m: 0.0472 - val_recall_m: 0.3651 - val_f1_m: 0.0834\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.51620\n",
      "Epoch 23/100\n",
      "16/16 - 1s - loss: 0.9065 - accuracy: 0.6967 - precision_m: 0.0844 - recall_m: 0.4466 - f1_m: 0.1407 - val_loss: 0.9412 - val_accuracy: 0.6816 - val_precision_m: 0.0476 - val_recall_m: 0.3908 - val_f1_m: 0.0847\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.51620\n",
      "Epoch 24/100\n",
      "16/16 - 1s - loss: 0.9061 - accuracy: 0.6696 - precision_m: 0.0852 - recall_m: 0.5012 - f1_m: 0.1435 - val_loss: 0.9408 - val_accuracy: 0.6657 - val_precision_m: 0.0477 - val_recall_m: 0.4142 - val_f1_m: 0.0854\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.51620\n",
      "Epoch 25/100\n",
      "16/16 - 1s - loss: 0.9045 - accuracy: 0.6287 - precision_m: 0.0879 - recall_m: 0.5848 - f1_m: 0.1505 - val_loss: 0.9402 - val_accuracy: 0.6492 - val_precision_m: 0.0480 - val_recall_m: 0.4407 - val_f1_m: 0.0865\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.51620\n",
      "Epoch 26/100\n",
      "16/16 - 1s - loss: 0.9034 - accuracy: 0.5862 - precision_m: 0.0880 - recall_m: 0.6743 - f1_m: 0.1542 - val_loss: 0.9403 - val_accuracy: 0.6205 - val_precision_m: 0.0479 - val_recall_m: 0.4795 - val_f1_m: 0.0869\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.51620\n",
      "Epoch 27/100\n",
      "16/16 - 1s - loss: 0.9014 - accuracy: 0.5701 - precision_m: 0.0914 - recall_m: 0.7277 - f1_m: 0.1616 - val_loss: 0.9403 - val_accuracy: 0.5686 - val_precision_m: 0.0479 - val_recall_m: 0.5535 - val_f1_m: 0.0881\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.51620\n",
      "Epoch 28/100\n",
      "16/16 - 1s - loss: 0.9010 - accuracy: 0.5569 - precision_m: 0.0912 - recall_m: 0.7677 - f1_m: 0.1619 - val_loss: 0.9400 - val_accuracy: 0.4988 - val_precision_m: 0.0479 - val_recall_m: 0.6511 - val_f1_m: 0.0891\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.51620\n",
      "Epoch 29/100\n",
      "16/16 - 1s - loss: 0.8995 - accuracy: 0.5534 - precision_m: 0.0946 - recall_m: 0.8010 - f1_m: 0.1681 - val_loss: 0.9396 - val_accuracy: 0.4587 - val_precision_m: 0.0482 - val_recall_m: 0.7130 - val_f1_m: 0.0902\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.51620\n",
      "Epoch 30/100\n",
      "16/16 - 1s - loss: 0.8986 - accuracy: 0.5471 - precision_m: 0.0955 - recall_m: 0.8110 - f1_m: 0.1692 - val_loss: 0.9394 - val_accuracy: 0.4243 - val_precision_m: 0.0484 - val_recall_m: 0.7637 - val_f1_m: 0.0909\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.51620\n",
      "Epoch 31/100\n",
      "16/16 - 1s - loss: 0.8976 - accuracy: 0.5503 - precision_m: 0.0961 - recall_m: 0.8336 - f1_m: 0.1713 - val_loss: 0.9392 - val_accuracy: 0.4070 - val_precision_m: 0.0483 - val_recall_m: 0.7884 - val_f1_m: 0.0910\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.51620\n",
      "Epoch 32/100\n",
      "16/16 - 1s - loss: 0.8960 - accuracy: 0.5578 - precision_m: 0.0985 - recall_m: 0.8215 - f1_m: 0.1748 - val_loss: 0.9394 - val_accuracy: 0.3967 - val_precision_m: 0.0485 - val_recall_m: 0.8061 - val_f1_m: 0.0914\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.51620\n",
      "Epoch 33/100\n",
      "16/16 - 1s - loss: 0.8948 - accuracy: 0.5624 - precision_m: 0.1021 - recall_m: 0.8613 - f1_m: 0.1818 - val_loss: 0.9393 - val_accuracy: 0.3945 - val_precision_m: 0.0487 - val_recall_m: 0.8131 - val_f1_m: 0.0918\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.51620\n",
      "Epoch 34/100\n",
      "16/16 - 1s - loss: 0.8944 - accuracy: 0.5622 - precision_m: 0.1025 - recall_m: 0.8706 - f1_m: 0.1809 - val_loss: 0.9387 - val_accuracy: 0.3964 - val_precision_m: 0.0492 - val_recall_m: 0.8181 - val_f1_m: 0.0926\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.51620\n",
      "Epoch 35/100\n",
      "16/16 - 1s - loss: 0.8933 - accuracy: 0.5739 - precision_m: 0.1057 - recall_m: 0.8813 - f1_m: 0.1880 - val_loss: 0.9392 - val_accuracy: 0.3852 - val_precision_m: 0.0489 - val_recall_m: 0.8306 - val_f1_m: 0.0923\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.51620\n",
      "Epoch 36/100\n",
      "16/16 - 1s - loss: 0.8916 - accuracy: 0.5818 - precision_m: 0.1098 - recall_m: 0.8788 - f1_m: 0.1938 - val_loss: 0.9381 - val_accuracy: 0.3973 - val_precision_m: 0.0493 - val_recall_m: 0.8205 - val_f1_m: 0.0929\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.51620\n",
      "Epoch 37/100\n",
      "16/16 - 1s - loss: 0.8906 - accuracy: 0.5936 - precision_m: 0.1118 - recall_m: 0.8798 - f1_m: 0.1973 - val_loss: 0.9372 - val_accuracy: 0.4073 - val_precision_m: 0.0495 - val_recall_m: 0.8092 - val_f1_m: 0.0932\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.51620\n",
      "Epoch 38/100\n",
      "16/16 - 1s - loss: 0.8893 - accuracy: 0.5988 - precision_m: 0.1134 - recall_m: 0.8904 - f1_m: 0.1993 - val_loss: 0.9375 - val_accuracy: 0.4045 - val_precision_m: 0.0496 - val_recall_m: 0.8141 - val_f1_m: 0.0934\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.51620\n",
      "Epoch 39/100\n",
      "16/16 - 1s - loss: 0.8881 - accuracy: 0.6066 - precision_m: 0.1145 - recall_m: 0.8913 - f1_m: 0.2010 - val_loss: 0.9367 - val_accuracy: 0.4039 - val_precision_m: 0.0497 - val_recall_m: 0.8180 - val_f1_m: 0.0937\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.51620\n",
      "Epoch 40/100\n",
      "16/16 - 1s - loss: 0.8861 - accuracy: 0.6144 - precision_m: 0.1186 - recall_m: 0.8914 - f1_m: 0.2080 - val_loss: 0.9367 - val_accuracy: 0.4094 - val_precision_m: 0.0499 - val_recall_m: 0.8145 - val_f1_m: 0.0940\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.51620\n",
      "Epoch 41/100\n",
      "16/16 - 1s - loss: 0.8853 - accuracy: 0.6146 - precision_m: 0.1200 - recall_m: 0.8974 - f1_m: 0.2098 - val_loss: 0.9378 - val_accuracy: 0.3900 - val_precision_m: 0.0499 - val_recall_m: 0.8405 - val_f1_m: 0.0941\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.51620\n",
      "Epoch 42/100\n",
      "16/16 - 1s - loss: 0.8845 - accuracy: 0.6430 - precision_m: 0.1232 - recall_m: 0.8791 - f1_m: 0.2148 - val_loss: 0.9363 - val_accuracy: 0.4078 - val_precision_m: 0.0502 - val_recall_m: 0.8208 - val_f1_m: 0.0945\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.51620\n",
      "Epoch 43/100\n",
      "16/16 - 1s - loss: 0.8818 - accuracy: 0.6493 - precision_m: 0.1291 - recall_m: 0.8935 - f1_m: 0.2238 - val_loss: 0.9364 - val_accuracy: 0.4034 - val_precision_m: 0.0507 - val_recall_m: 0.8352 - val_f1_m: 0.0954\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.51620\n",
      "Epoch 44/100\n",
      "16/16 - 1s - loss: 0.8808 - accuracy: 0.6515 - precision_m: 0.1292 - recall_m: 0.8757 - f1_m: 0.2220 - val_loss: 0.9366 - val_accuracy: 0.4018 - val_precision_m: 0.0501 - val_recall_m: 0.8277 - val_f1_m: 0.0944\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.51620\n",
      "Epoch 45/100\n",
      "16/16 - 1s - loss: 0.8795 - accuracy: 0.6654 - precision_m: 0.1333 - recall_m: 0.8858 - f1_m: 0.2279 - val_loss: 0.9370 - val_accuracy: 0.3824 - val_precision_m: 0.0505 - val_recall_m: 0.8612 - val_f1_m: 0.0953\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.51620\n",
      "Epoch 46/100\n",
      "16/16 - 1s - loss: 0.8769 - accuracy: 0.6852 - precision_m: 0.1398 - recall_m: 0.8877 - f1_m: 0.2389 - val_loss: 0.9379 - val_accuracy: 0.3679 - val_precision_m: 0.0498 - val_recall_m: 0.8697 - val_f1_m: 0.0941\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.51620\n",
      "Epoch 47/100\n",
      "16/16 - 1s - loss: 0.8749 - accuracy: 0.7091 - precision_m: 0.1471 - recall_m: 0.8666 - f1_m: 0.2484 - val_loss: 0.9369 - val_accuracy: 0.3843 - val_precision_m: 0.0500 - val_recall_m: 0.8523 - val_f1_m: 0.0944\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.51620\n",
      "Epoch 48/100\n",
      "16/16 - 1s - loss: 0.8763 - accuracy: 0.6683 - precision_m: 0.1344 - recall_m: 0.8773 - f1_m: 0.2293 - val_loss: 0.9388 - val_accuracy: 0.3481 - val_precision_m: 0.0498 - val_recall_m: 0.8975 - val_f1_m: 0.0943\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.51620\n",
      "Epoch 49/100\n",
      "16/16 - 1s - loss: 0.8716 - accuracy: 0.7202 - precision_m: 0.1523 - recall_m: 0.8677 - f1_m: 0.2555 - val_loss: 0.9362 - val_accuracy: 0.3924 - val_precision_m: 0.0513 - val_recall_m: 0.8592 - val_f1_m: 0.0966\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.51620\n",
      "Epoch 50/100\n",
      "16/16 - 1s - loss: 0.8676 - accuracy: 0.7401 - precision_m: 0.1629 - recall_m: 0.8673 - f1_m: 0.2684 - val_loss: 0.9364 - val_accuracy: 0.3823 - val_precision_m: 0.0507 - val_recall_m: 0.8676 - val_f1_m: 0.0957\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.51620\n",
      "Epoch 51/100\n",
      "16/16 - 1s - loss: 0.8654 - accuracy: 0.7273 - precision_m: 0.1636 - recall_m: 0.8985 - f1_m: 0.2714 - val_loss: 0.9370 - val_accuracy: 0.3633 - val_precision_m: 0.0508 - val_recall_m: 0.8957 - val_f1_m: 0.0961\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.51620\n",
      "Epoch 52/100\n",
      "16/16 - 1s - loss: 0.8660 - accuracy: 0.7463 - precision_m: 0.1614 - recall_m: 0.8301 - f1_m: 0.2675 - val_loss: 0.9366 - val_accuracy: 0.3707 - val_precision_m: 0.0509 - val_recall_m: 0.8859 - val_f1_m: 0.0961\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.51620\n",
      "Epoch 53/100\n",
      "16/16 - 1s - loss: 0.8678 - accuracy: 0.7092 - precision_m: 0.1532 - recall_m: 0.8726 - f1_m: 0.2510 - val_loss: 0.9357 - val_accuracy: 0.3813 - val_precision_m: 0.0515 - val_recall_m: 0.8822 - val_f1_m: 0.0973\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.51620\n",
      "Epoch 54/100\n",
      "16/16 - 1s - loss: 0.8614 - accuracy: 0.7519 - precision_m: 0.1669 - recall_m: 0.8543 - f1_m: 0.2784 - val_loss: 0.9365 - val_accuracy: 0.3624 - val_precision_m: 0.0505 - val_recall_m: 0.8921 - val_f1_m: 0.0955\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.51620\n",
      "Epoch 55/100\n",
      "16/16 - 1s - loss: 0.8569 - accuracy: 0.7677 - precision_m: 0.1785 - recall_m: 0.8575 - f1_m: 0.2920 - val_loss: 0.9357 - val_accuracy: 0.3763 - val_precision_m: 0.0509 - val_recall_m: 0.8799 - val_f1_m: 0.0961\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.51620\n",
      "Epoch 56/100\n",
      "16/16 - 1s - loss: 0.8562 - accuracy: 0.7716 - precision_m: 0.1802 - recall_m: 0.8512 - f1_m: 0.2922 - val_loss: 0.9352 - val_accuracy: 0.3957 - val_precision_m: 0.0506 - val_recall_m: 0.8490 - val_f1_m: 0.0955\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.51620\n",
      "Epoch 57/100\n",
      "16/16 - 1s - loss: 0.8551 - accuracy: 0.7828 - precision_m: 0.1820 - recall_m: 0.8294 - f1_m: 0.2943 - val_loss: 0.9364 - val_accuracy: 0.3458 - val_precision_m: 0.0503 - val_recall_m: 0.9110 - val_f1_m: 0.0952\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.51620\n",
      "Epoch 58/100\n",
      "16/16 - 1s - loss: 0.8510 - accuracy: 0.7845 - precision_m: 0.1877 - recall_m: 0.8418 - f1_m: 0.3033 - val_loss: 0.9364 - val_accuracy: 0.3422 - val_precision_m: 0.0501 - val_recall_m: 0.9117 - val_f1_m: 0.0949\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.51620\n",
      "Epoch 59/100\n",
      "16/16 - 1s - loss: 0.8504 - accuracy: 0.7827 - precision_m: 0.1858 - recall_m: 0.8399 - f1_m: 0.3001 - val_loss: 0.9348 - val_accuracy: 0.3808 - val_precision_m: 0.0516 - val_recall_m: 0.8834 - val_f1_m: 0.0974\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.51620\n",
      "Epoch 60/100\n",
      "16/16 - 1s - loss: 0.8520 - accuracy: 0.7996 - precision_m: 0.1910 - recall_m: 0.7968 - f1_m: 0.3025 - val_loss: 0.9332 - val_accuracy: 0.4084 - val_precision_m: 0.0522 - val_recall_m: 0.8545 - val_f1_m: 0.0983\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.51620\n",
      "Epoch 61/100\n",
      "16/16 - 1s - loss: 0.8475 - accuracy: 0.7766 - precision_m: 0.1873 - recall_m: 0.8489 - f1_m: 0.3029 - val_loss: 0.9330 - val_accuracy: 0.4124 - val_precision_m: 0.0533 - val_recall_m: 0.8649 - val_f1_m: 0.1003\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.51620\n",
      "Epoch 62/100\n",
      "16/16 - 1s - loss: 0.8475 - accuracy: 0.7948 - precision_m: 0.1932 - recall_m: 0.8018 - f1_m: 0.3078 - val_loss: 0.9337 - val_accuracy: 0.3910 - val_precision_m: 0.0525 - val_recall_m: 0.8826 - val_f1_m: 0.0991\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.51620\n",
      "Epoch 63/100\n",
      "16/16 - 1s - loss: 0.8481 - accuracy: 0.7874 - precision_m: 0.1892 - recall_m: 0.8349 - f1_m: 0.2977 - val_loss: 0.9314 - val_accuracy: 0.4377 - val_precision_m: 0.0557 - val_recall_m: 0.8631 - val_f1_m: 0.1046\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.51620\n",
      "Epoch 64/100\n",
      "16/16 - 1s - loss: 0.8387 - accuracy: 0.8104 - precision_m: 0.2088 - recall_m: 0.8373 - f1_m: 0.3295 - val_loss: 0.9311 - val_accuracy: 0.4421 - val_precision_m: 0.0564 - val_recall_m: 0.8656 - val_f1_m: 0.1058\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.51620\n",
      "Epoch 65/100\n",
      "16/16 - 1s - loss: 0.8435 - accuracy: 0.7967 - precision_m: 0.1975 - recall_m: 0.8305 - f1_m: 0.3127 - val_loss: 0.9284 - val_accuracy: 0.5069 - val_precision_m: 0.0605 - val_recall_m: 0.8196 - val_f1_m: 0.1126\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.51620\n",
      "Epoch 66/100\n",
      "16/16 - 1s - loss: 0.8411 - accuracy: 0.8072 - precision_m: 0.2021 - recall_m: 0.8277 - f1_m: 0.3213 - val_loss: 0.9263 - val_accuracy: 0.5423 - val_precision_m: 0.0630 - val_recall_m: 0.7887 - val_f1_m: 0.1167\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.51620\n",
      "Epoch 67/100\n",
      "16/16 - 1s - loss: 0.8359 - accuracy: 0.8226 - precision_m: 0.2160 - recall_m: 0.8221 - f1_m: 0.3389 - val_loss: 0.9261 - val_accuracy: 0.5564 - val_precision_m: 0.0632 - val_recall_m: 0.7669 - val_f1_m: 0.1168\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.51620\n",
      "Epoch 68/100\n",
      "16/16 - 1s - loss: 0.8393 - accuracy: 0.8081 - precision_m: 0.2038 - recall_m: 0.8277 - f1_m: 0.3212 - val_loss: 0.9265 - val_accuracy: 0.5198 - val_precision_m: 0.0617 - val_recall_m: 0.8117 - val_f1_m: 0.1147\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.51620\n",
      "Epoch 69/100\n",
      "16/16 - 1s - loss: 0.8340 - accuracy: 0.8089 - precision_m: 0.2068 - recall_m: 0.8496 - f1_m: 0.3302 - val_loss: 0.9283 - val_accuracy: 0.4783 - val_precision_m: 0.0594 - val_recall_m: 0.8544 - val_f1_m: 0.1111\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.51620\n",
      "Epoch 70/100\n",
      "16/16 - 1s - loss: 0.8315 - accuracy: 0.8212 - precision_m: 0.2203 - recall_m: 0.8327 - f1_m: 0.3409 - val_loss: 0.9283 - val_accuracy: 0.4860 - val_precision_m: 0.0593 - val_recall_m: 0.8385 - val_f1_m: 0.1108\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.51620\n",
      "Epoch 71/100\n",
      "16/16 - 1s - loss: 0.8318 - accuracy: 0.8173 - precision_m: 0.2164 - recall_m: 0.8388 - f1_m: 0.3388 - val_loss: 0.9291 - val_accuracy: 0.4564 - val_precision_m: 0.0574 - val_recall_m: 0.8593 - val_f1_m: 0.1075\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.51620\n",
      "Epoch 72/100\n",
      "16/16 - 1s - loss: 0.8340 - accuracy: 0.7975 - precision_m: 0.2067 - recall_m: 0.8490 - f1_m: 0.3241 - val_loss: 0.9282 - val_accuracy: 0.4757 - val_precision_m: 0.0589 - val_recall_m: 0.8497 - val_f1_m: 0.1101\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.51620\n",
      "Epoch 73/100\n",
      "16/16 - 1s - loss: 0.8340 - accuracy: 0.8143 - precision_m: 0.2089 - recall_m: 0.8166 - f1_m: 0.3243 - val_loss: 0.9264 - val_accuracy: 0.5125 - val_precision_m: 0.0616 - val_recall_m: 0.8271 - val_f1_m: 0.1146\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.51620\n",
      "Epoch 74/100\n",
      "16/16 - 1s - loss: 0.8311 - accuracy: 0.8143 - precision_m: 0.2120 - recall_m: 0.8266 - f1_m: 0.3339 - val_loss: 0.9254 - val_accuracy: 0.5598 - val_precision_m: 0.0629 - val_recall_m: 0.7565 - val_f1_m: 0.1161\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.51620\n",
      "Epoch 75/100\n",
      "16/16 - 1s - loss: 0.8265 - accuracy: 0.8251 - precision_m: 0.2229 - recall_m: 0.8438 - f1_m: 0.3491 - val_loss: 0.9241 - val_accuracy: 0.5444 - val_precision_m: 0.0648 - val_recall_m: 0.8132 - val_f1_m: 0.1199\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.51620\n",
      "Epoch 76/100\n",
      "16/16 - 1s - loss: 0.8273 - accuracy: 0.8221 - precision_m: 0.2193 - recall_m: 0.8449 - f1_m: 0.3432 - val_loss: 0.9270 - val_accuracy: 0.4851 - val_precision_m: 0.0600 - val_recall_m: 0.8534 - val_f1_m: 0.1121\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.51620\n",
      "Epoch 77/100\n",
      "16/16 - 1s - loss: 0.8250 - accuracy: 0.8311 - precision_m: 0.2264 - recall_m: 0.8167 - f1_m: 0.3512 - val_loss: 0.9244 - val_accuracy: 0.5562 - val_precision_m: 0.0638 - val_recall_m: 0.7753 - val_f1_m: 0.1178\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.51620\n",
      "Epoch 78/100\n",
      "16/16 - 1s - loss: 0.8276 - accuracy: 0.8273 - precision_m: 0.2182 - recall_m: 0.8096 - f1_m: 0.3381 - val_loss: 0.9243 - val_accuracy: 0.5698 - val_precision_m: 0.0645 - val_recall_m: 0.7601 - val_f1_m: 0.1188\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.51620\n",
      "Epoch 79/100\n",
      "16/16 - 1s - loss: 0.8258 - accuracy: 0.8305 - precision_m: 0.2280 - recall_m: 0.8248 - f1_m: 0.3527 - val_loss: 0.9254 - val_accuracy: 0.5176 - val_precision_m: 0.0621 - val_recall_m: 0.8254 - val_f1_m: 0.1155\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.51620\n",
      "Epoch 80/100\n",
      "16/16 - 1s - loss: 0.8215 - accuracy: 0.8238 - precision_m: 0.2263 - recall_m: 0.8538 - f1_m: 0.3519 - val_loss: 0.9240 - val_accuracy: 0.5604 - val_precision_m: 0.0654 - val_recall_m: 0.7880 - val_f1_m: 0.1207\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.51620\n",
      "Epoch 81/100\n",
      "16/16 - 1s - loss: 0.8187 - accuracy: 0.8418 - precision_m: 0.2436 - recall_m: 0.8251 - f1_m: 0.3707 - val_loss: 0.9241 - val_accuracy: 0.5454 - val_precision_m: 0.0640 - val_recall_m: 0.7983 - val_f1_m: 0.1184\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.51620\n",
      "Epoch 82/100\n",
      "16/16 - 1s - loss: 0.8218 - accuracy: 0.8314 - precision_m: 0.2303 - recall_m: 0.8194 - f1_m: 0.3513 - val_loss: 0.9237 - val_accuracy: 0.5612 - val_precision_m: 0.0653 - val_recall_m: 0.7850 - val_f1_m: 0.1206\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.51620\n",
      "Epoch 83/100\n",
      "16/16 - 1s - loss: 0.8216 - accuracy: 0.8323 - precision_m: 0.2268 - recall_m: 0.8205 - f1_m: 0.3501 - val_loss: 0.9262 - val_accuracy: 0.4933 - val_precision_m: 0.0609 - val_recall_m: 0.8513 - val_f1_m: 0.1137\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.51620\n",
      "Epoch 84/100\n",
      "16/16 - 1s - loss: 0.8221 - accuracy: 0.8254 - precision_m: 0.2278 - recall_m: 0.8437 - f1_m: 0.3508 - val_loss: 0.9234 - val_accuracy: 0.5642 - val_precision_m: 0.0663 - val_recall_m: 0.7897 - val_f1_m: 0.1222\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.51620\n",
      "Epoch 85/100\n",
      "16/16 - 1s - loss: 0.8179 - accuracy: 0.8385 - precision_m: 0.2376 - recall_m: 0.8296 - f1_m: 0.3620 - val_loss: 0.9242 - val_accuracy: 0.5411 - val_precision_m: 0.0641 - val_recall_m: 0.8092 - val_f1_m: 0.1187\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.51620\n",
      "Epoch 86/100\n",
      "16/16 - 1s - loss: 0.8217 - accuracy: 0.8313 - precision_m: 0.2300 - recall_m: 0.8052 - f1_m: 0.3472 - val_loss: 0.9208 - val_accuracy: 0.6099 - val_precision_m: 0.0701 - val_recall_m: 0.7425 - val_f1_m: 0.1280\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.51620\n",
      "Epoch 87/100\n",
      "16/16 - 1s - loss: 0.8180 - accuracy: 0.8378 - precision_m: 0.2335 - recall_m: 0.8113 - f1_m: 0.3590 - val_loss: 0.9219 - val_accuracy: 0.5647 - val_precision_m: 0.0673 - val_recall_m: 0.8044 - val_f1_m: 0.1242\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.51620\n",
      "Epoch 88/100\n",
      "16/16 - 1s - loss: 0.8178 - accuracy: 0.8287 - precision_m: 0.2258 - recall_m: 0.8406 - f1_m: 0.3516 - val_loss: 0.9204 - val_accuracy: 0.6193 - val_precision_m: 0.0703 - val_recall_m: 0.7258 - val_f1_m: 0.1282\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.51620\n",
      "Epoch 89/100\n",
      "16/16 - 1s - loss: 0.8161 - accuracy: 0.8411 - precision_m: 0.2387 - recall_m: 0.8110 - f1_m: 0.3654 - val_loss: 0.9207 - val_accuracy: 0.6309 - val_precision_m: 0.0706 - val_recall_m: 0.7044 - val_f1_m: 0.1283\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.51620\n",
      "Epoch 90/100\n",
      "16/16 - 1s - loss: 0.8158 - accuracy: 0.8446 - precision_m: 0.2446 - recall_m: 0.8238 - f1_m: 0.3686 - val_loss: 0.9184 - val_accuracy: 0.6442 - val_precision_m: 0.0734 - val_recall_m: 0.7043 - val_f1_m: 0.1329\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.51620\n",
      "Epoch 91/100\n",
      "16/16 - 1s - loss: 0.8096 - accuracy: 0.8438 - precision_m: 0.2482 - recall_m: 0.8492 - f1_m: 0.3806 - val_loss: 0.9208 - val_accuracy: 0.5893 - val_precision_m: 0.0697 - val_recall_m: 0.7826 - val_f1_m: 0.1280\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.51620\n",
      "Epoch 92/100\n",
      "16/16 - 1s - loss: 0.8134 - accuracy: 0.8428 - precision_m: 0.2437 - recall_m: 0.8305 - f1_m: 0.3702 - val_loss: 0.9186 - val_accuracy: 0.6405 - val_precision_m: 0.0736 - val_recall_m: 0.7149 - val_f1_m: 0.1335\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.51620\n",
      "Epoch 93/100\n",
      "16/16 - 1s - loss: 0.8133 - accuracy: 0.8475 - precision_m: 0.2441 - recall_m: 0.8210 - f1_m: 0.3723 - val_loss: 0.9195 - val_accuracy: 0.6285 - val_precision_m: 0.0712 - val_recall_m: 0.7158 - val_f1_m: 0.1295\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.51620\n",
      "Epoch 94/100\n",
      "16/16 - 1s - loss: 0.8101 - accuracy: 0.8474 - precision_m: 0.2492 - recall_m: 0.8299 - f1_m: 0.3725 - val_loss: 0.9189 - val_accuracy: 0.6562 - val_precision_m: 0.0725 - val_recall_m: 0.6681 - val_f1_m: 0.1308\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.51620\n",
      "Epoch 95/100\n",
      "16/16 - 1s - loss: 0.8104 - accuracy: 0.8411 - precision_m: 0.2413 - recall_m: 0.8326 - f1_m: 0.3697 - val_loss: 0.9192 - val_accuracy: 0.6519 - val_precision_m: 0.0720 - val_recall_m: 0.6724 - val_f1_m: 0.1301\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.51620\n",
      "Epoch 96/100\n",
      "16/16 - 1s - loss: 0.8103 - accuracy: 0.8480 - precision_m: 0.2507 - recall_m: 0.8338 - f1_m: 0.3765 - val_loss: 0.9177 - val_accuracy: 0.6635 - val_precision_m: 0.0747 - val_recall_m: 0.6736 - val_f1_m: 0.1346\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.51620\n",
      "Epoch 97/100\n",
      "16/16 - 1s - loss: 0.8092 - accuracy: 0.8535 - precision_m: 0.2571 - recall_m: 0.8329 - f1_m: 0.3809 - val_loss: 0.9174 - val_accuracy: 0.6799 - val_precision_m: 0.0745 - val_recall_m: 0.6344 - val_f1_m: 0.1334\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.51620\n",
      "Epoch 98/100\n",
      "16/16 - 1s - loss: 0.8094 - accuracy: 0.8563 - precision_m: 0.2559 - recall_m: 0.8176 - f1_m: 0.3806 - val_loss: 0.9177 - val_accuracy: 0.6600 - val_precision_m: 0.0743 - val_recall_m: 0.6776 - val_f1_m: 0.1340\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.51620\n",
      "Epoch 99/100\n",
      "16/16 - 1s - loss: 0.8082 - accuracy: 0.8416 - precision_m: 0.2441 - recall_m: 0.8423 - f1_m: 0.3680 - val_loss: 0.9187 - val_accuracy: 0.6495 - val_precision_m: 0.0719 - val_recall_m: 0.6764 - val_f1_m: 0.1299\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.51620\n",
      "Epoch 100/100\n",
      "16/16 - 1s - loss: 0.8087 - accuracy: 0.8545 - precision_m: 0.2569 - recall_m: 0.8117 - f1_m: 0.3828 - val_loss: 0.9182 - val_accuracy: 0.6449 - val_precision_m: 0.0725 - val_recall_m: 0.6915 - val_f1_m: 0.1313\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.51620\n",
      "5/5 - 0s - loss: 0.8526 - accuracy: 0.7127 - precision_m: 0.1449 - recall_m: 0.8065 - f1_m: 0.2454\n",
      "The metrics for the test set with loss tversky, learning rate 1e-05,  filters 16, and batch size 16 is [0.8526407480239868, 0.712692379951477, 0.14493897557258606, 0.8065109252929688, 0.24539172649383545]\n",
      "__________________________________________________________________________________________________________________________________________________________________________\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 128, 128, 5) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 128, 128, 32) 1472        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 128, 128, 32) 128         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 128, 128, 32) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 32) 9248        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 32) 128         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, 128, 32) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 64, 64, 32)   0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 64)   18496       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64, 64, 64)   256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 64, 64, 64)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 64, 64)   36928       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 64)   256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64, 64, 64)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 64)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 128)  73856       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 128)  512         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 128)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 128)  147584      activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 128)  512         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 128)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 128)  0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 16, 16, 256)  295168      max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 16, 16, 256)  1024        conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 16, 16, 256)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 16, 16, 256)  590080      activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 16, 16, 256)  1024        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16, 16, 256)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 256)    0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 8, 8, 512)    1180160     max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 8, 8, 512)    2048        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 8, 8, 512)    0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 8, 8, 512)    2359808     activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 8, 8, 512)    2048        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 8, 8, 512)    0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 16, 16, 128)  262272      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 16, 16, 384)  0           conv2d_transpose[0][0]           \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 256)  884992      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 256)  1024        conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 256)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 256)  590080      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 256)  1024        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 256)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 32, 32, 64)   65600       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 192)  0           conv2d_transpose_1[0][0]         \n",
      "                                                                 activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 128)  221312      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 32, 32, 128)  512         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 32, 32, 128)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 32, 128)  147584      activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 32, 32, 128)  512         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 32, 32, 128)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 64, 64, 32)   16416       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 64, 64, 96)   0           conv2d_transpose_2[0][0]         \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 64)   55360       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64, 64, 64)   256         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 64, 64, 64)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 64)   36928       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 64, 64, 64)   256         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 64, 64, 64)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 128, 128, 16) 4112        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 128, 128, 48) 0           conv2d_transpose_3[0][0]         \n",
      "                                                                 activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 128, 128, 32) 13856       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 128, 128, 32) 128         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 128, 128, 32) 0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 128, 128, 32) 9248        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 128, 128, 32) 128         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 128, 128, 32) 0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 128, 128, 1)  33          activation_17[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 7,032,369\n",
      "Trainable params: 7,026,481\n",
      "Non-trainable params: 5,888\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "Initialising in 3..........2..........1......................................\n",
      "Training on 284 images with learning rate 1e-05, batch size 16, and number of filters 32\n",
      "Epoch 1/100\n",
      "16/16 - 3s - loss: 0.9241 - accuracy: 0.7426 - precision_m: 0.0481 - recall_m: 0.1888 - f1_m: 0.0752 - val_loss: 0.9464 - val_accuracy: 0.8159 - val_precision_m: 0.0254 - val_recall_m: 0.0923 - val_f1_m: 0.0397\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.51620\n",
      "Epoch 2/100\n",
      "16/16 - 1s - loss: 0.9232 - accuracy: 0.6224 - precision_m: 0.0589 - recall_m: 0.3808 - f1_m: 0.1005 - val_loss: 0.9462 - val_accuracy: 0.8637 - val_precision_m: 0.0280 - val_recall_m: 0.0729 - val_f1_m: 0.0403\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.51620\n",
      "Epoch 3/100\n",
      "16/16 - 1s - loss: 0.9222 - accuracy: 0.4717 - precision_m: 0.0652 - recall_m: 0.6296 - f1_m: 0.1171 - val_loss: 0.9460 - val_accuracy: 0.8402 - val_precision_m: 0.0352 - val_recall_m: 0.1201 - val_f1_m: 0.0543\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.51620\n",
      "Epoch 4/100\n",
      "16/16 - 1s - loss: 0.9213 - accuracy: 0.3484 - precision_m: 0.0677 - recall_m: 0.8221 - f1_m: 0.1244 - val_loss: 0.9458 - val_accuracy: 0.7700 - val_precision_m: 0.0376 - val_recall_m: 0.2055 - val_f1_m: 0.0633\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.51620\n",
      "Epoch 5/100\n",
      "16/16 - 1s - loss: 0.9204 - accuracy: 0.3007 - precision_m: 0.0678 - recall_m: 0.8950 - f1_m: 0.1255 - val_loss: 0.9456 - val_accuracy: 0.5722 - val_precision_m: 0.0412 - val_recall_m: 0.4661 - val_f1_m: 0.0755\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.51620\n",
      "Epoch 6/100\n",
      "16/16 - 1s - loss: 0.9195 - accuracy: 0.2936 - precision_m: 0.0686 - recall_m: 0.9192 - f1_m: 0.1272 - val_loss: 0.9454 - val_accuracy: 0.3880 - val_precision_m: 0.0434 - val_recall_m: 0.7282 - val_f1_m: 0.0819\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.51620\n",
      "Epoch 7/100\n",
      "16/16 - 1s - loss: 0.9187 - accuracy: 0.2991 - precision_m: 0.0692 - recall_m: 0.9176 - f1_m: 0.1275 - val_loss: 0.9451 - val_accuracy: 0.3142 - val_precision_m: 0.0441 - val_recall_m: 0.8349 - val_f1_m: 0.0837\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.51620\n",
      "Epoch 8/100\n",
      "16/16 - 1s - loss: 0.9170 - accuracy: 0.3176 - precision_m: 0.0705 - recall_m: 0.9062 - f1_m: 0.1304 - val_loss: 0.9449 - val_accuracy: 0.2895 - val_precision_m: 0.0445 - val_recall_m: 0.8744 - val_f1_m: 0.0845\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.51620\n",
      "Epoch 9/100\n",
      "16/16 - 1s - loss: 0.9156 - accuracy: 0.3354 - precision_m: 0.0725 - recall_m: 0.9082 - f1_m: 0.1331 - val_loss: 0.9446 - val_accuracy: 0.2852 - val_precision_m: 0.0447 - val_recall_m: 0.8846 - val_f1_m: 0.0850\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.51620\n",
      "Epoch 10/100\n",
      "16/16 - 1s - loss: 0.9144 - accuracy: 0.3528 - precision_m: 0.0721 - recall_m: 0.8804 - f1_m: 0.1325 - val_loss: 0.9442 - val_accuracy: 0.2807 - val_precision_m: 0.0448 - val_recall_m: 0.8929 - val_f1_m: 0.0853\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.51620\n",
      "Epoch 11/100\n",
      "16/16 - 1s - loss: 0.9123 - accuracy: 0.3786 - precision_m: 0.0747 - recall_m: 0.8802 - f1_m: 0.1370 - val_loss: 0.9437 - val_accuracy: 0.2889 - val_precision_m: 0.0453 - val_recall_m: 0.8917 - val_f1_m: 0.0861\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.51620\n",
      "Epoch 12/100\n",
      "16/16 - 1s - loss: 0.9099 - accuracy: 0.3979 - precision_m: 0.0774 - recall_m: 0.8870 - f1_m: 0.1409 - val_loss: 0.9431 - val_accuracy: 0.2900 - val_precision_m: 0.0455 - val_recall_m: 0.8941 - val_f1_m: 0.0865\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.51620\n",
      "Epoch 13/100\n",
      "16/16 - 1s - loss: 0.9073 - accuracy: 0.4186 - precision_m: 0.0799 - recall_m: 0.8739 - f1_m: 0.1454 - val_loss: 0.9423 - val_accuracy: 0.3033 - val_precision_m: 0.0464 - val_recall_m: 0.8932 - val_f1_m: 0.0881\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.51620\n",
      "Epoch 14/100\n",
      "16/16 - 1s - loss: 0.9053 - accuracy: 0.4316 - precision_m: 0.0810 - recall_m: 0.8782 - f1_m: 0.1474 - val_loss: 0.9414 - val_accuracy: 0.3112 - val_precision_m: 0.0470 - val_recall_m: 0.8950 - val_f1_m: 0.0893\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.51620\n",
      "Epoch 15/100\n",
      "16/16 - 1s - loss: 0.9018 - accuracy: 0.4463 - precision_m: 0.0846 - recall_m: 0.8900 - f1_m: 0.1528 - val_loss: 0.9406 - val_accuracy: 0.3065 - val_precision_m: 0.0480 - val_recall_m: 0.9205 - val_f1_m: 0.0912\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.51620\n",
      "Epoch 16/100\n",
      "16/16 - 1s - loss: 0.8988 - accuracy: 0.4594 - precision_m: 0.0876 - recall_m: 0.9096 - f1_m: 0.1583 - val_loss: 0.9402 - val_accuracy: 0.3066 - val_precision_m: 0.0473 - val_recall_m: 0.9080 - val_f1_m: 0.0899\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.51620\n",
      "Epoch 17/100\n",
      "16/16 - 1s - loss: 0.8954 - accuracy: 0.4771 - precision_m: 0.0905 - recall_m: 0.9157 - f1_m: 0.1639 - val_loss: 0.9403 - val_accuracy: 0.2801 - val_precision_m: 0.0476 - val_recall_m: 0.9482 - val_f1_m: 0.0906\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.51620\n",
      "Epoch 18/100\n",
      "16/16 - 1s - loss: 0.8936 - accuracy: 0.4920 - precision_m: 0.0918 - recall_m: 0.9236 - f1_m: 0.1650 - val_loss: 0.9406 - val_accuracy: 0.2515 - val_precision_m: 0.0463 - val_recall_m: 0.9561 - val_f1_m: 0.0882\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.51620\n",
      "Epoch 19/100\n",
      "16/16 - 1s - loss: 0.8885 - accuracy: 0.5119 - precision_m: 0.0982 - recall_m: 0.9256 - f1_m: 0.1762 - val_loss: 0.9401 - val_accuracy: 0.2592 - val_precision_m: 0.0457 - val_recall_m: 0.9365 - val_f1_m: 0.0871\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.51620\n",
      "Epoch 20/100\n",
      "16/16 - 1s - loss: 0.8855 - accuracy: 0.5305 - precision_m: 0.1006 - recall_m: 0.9343 - f1_m: 0.1805 - val_loss: 0.9382 - val_accuracy: 0.3090 - val_precision_m: 0.0474 - val_recall_m: 0.9062 - val_f1_m: 0.0899\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.51620\n",
      "Epoch 21/100\n",
      "16/16 - 1s - loss: 0.8816 - accuracy: 0.5459 - precision_m: 0.1041 - recall_m: 0.9408 - f1_m: 0.1859 - val_loss: 0.9406 - val_accuracy: 0.2154 - val_precision_m: 0.0444 - val_recall_m: 0.9601 - val_f1_m: 0.0848\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.51620\n",
      "Epoch 22/100\n",
      "16/16 - 1s - loss: 0.8747 - accuracy: 0.5813 - precision_m: 0.1136 - recall_m: 0.9375 - f1_m: 0.2016 - val_loss: 0.9406 - val_accuracy: 0.2226 - val_precision_m: 0.0444 - val_recall_m: 0.9527 - val_f1_m: 0.0848\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.51620\n",
      "Epoch 23/100\n",
      "16/16 - 1s - loss: 0.8693 - accuracy: 0.6042 - precision_m: 0.1191 - recall_m: 0.9452 - f1_m: 0.2102 - val_loss: 0.9407 - val_accuracy: 0.1845 - val_precision_m: 0.0438 - val_recall_m: 0.9806 - val_f1_m: 0.0837\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.51620\n",
      "Epoch 24/100\n",
      "16/16 - 1s - loss: 0.8642 - accuracy: 0.6330 - precision_m: 0.1259 - recall_m: 0.9259 - f1_m: 0.2194 - val_loss: 0.9407 - val_accuracy: 0.1802 - val_precision_m: 0.0434 - val_recall_m: 0.9775 - val_f1_m: 0.0830\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.51620\n",
      "Epoch 25/100\n",
      "16/16 - 1s - loss: 0.8595 - accuracy: 0.6607 - precision_m: 0.1325 - recall_m: 0.9196 - f1_m: 0.2292 - val_loss: 0.9401 - val_accuracy: 0.1823 - val_precision_m: 0.0436 - val_recall_m: 0.9802 - val_f1_m: 0.0834\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.51620\n",
      "Epoch 26/100\n",
      "16/16 - 1s - loss: 0.8508 - accuracy: 0.6803 - precision_m: 0.1426 - recall_m: 0.9382 - f1_m: 0.2422 - val_loss: 0.9403 - val_accuracy: 0.1682 - val_precision_m: 0.0431 - val_recall_m: 0.9842 - val_f1_m: 0.0825\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.51620\n",
      "Epoch 27/100\n",
      "16/16 - 1s - loss: 0.8421 - accuracy: 0.7115 - precision_m: 0.1547 - recall_m: 0.9371 - f1_m: 0.2617 - val_loss: 0.9396 - val_accuracy: 0.1767 - val_precision_m: 0.0436 - val_recall_m: 0.9854 - val_f1_m: 0.0834\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.51620\n",
      "Epoch 28/100\n",
      "16/16 - 1s - loss: 0.8355 - accuracy: 0.7295 - precision_m: 0.1635 - recall_m: 0.9245 - f1_m: 0.2760 - val_loss: 0.9409 - val_accuracy: 0.1292 - val_precision_m: 0.0416 - val_recall_m: 0.9907 - val_f1_m: 0.0797\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.51620\n",
      "Epoch 29/100\n",
      "16/16 - 1s - loss: 0.8411 - accuracy: 0.7336 - precision_m: 0.1584 - recall_m: 0.8643 - f1_m: 0.2628 - val_loss: 0.9381 - val_accuracy: 0.2197 - val_precision_m: 0.0455 - val_recall_m: 0.9775 - val_f1_m: 0.0868\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.51620\n",
      "Epoch 30/100\n",
      "16/16 - 1s - loss: 0.8355 - accuracy: 0.7311 - precision_m: 0.1602 - recall_m: 0.8932 - f1_m: 0.2683 - val_loss: 0.9369 - val_accuracy: 0.2743 - val_precision_m: 0.0469 - val_recall_m: 0.9421 - val_f1_m: 0.0893\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.51620\n",
      "Epoch 31/100\n",
      "16/16 - 1s - loss: 0.8253 - accuracy: 0.7457 - precision_m: 0.1736 - recall_m: 0.9137 - f1_m: 0.2891 - val_loss: 0.9390 - val_accuracy: 0.1889 - val_precision_m: 0.0441 - val_recall_m: 0.9832 - val_f1_m: 0.0844\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.51620\n",
      "Epoch 32/100\n",
      "16/16 - 1s - loss: 0.8200 - accuracy: 0.7673 - precision_m: 0.1835 - recall_m: 0.9029 - f1_m: 0.3031 - val_loss: 0.9394 - val_accuracy: 0.1796 - val_precision_m: 0.0437 - val_recall_m: 0.9839 - val_f1_m: 0.0836\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.51620\n",
      "Epoch 33/100\n",
      "16/16 - 1s - loss: 0.8195 - accuracy: 0.7923 - precision_m: 0.1913 - recall_m: 0.8426 - f1_m: 0.3076 - val_loss: 0.9379 - val_accuracy: 0.2393 - val_precision_m: 0.0459 - val_recall_m: 0.9624 - val_f1_m: 0.0874\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.51620\n",
      "Epoch 34/100\n",
      "16/16 - 1s - loss: 0.8197 - accuracy: 0.7763 - precision_m: 0.1833 - recall_m: 0.8645 - f1_m: 0.3003 - val_loss: 0.9376 - val_accuracy: 0.2395 - val_precision_m: 0.0461 - val_recall_m: 0.9660 - val_f1_m: 0.0878\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.51620\n",
      "Epoch 35/100\n",
      "16/16 - 1s - loss: 0.8083 - accuracy: 0.8027 - precision_m: 0.2086 - recall_m: 0.8808 - f1_m: 0.3292 - val_loss: 0.9352 - val_accuracy: 0.3067 - val_precision_m: 0.0489 - val_recall_m: 0.9379 - val_f1_m: 0.0928\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.51620\n",
      "Epoch 36/100\n",
      "16/16 - 1s - loss: 0.8028 - accuracy: 0.8107 - precision_m: 0.2139 - recall_m: 0.8867 - f1_m: 0.3422 - val_loss: 0.9348 - val_accuracy: 0.3009 - val_precision_m: 0.0493 - val_recall_m: 0.9542 - val_f1_m: 0.0937\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.51620\n",
      "Epoch 37/100\n",
      "16/16 - 1s - loss: 0.8066 - accuracy: 0.8013 - precision_m: 0.2052 - recall_m: 0.8821 - f1_m: 0.3278 - val_loss: 0.9354 - val_accuracy: 0.2660 - val_precision_m: 0.0479 - val_recall_m: 0.9709 - val_f1_m: 0.0912\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.51620\n",
      "Epoch 38/100\n",
      "16/16 - 1s - loss: 0.8054 - accuracy: 0.8222 - precision_m: 0.2126 - recall_m: 0.8164 - f1_m: 0.3350 - val_loss: 0.9324 - val_accuracy: 0.3391 - val_precision_m: 0.0514 - val_recall_m: 0.9420 - val_f1_m: 0.0974\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.51620\n",
      "Epoch 39/100\n",
      "16/16 - 1s - loss: 0.8059 - accuracy: 0.8131 - precision_m: 0.2089 - recall_m: 0.8452 - f1_m: 0.3302 - val_loss: 0.9318 - val_accuracy: 0.3564 - val_precision_m: 0.0518 - val_recall_m: 0.9244 - val_f1_m: 0.0980\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.51620\n",
      "Epoch 40/100\n",
      "16/16 - 1s - loss: 0.8196 - accuracy: 0.7685 - precision_m: 0.1758 - recall_m: 0.8340 - f1_m: 0.2865 - val_loss: 0.9325 - val_accuracy: 0.3042 - val_precision_m: 0.0501 - val_recall_m: 0.9651 - val_f1_m: 0.0952\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.51620\n",
      "Epoch 41/100\n",
      "16/16 - 1s - loss: 0.8131 - accuracy: 0.7934 - precision_m: 0.1914 - recall_m: 0.8306 - f1_m: 0.3024 - val_loss: 0.9267 - val_accuracy: 0.4016 - val_precision_m: 0.0563 - val_recall_m: 0.9353 - val_f1_m: 0.1062\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.51620\n",
      "Epoch 42/100\n",
      "16/16 - 1s - loss: 0.7953 - accuracy: 0.8211 - precision_m: 0.2208 - recall_m: 0.8391 - f1_m: 0.3440 - val_loss: 0.9258 - val_accuracy: 0.4343 - val_precision_m: 0.0574 - val_recall_m: 0.8999 - val_f1_m: 0.1079\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.51620\n",
      "Epoch 43/100\n",
      "16/16 - 1s - loss: 0.7924 - accuracy: 0.8214 - precision_m: 0.2259 - recall_m: 0.8572 - f1_m: 0.3515 - val_loss: 0.9237 - val_accuracy: 0.4832 - val_precision_m: 0.0594 - val_recall_m: 0.8483 - val_f1_m: 0.1110\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.51620\n",
      "Epoch 44/100\n",
      "16/16 - 1s - loss: 0.7893 - accuracy: 0.8294 - precision_m: 0.2304 - recall_m: 0.8557 - f1_m: 0.3554 - val_loss: 0.9215 - val_accuracy: 0.4937 - val_precision_m: 0.0617 - val_recall_m: 0.8638 - val_f1_m: 0.1151\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.51620\n",
      "Epoch 45/100\n",
      "16/16 - 1s - loss: 0.7991 - accuracy: 0.8187 - precision_m: 0.2125 - recall_m: 0.8237 - f1_m: 0.3332 - val_loss: 0.9224 - val_accuracy: 0.4765 - val_precision_m: 0.0607 - val_recall_m: 0.8784 - val_f1_m: 0.1135\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.51620\n",
      "Epoch 46/100\n",
      "16/16 - 1s - loss: 0.7994 - accuracy: 0.8139 - precision_m: 0.2102 - recall_m: 0.8179 - f1_m: 0.3288 - val_loss: 0.9242 - val_accuracy: 0.4333 - val_precision_m: 0.0581 - val_recall_m: 0.9097 - val_f1_m: 0.1092\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.51620\n",
      "Epoch 47/100\n",
      "16/16 - 1s - loss: 0.7807 - accuracy: 0.8384 - precision_m: 0.2417 - recall_m: 0.8662 - f1_m: 0.3707 - val_loss: 0.9199 - val_accuracy: 0.5126 - val_precision_m: 0.0628 - val_recall_m: 0.8442 - val_f1_m: 0.1168\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.51620\n",
      "Epoch 48/100\n",
      "16/16 - 1s - loss: 0.7836 - accuracy: 0.8406 - precision_m: 0.2402 - recall_m: 0.8606 - f1_m: 0.3717 - val_loss: 0.9186 - val_accuracy: 0.5178 - val_precision_m: 0.0643 - val_recall_m: 0.8556 - val_f1_m: 0.1197\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.51620\n",
      "Epoch 49/100\n",
      "16/16 - 1s - loss: 0.7848 - accuracy: 0.8250 - precision_m: 0.2304 - recall_m: 0.8633 - f1_m: 0.3561 - val_loss: 0.9149 - val_accuracy: 0.5749 - val_precision_m: 0.0689 - val_recall_m: 0.8022 - val_f1_m: 0.1269\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.51620\n",
      "Epoch 50/100\n",
      "16/16 - 1s - loss: 0.7761 - accuracy: 0.8493 - precision_m: 0.2529 - recall_m: 0.8433 - f1_m: 0.3831 - val_loss: 0.9117 - val_accuracy: 0.5989 - val_precision_m: 0.0725 - val_recall_m: 0.7928 - val_f1_m: 0.1329\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.51620\n",
      "Epoch 51/100\n",
      "16/16 - 1s - loss: 0.7762 - accuracy: 0.8484 - precision_m: 0.2512 - recall_m: 0.8380 - f1_m: 0.3808 - val_loss: 0.9153 - val_accuracy: 0.5647 - val_precision_m: 0.0681 - val_recall_m: 0.8129 - val_f1_m: 0.1257\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.51620\n",
      "Epoch 52/100\n",
      "16/16 - 1s - loss: 0.7777 - accuracy: 0.8494 - precision_m: 0.2526 - recall_m: 0.8353 - f1_m: 0.3807 - val_loss: 0.9161 - val_accuracy: 0.5686 - val_precision_m: 0.0674 - val_recall_m: 0.7942 - val_f1_m: 0.1243\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.51620\n",
      "Epoch 53/100\n",
      "16/16 - 1s - loss: 0.7857 - accuracy: 0.8376 - precision_m: 0.2293 - recall_m: 0.8065 - f1_m: 0.3523 - val_loss: 0.9161 - val_accuracy: 0.5365 - val_precision_m: 0.0664 - val_recall_m: 0.8459 - val_f1_m: 0.1231\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.51620\n",
      "Epoch 54/100\n",
      "16/16 - 1s - loss: 0.7697 - accuracy: 0.8410 - precision_m: 0.2490 - recall_m: 0.8778 - f1_m: 0.3824 - val_loss: 0.9176 - val_accuracy: 0.5247 - val_precision_m: 0.0646 - val_recall_m: 0.8452 - val_f1_m: 0.1199\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.51620\n",
      "Epoch 55/100\n",
      "16/16 - 1s - loss: 0.7776 - accuracy: 0.8462 - precision_m: 0.2522 - recall_m: 0.8180 - f1_m: 0.3748 - val_loss: 0.9189 - val_accuracy: 0.5264 - val_precision_m: 0.0633 - val_recall_m: 0.8251 - val_f1_m: 0.1175\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.51620\n",
      "Epoch 56/100\n",
      "16/16 - 1s - loss: 0.7673 - accuracy: 0.8535 - precision_m: 0.2606 - recall_m: 0.8569 - f1_m: 0.3916 - val_loss: 0.9174 - val_accuracy: 0.5322 - val_precision_m: 0.0646 - val_recall_m: 0.8320 - val_f1_m: 0.1199\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.51620\n",
      "Epoch 57/100\n",
      "16/16 - 1s - loss: 0.7591 - accuracy: 0.8713 - precision_m: 0.2840 - recall_m: 0.8337 - f1_m: 0.4204 - val_loss: 0.9147 - val_accuracy: 0.5715 - val_precision_m: 0.0683 - val_recall_m: 0.8012 - val_f1_m: 0.1259\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.51620\n",
      "Epoch 58/100\n",
      "16/16 - 1s - loss: 0.7616 - accuracy: 0.8646 - precision_m: 0.2745 - recall_m: 0.8597 - f1_m: 0.4127 - val_loss: 0.9121 - val_accuracy: 0.5882 - val_precision_m: 0.0710 - val_recall_m: 0.7993 - val_f1_m: 0.1304\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.51620\n",
      "Epoch 59/100\n",
      "16/16 - 1s - loss: 0.7660 - accuracy: 0.8634 - precision_m: 0.2735 - recall_m: 0.8066 - f1_m: 0.3971 - val_loss: 0.9155 - val_accuracy: 0.5548 - val_precision_m: 0.0667 - val_recall_m: 0.8147 - val_f1_m: 0.1233\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.51620\n",
      "Epoch 60/100\n",
      "16/16 - 1s - loss: 0.7697 - accuracy: 0.8489 - precision_m: 0.2503 - recall_m: 0.8353 - f1_m: 0.3810 - val_loss: 0.9139 - val_accuracy: 0.5592 - val_precision_m: 0.0685 - val_recall_m: 0.8275 - val_f1_m: 0.1265\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.51620\n",
      "Epoch 61/100\n",
      "16/16 - 1s - loss: 0.7593 - accuracy: 0.8661 - precision_m: 0.2794 - recall_m: 0.8314 - f1_m: 0.4018 - val_loss: 0.9146 - val_accuracy: 0.5497 - val_precision_m: 0.0674 - val_recall_m: 0.8334 - val_f1_m: 0.1247\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.51620\n",
      "Epoch 62/100\n",
      "16/16 - 1s - loss: 0.7552 - accuracy: 0.8682 - precision_m: 0.2815 - recall_m: 0.8518 - f1_m: 0.4176 - val_loss: 0.9121 - val_accuracy: 0.5549 - val_precision_m: 0.0703 - val_recall_m: 0.8620 - val_f1_m: 0.1300\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.51620\n",
      "Epoch 63/100\n",
      "16/16 - 1s - loss: 0.7648 - accuracy: 0.8657 - precision_m: 0.2684 - recall_m: 0.8115 - f1_m: 0.3942 - val_loss: 0.9126 - val_accuracy: 0.5722 - val_precision_m: 0.0698 - val_recall_m: 0.8197 - val_f1_m: 0.1286\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.51620\n",
      "Epoch 64/100\n",
      "16/16 - 1s - loss: 0.7518 - accuracy: 0.8810 - precision_m: 0.2949 - recall_m: 0.8305 - f1_m: 0.4278 - val_loss: 0.9093 - val_accuracy: 0.6171 - val_precision_m: 0.0739 - val_recall_m: 0.7701 - val_f1_m: 0.1349\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.51620\n",
      "Epoch 65/100\n",
      "16/16 - 1s - loss: 0.7632 - accuracy: 0.8649 - precision_m: 0.2675 - recall_m: 0.8256 - f1_m: 0.3986 - val_loss: 0.9080 - val_accuracy: 0.6137 - val_precision_m: 0.0747 - val_recall_m: 0.7860 - val_f1_m: 0.1364\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.51620\n",
      "Epoch 66/100\n",
      "16/16 - 1s - loss: 0.7434 - accuracy: 0.8863 - precision_m: 0.3137 - recall_m: 0.8379 - f1_m: 0.4482 - val_loss: 0.9045 - val_accuracy: 0.6855 - val_precision_m: 0.0809 - val_recall_m: 0.6781 - val_f1_m: 0.1446\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.51620\n",
      "Epoch 67/100\n",
      "16/16 - 1s - loss: 0.7443 - accuracy: 0.8873 - precision_m: 0.3144 - recall_m: 0.8233 - f1_m: 0.4487 - val_loss: 0.9082 - val_accuracy: 0.6189 - val_precision_m: 0.0748 - val_recall_m: 0.7765 - val_f1_m: 0.1365\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.51620\n",
      "Epoch 68/100\n",
      "16/16 - 1s - loss: 0.7429 - accuracy: 0.8830 - precision_m: 0.3094 - recall_m: 0.8375 - f1_m: 0.4457 - val_loss: 0.9028 - val_accuracy: 0.6747 - val_precision_m: 0.0826 - val_recall_m: 0.7204 - val_f1_m: 0.1482\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.51620\n",
      "Epoch 69/100\n",
      "16/16 - 1s - loss: 0.7504 - accuracy: 0.8862 - precision_m: 0.3062 - recall_m: 0.7974 - f1_m: 0.4324 - val_loss: 0.9026 - val_accuracy: 0.6957 - val_precision_m: 0.0837 - val_recall_m: 0.6775 - val_f1_m: 0.1490\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.51620\n",
      "Epoch 70/100\n",
      "16/16 - 1s - loss: 0.7496 - accuracy: 0.8733 - precision_m: 0.2884 - recall_m: 0.8516 - f1_m: 0.4201 - val_loss: 0.9028 - val_accuracy: 0.6992 - val_precision_m: 0.0838 - val_recall_m: 0.6662 - val_f1_m: 0.1489\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.51620\n",
      "Epoch 71/100\n",
      "16/16 - 1s - loss: 0.7433 - accuracy: 0.8801 - precision_m: 0.3071 - recall_m: 0.8439 - f1_m: 0.4388 - val_loss: 0.8969 - val_accuracy: 0.7310 - val_precision_m: 0.0916 - val_recall_m: 0.6444 - val_f1_m: 0.1604\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.51620\n",
      "Epoch 72/100\n",
      "16/16 - 1s - loss: 0.7485 - accuracy: 0.8794 - precision_m: 0.3009 - recall_m: 0.8187 - f1_m: 0.4329 - val_loss: 0.9023 - val_accuracy: 0.6717 - val_precision_m: 0.0817 - val_recall_m: 0.7231 - val_f1_m: 0.1468\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.51620\n",
      "Epoch 73/100\n",
      "16/16 - 1s - loss: 0.7429 - accuracy: 0.8721 - precision_m: 0.2945 - recall_m: 0.8678 - f1_m: 0.4312 - val_loss: 0.8990 - val_accuracy: 0.6848 - val_precision_m: 0.0861 - val_recall_m: 0.7273 - val_f1_m: 0.1539\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.51620\n",
      "Epoch 74/100\n",
      "16/16 - 1s - loss: 0.7554 - accuracy: 0.8688 - precision_m: 0.2937 - recall_m: 0.8139 - f1_m: 0.4200 - val_loss: 0.9032 - val_accuracy: 0.6944 - val_precision_m: 0.0823 - val_recall_m: 0.6705 - val_f1_m: 0.1466\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.51620\n",
      "Epoch 75/100\n",
      "16/16 - 1s - loss: 0.7536 - accuracy: 0.8740 - precision_m: 0.2906 - recall_m: 0.8150 - f1_m: 0.4156 - val_loss: 0.8982 - val_accuracy: 0.6976 - val_precision_m: 0.0874 - val_recall_m: 0.7029 - val_f1_m: 0.1554\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.51620\n",
      "Epoch 76/100\n",
      "16/16 - 1s - loss: 0.7385 - accuracy: 0.8892 - precision_m: 0.3170 - recall_m: 0.8444 - f1_m: 0.4500 - val_loss: 0.8999 - val_accuracy: 0.7163 - val_precision_m: 0.0877 - val_recall_m: 0.6536 - val_f1_m: 0.1546\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.51620\n",
      "Epoch 77/100\n",
      "16/16 - 1s - loss: 0.7315 - accuracy: 0.8953 - precision_m: 0.3323 - recall_m: 0.8271 - f1_m: 0.4679 - val_loss: 0.8959 - val_accuracy: 0.7372 - val_precision_m: 0.0929 - val_recall_m: 0.6441 - val_f1_m: 0.1624\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.51620\n",
      "Epoch 78/100\n",
      "16/16 - 1s - loss: 0.7362 - accuracy: 0.8887 - precision_m: 0.3127 - recall_m: 0.8350 - f1_m: 0.4470 - val_loss: 0.8955 - val_accuracy: 0.7312 - val_precision_m: 0.0928 - val_recall_m: 0.6562 - val_f1_m: 0.1626\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.51620\n",
      "Epoch 79/100\n",
      "16/16 - 1s - loss: 0.7408 - accuracy: 0.8954 - precision_m: 0.3284 - recall_m: 0.8258 - f1_m: 0.4556 - val_loss: 0.8919 - val_accuracy: 0.7890 - val_precision_m: 0.1057 - val_recall_m: 0.5577 - val_f1_m: 0.1775\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.51620\n",
      "Epoch 80/100\n",
      "16/16 - 1s - loss: 0.7638 - accuracy: 0.8674 - precision_m: 0.2703 - recall_m: 0.7822 - f1_m: 0.3932 - val_loss: 0.8851 - val_accuracy: 0.7859 - val_precision_m: 0.1111 - val_recall_m: 0.5968 - val_f1_m: 0.1869\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.51620\n",
      "Epoch 81/100\n",
      "16/16 - 1s - loss: 0.7431 - accuracy: 0.8806 - precision_m: 0.3003 - recall_m: 0.8244 - f1_m: 0.4315 - val_loss: 0.8941 - val_accuracy: 0.7701 - val_precision_m: 0.0994 - val_recall_m: 0.5821 - val_f1_m: 0.1698\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.51620\n",
      "Epoch 82/100\n",
      "16/16 - 1s - loss: 0.7315 - accuracy: 0.8985 - precision_m: 0.3364 - recall_m: 0.8108 - f1_m: 0.4685 - val_loss: 0.9021 - val_accuracy: 0.6779 - val_precision_m: 0.0828 - val_recall_m: 0.7161 - val_f1_m: 0.1484\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.51620\n",
      "Epoch 83/100\n",
      "16/16 - 1s - loss: 0.7374 - accuracy: 0.8853 - precision_m: 0.3068 - recall_m: 0.8479 - f1_m: 0.4426 - val_loss: 0.8927 - val_accuracy: 0.7498 - val_precision_m: 0.0976 - val_recall_m: 0.6337 - val_f1_m: 0.1690\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.51620\n",
      "Epoch 84/100\n",
      "16/16 - 1s - loss: 0.7391 - accuracy: 0.8860 - precision_m: 0.3118 - recall_m: 0.8177 - f1_m: 0.4416 - val_loss: 0.8956 - val_accuracy: 0.7237 - val_precision_m: 0.0918 - val_recall_m: 0.6733 - val_f1_m: 0.1615\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.51620\n",
      "Epoch 85/100\n",
      "16/16 - 1s - loss: 0.7318 - accuracy: 0.8958 - precision_m: 0.3443 - recall_m: 0.8274 - f1_m: 0.4688 - val_loss: 0.8964 - val_accuracy: 0.7568 - val_precision_m: 0.0952 - val_recall_m: 0.5900 - val_f1_m: 0.1639\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.51620\n",
      "Epoch 86/100\n",
      "16/16 - 1s - loss: 0.7423 - accuracy: 0.8829 - precision_m: 0.3030 - recall_m: 0.8238 - f1_m: 0.4306 - val_loss: 0.8956 - val_accuracy: 0.7238 - val_precision_m: 0.0915 - val_recall_m: 0.6711 - val_f1_m: 0.1611\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.51620\n",
      "Epoch 87/100\n",
      "16/16 - 1s - loss: 0.7348 - accuracy: 0.8868 - precision_m: 0.3127 - recall_m: 0.8455 - f1_m: 0.4493 - val_loss: 0.8930 - val_accuracy: 0.7430 - val_precision_m: 0.0964 - val_recall_m: 0.6472 - val_f1_m: 0.1677\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.51620\n",
      "Epoch 88/100\n",
      "16/16 - 1s - loss: 0.7216 - accuracy: 0.9036 - precision_m: 0.3501 - recall_m: 0.8420 - f1_m: 0.4829 - val_loss: 0.8939 - val_accuracy: 0.7588 - val_precision_m: 0.0978 - val_recall_m: 0.6082 - val_f1_m: 0.1684\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.51620\n",
      "Epoch 89/100\n",
      "16/16 - 1s - loss: 0.7227 - accuracy: 0.9000 - precision_m: 0.3438 - recall_m: 0.8436 - f1_m: 0.4792 - val_loss: 0.8921 - val_accuracy: 0.7510 - val_precision_m: 0.0984 - val_recall_m: 0.6385 - val_f1_m: 0.1704\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.51620\n",
      "Epoch 90/100\n",
      "16/16 - 1s - loss: 0.7268 - accuracy: 0.8952 - precision_m: 0.3335 - recall_m: 0.8272 - f1_m: 0.4693 - val_loss: 0.8822 - val_accuracy: 0.8057 - val_precision_m: 0.1220 - val_recall_m: 0.5659 - val_f1_m: 0.1998\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.51620\n",
      "Epoch 91/100\n",
      "16/16 - 1s - loss: 0.7330 - accuracy: 0.8980 - precision_m: 0.3312 - recall_m: 0.8051 - f1_m: 0.4581 - val_loss: 0.8819 - val_accuracy: 0.8149 - val_precision_m: 0.1249 - val_recall_m: 0.5545 - val_f1_m: 0.2032\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.51620\n",
      "Epoch 92/100\n",
      "16/16 - 1s - loss: 0.7238 - accuracy: 0.8976 - precision_m: 0.3384 - recall_m: 0.8390 - f1_m: 0.4759 - val_loss: 0.8901 - val_accuracy: 0.7935 - val_precision_m: 0.1082 - val_recall_m: 0.5513 - val_f1_m: 0.1807\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.51620\n",
      "Epoch 93/100\n",
      "16/16 - 1s - loss: 0.7282 - accuracy: 0.8948 - precision_m: 0.3367 - recall_m: 0.8191 - f1_m: 0.4663 - val_loss: 0.8833 - val_accuracy: 0.7860 - val_precision_m: 0.1142 - val_recall_m: 0.6146 - val_f1_m: 0.1923\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.51620\n",
      "Epoch 94/100\n",
      "16/16 - 1s - loss: 0.7172 - accuracy: 0.9013 - precision_m: 0.3489 - recall_m: 0.8591 - f1_m: 0.4860 - val_loss: 0.8995 - val_accuracy: 0.7065 - val_precision_m: 0.0868 - val_recall_m: 0.6801 - val_f1_m: 0.1540\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.51620\n",
      "Epoch 95/100\n",
      "16/16 - 1s - loss: 0.7209 - accuracy: 0.8987 - precision_m: 0.3481 - recall_m: 0.8342 - f1_m: 0.4782 - val_loss: 0.8888 - val_accuracy: 0.7517 - val_precision_m: 0.1011 - val_recall_m: 0.6595 - val_f1_m: 0.1752\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.51620\n",
      "Epoch 96/100\n",
      "16/16 - 1s - loss: 0.7220 - accuracy: 0.9062 - precision_m: 0.3536 - recall_m: 0.8206 - f1_m: 0.4810 - val_loss: 0.9006 - val_accuracy: 0.7085 - val_precision_m: 0.0856 - val_recall_m: 0.6676 - val_f1_m: 0.1517\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.51620\n",
      "Epoch 97/100\n",
      "16/16 - 1s - loss: 0.7128 - accuracy: 0.9106 - precision_m: 0.3730 - recall_m: 0.8200 - f1_m: 0.5068 - val_loss: 0.8934 - val_accuracy: 0.7337 - val_precision_m: 0.0943 - val_recall_m: 0.6691 - val_f1_m: 0.1654\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.51620\n",
      "Epoch 98/100\n",
      "16/16 - 1s - loss: 0.7351 - accuracy: 0.8925 - precision_m: 0.3185 - recall_m: 0.7940 - f1_m: 0.4494 - val_loss: 0.8790 - val_accuracy: 0.8084 - val_precision_m: 0.1244 - val_recall_m: 0.5791 - val_f1_m: 0.2042\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.51620\n",
      "Epoch 99/100\n",
      "16/16 - 1s - loss: 0.7309 - accuracy: 0.9101 - precision_m: 0.3551 - recall_m: 0.7702 - f1_m: 0.4776 - val_loss: 0.8932 - val_accuracy: 0.7718 - val_precision_m: 0.0996 - val_recall_m: 0.5867 - val_f1_m: 0.1702\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.51620\n",
      "Epoch 100/100\n",
      "16/16 - 1s - loss: 0.7262 - accuracy: 0.9038 - precision_m: 0.3445 - recall_m: 0.8029 - f1_m: 0.4767 - val_loss: 0.8973 - val_accuracy: 0.7046 - val_precision_m: 0.0881 - val_recall_m: 0.7026 - val_f1_m: 0.1565\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.51620\n",
      "5/5 - 0s - loss: 0.8110 - accuracy: 0.7646 - precision_m: 0.1731 - recall_m: 0.8082 - f1_m: 0.2845\n",
      "The metrics for the test set with loss tversky, learning rate 1e-05,  filters 32, and batch size 16 is [0.8110126852989197, 0.7646012306213379, 0.17309829592704773, 0.808214008808136, 0.2845427989959717]\n",
      "__________________________________________________________________________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Number of epochs\n",
    "NUMBER_EPOCHS = 100\n",
    "\n",
    "# List of hyperparameters\n",
    "lr = [1e-3,1e-4,1e-5]\n",
    "batch_size_list = [8,16]\n",
    "filters = [8,16,32] \n",
    "\n",
    "# Loss \n",
    "loss_name = \"tversky\"\n",
    "\n",
    "# Dictionary of results\n",
    "results = {\n",
    "           \"filters\": [],\n",
    "           \"learning_rate\": [],\n",
    "           \"batch_size\": [],\n",
    "           \"loss\": [],\n",
    "           \"accuracy\": [],\n",
    "           \"precision\": [],\n",
    "           \"recall\": [],\n",
    "           \"f1\": []\n",
    "           }\n",
    "\n",
    "for learning_rate in lr:\n",
    "\n",
    "    for batch_size in batch_size_list:\n",
    "\n",
    "        for no_filters in filters:\n",
    "            \n",
    "            # Here you can choose between the two models that you prefer. If you choose one, comment the other one. \n",
    "            model = UNet(loss=focal_tversky, lr=learning_rate, filters=no_filters,pretrained_weights=None, input_size=(PATCHSIZE, PATCHSIZE, NBANDS))\n",
    "            # model = attn_unet(loss=tversky_loss, lr=learning_rate, filters=no_filters,pretrained_weights=None, input_size=(PATCHSIZE, PATCHSIZE, NBANDS))\n",
    "\n",
    "            print(\"---------------------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "            print(\"Initialising in 3..........2..........1......................................\")\n",
    "\n",
    "            print(f'Training on {X_train.shape[0]} images with learning rate {learning_rate}, batch size {batch_size}, and number of filters {no_filters}')\n",
    "\n",
    "            history = model.fit(x = tf.cast(X_train, tf.float32), \n",
    "                        y = tf.cast(Y_train, tf.float32),\n",
    "                        epochs = NUMBER_EPOCHS,\n",
    "                        batch_size = batch_size,\n",
    "                        validation_split = 0.1,\n",
    "                        callbacks = callback_list,\n",
    "                        verbose = 2) \n",
    "\n",
    "            ########      EVALUATION      ###########\n",
    "            tf.keras.backend.clear_session()\n",
    "            evaluate = model.evaluate(tf.cast(X_test, tf.float32), tf.cast(Y_test, tf.float32), verbose=2)\n",
    "\n",
    "            ########      Saving results      ###########\n",
    "            results[\"filters\"].append(no_filters)\n",
    "            results[\"learning_rate\"].append(learning_rate)\n",
    "            results[\"batch_size\"].append(batch_size)\n",
    "            results[\"loss\"].append(evaluate[0])\n",
    "            results[\"accuracy\"].append(evaluate[1])\n",
    "            results[\"precision\"].append(evaluate[2])\n",
    "            results[\"recall\"].append(evaluate[3])\n",
    "            results[\"f1\"].append(evaluate[4])\n",
    "\n",
    "            # Convert results to a pandas dataframe\n",
    "            results_df = pd.DataFrame(results)\n",
    "\n",
    "            # Export as a CSV\n",
    "            save_path_results = f'Data/Results in CSV'\n",
    "            if not os.path.exists(save_path_results):\n",
    "                os.makedirs(save_path_results)\n",
    "\n",
    "            results_df.to_csv(f'{save_path_results}/learning_rate_{learning_rate}_batch_size_{batch_size}_filters_{no_filters}_loss_{loss_name}.csv', index=False)\n",
    "\n",
    "            tf.compat.v1.reset_default_graph()\n",
    "\n",
    "            print(f\"The metrics for the test set with loss {loss_name}, learning rate {learning_rate},  filters {no_filters}, and batch size {batch_size} is {evaluate}\")\n",
    "            print(\"__________________________________________________________________________________________________________________________________________________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wbCCPDoPKIYw"
   },
   "source": [
    "### Visualise the curves of training error and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2627,
     "status": "ok",
     "timestamp": 1610970440991,
     "user": {
      "displayName": "Kushanav Bhuyan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhpJDzYajMCPL_21Vep_aig5cbphA73G195JQkf=s64",
      "userId": "13534778218088456884"
     },
     "user_tz": -60
    },
    "id": "azZQDWhHJTUc",
    "outputId": "4c88b973-0161-48c5-d0b8-ea4b63343068"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot learning curves\n",
    "\n",
    "#Training Loss curve\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.title(\"Sample net training curve loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.show()\n",
    "\n",
    "#Validation Loss curve\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.title(\"Sample net validation curve loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SmFTAVATJPM1"
   },
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD MODELS FROM PREVIOUS SESSIONS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 128, 128, 5) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 128, 128, 16) 736         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 128, 128, 16) 64          conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 128, 128, 16) 0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 128, 128, 16) 2320        activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 128, 128, 16) 64          conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 128, 128, 16) 0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling2D) (None, 64, 64, 16)   0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 64, 64, 32)   4640        max_pooling2d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 64, 64, 32)   128         conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 64, 64, 32)   0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 64, 64, 32)   9248        activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 64, 64, 32)   128         conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 64, 64, 32)   0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling2D) (None, 32, 32, 32)   0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 32, 32, 64)   18496       max_pooling2d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 32, 32, 64)   256         conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 32, 32, 64)   0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 32, 32, 64)   36928       activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 32, 32, 64)   256         conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 32, 32, 64)   0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling2D) (None, 16, 16, 64)   0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 16, 16, 128)  73856       max_pooling2d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 16, 16, 128)  512         conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 16, 16, 128)  0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 16, 16, 128)  147584      activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 16, 16, 128)  512         conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 16, 16, 128)  0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling2D) (None, 8, 8, 128)    0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 8, 8, 256)    295168      max_pooling2d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 8, 8, 256)    1024        conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 8, 8, 256)    0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 8, 8, 256)    590080      activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 8, 8, 256)    1024        conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 8, 8, 256)    0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_12 (Conv2DTran (None, 16, 16, 128)  131200      activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 16, 16, 256)  0           conv2d_transpose_12[0][0]        \n",
      "                                                                 activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 16, 16, 128)  295040      concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 16, 16, 128)  512         conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 16, 16, 128)  0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 16, 16, 128)  147584      activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 16, 16, 128)  512         conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 16, 16, 128)  0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_13 (Conv2DTran (None, 32, 32, 64)   32832       activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 32, 32, 128)  0           conv2d_transpose_13[0][0]        \n",
      "                                                                 activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 32, 32, 64)   73792       concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 32, 32, 64)   256         conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 32, 32, 64)   0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 32, 32, 64)   36928       activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 32, 32, 64)   256         conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 32, 32, 64)   0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_14 (Conv2DTran (None, 64, 64, 32)   8224        activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 64, 64, 64)   0           conv2d_transpose_14[0][0]        \n",
      "                                                                 activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 64, 64, 32)   18464       concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 64, 64, 32)   128         conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 64, 64, 32)   0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 64, 64, 32)   9248        activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 64, 64, 32)   128         conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 64, 64, 32)   0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_15 (Conv2DTran (None, 128, 128, 16) 2064        activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 128, 128, 32) 0           conv2d_transpose_15[0][0]        \n",
      "                                                                 activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 128, 128, 16) 4624        concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 128, 128, 16) 64          conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 128, 128, 16) 0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 128, 128, 16) 2320        activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 128, 128, 16) 64          conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 128, 128, 16) 0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 128, 128, 1)  17          activation_71[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,947,281\n",
      "Trainable params: 1,944,337\n",
      "Non-trainable params: 2,944\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "The metric evaluations are [0.09956713020801544, 0.9822847843170166, 0.9313087463378906, 0.8488621115684509, 0.8881522417068481]\n"
     ]
    }
   ],
   "source": [
    "# To load model weights from older runs, if present. If you do not have older runs, skip to the next cell\n",
    "# Please load previous metrics and functions and other required variables.\n",
    "\n",
    "best_model = UNet(loss=tversky_loss, lr=1e-3, filters=16,pretrained_weights=None, input_size=(PATCHSIZE, PATCHSIZE, NBANDS))\n",
    "best_model.load_weights(\"D:Data/Checkpoints/weights.81-0.20.hdf5\") # Load one from your checkpoints\n",
    "results = best_model.evaluate(tf.cast(X_test, tf.float32), tf.cast(Y_test, tf.float32), verbose=0)\n",
    "print(f'The metric evaluations are {results}')\n",
    "\n",
    "best_model.save(\"D:Data/Checkpoints/model.hdf5\") # This is to save the weights as a \"model\" h5 file type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktTIs3ibnVEq"
   },
   "source": [
    "### LOAD MODELS FROM CURRENT RUN (Current Session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4195,
     "status": "ok",
     "timestamp": 1611692292014,
     "user": {
      "displayName": "Kushanav Bhuyan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhpJDzYajMCPL_21Vep_aig5cbphA73G195JQkf=s64",
      "userId": "13534778218088456884"
     },
     "user_tz": -60
    },
    "id": "4reSJk2Ew4XX",
    "outputId": "e4ce2e0a-6386-4fb1-bbe9-63fc0e073eb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 - 0s - loss: 0.9734 - accuracy: 0.9498 - precision_m: 0.0838 - recall_m: 0.7973 - f1_m: 0.1458\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9733657836914062,\n",
       " 0.9497690200805664,\n",
       " 0.08378888666629791,\n",
       " 0.7973179817199707,\n",
       " 0.14582642912864685]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To load model from current run\n",
    "best_model.evaluate(tf.cast(X_test, tf.float32), tf.cast(Y_test, tf.float32), verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6IYYcW6uVJ0"
   },
   "source": [
    "### Predict the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 10s 10s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 13ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 14ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 14ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 14ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 19ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the first 10 predictions\n",
    "for i in range(10):\n",
    "    preds_train_1 = best_model.predict(np.expand_dims(X_test[i],axis = 0), verbose=1)\n",
    "    # It's possible to change the 0.5 threshold to improve the results;\n",
    "    preds_train_t1 = (preds_train_1 > 0.5).astype(np.uint8)\n",
    "    f, axarr = plt.subplots(1,3,figsize=(10,10))\n",
    "    axarr[0].imshow(X_test[i][:,:,:3])\n",
    "    axarr[1].imshow(np.squeeze(preds_train_t1))\n",
    "    axarr[2].imshow(np.squeeze(Y_test[i]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOZg4r3JA6/w8iI+xXJFZGJ",
   "collapsed_sections": [
    "FM7QdfvHzyww",
    "sJSfvuyV01zL",
    "Wjt6-QOtDKM1",
    "HfLAs-f3O59V"
   ],
   "machine_shape": "hm",
   "name": "Local_v0.3.1.ipynb",
   "provenance": [
    {
     "file_id": "14VGS6iC4C5lY2O9U_veYZMeX0u8MGMuX",
     "timestamp": 1610574487969
    },
    {
     "file_id": "10ci_zSfdTzMzqO2m7J9Nwme_Ebr5PX-P",
     "timestamp": 1609747228851
    }
   ],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "28ea9cb954659dbdeb242555929885b4da1f2c57e320f1a07e78377d39f2b6a8"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
